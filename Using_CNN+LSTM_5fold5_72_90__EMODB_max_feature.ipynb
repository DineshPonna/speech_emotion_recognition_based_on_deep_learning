{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 470,
      "id": "e1f95dfd",
      "metadata": {
        "id": "e1f95dfd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import seaborn as sns\n",
        "import librosa\n",
        "import librosa.display\n",
        "import IPython.display as pld\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xe7uVNwQ3Ppw",
        "outputId": "1505244e-00cc-4a98-ee07-b05d4837a2f9"
      },
      "id": "Xe7uVNwQ3Ppw",
      "execution_count": 471,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 472,
      "id": "603a1332",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "603a1332",
        "outputId": "d7cbaaf4-8dff-443b-a5bb-49ea5d6e822f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Unnamed: 0          0          1          2          3          4  \\\n",
              "0             0 -469.48477  88.400730  -7.127512  29.156132   5.335554   \n",
              "1             1 -434.88647  41.972150 -29.416862  18.537344  -4.156565   \n",
              "2             2 -454.89886  45.067265  -0.193278  15.111545   3.080835   \n",
              "3             3 -447.12630  86.114920   4.772520  38.097256   8.324276   \n",
              "4             4 -451.45264  71.335200  12.223010  32.649555   3.997801   \n",
              "..          ...        ...        ...        ...        ...        ...   \n",
              "423         423 -403.50656  30.989262 -12.756512  27.214573 -10.573180   \n",
              "424         424 -418.62490  57.015880   6.383415  47.618423  -8.051490   \n",
              "425         425 -427.36716  51.473750   4.835125  40.900140   6.937289   \n",
              "426         426 -467.15588  52.217026  10.470471  47.414780   8.690019   \n",
              "427         427 -437.97220   5.289146 -22.667547  25.394840 -28.545520   \n",
              "\n",
              "             5          6         7          8  ...       250       251  \\\n",
              "0     7.147227  -6.727572 -8.307674  -3.364513  ...  0.635522  0.544438   \n",
              "1     5.257353 -11.410935 -8.983023 -11.285996  ...  0.648380  0.675295   \n",
              "2     4.204241 -10.440731 -6.615343 -16.249382  ...  0.575784  0.523305   \n",
              "3     8.538317  -4.507682 -7.680664  -7.317249  ...  0.615263  0.566964   \n",
              "4    15.200773  -2.812883 -1.384373  -6.467040  ...  0.719856  0.667424   \n",
              "..         ...        ...       ...        ...  ...       ...       ...   \n",
              "423  -0.137991 -15.470394 -8.865036 -16.384440  ...  0.531090  0.567376   \n",
              "424   4.213936 -15.029120 -2.448467 -10.805821  ...  0.569263  0.551809   \n",
              "425  13.700687  -8.331753 -0.583414  -6.279562  ...  0.638155  0.574542   \n",
              "426  15.601270  -2.032935  4.985774  -7.432734  ...  0.523647  0.509723   \n",
              "427   0.428451 -11.650926 -8.022680 -18.337156  ...  0.495079  0.539387   \n",
              "\n",
              "          252       253       254       255       256       257       258  0.1  \n",
              "0    0.532856 -0.018488  0.011973 -0.011038  0.079758 -0.021044 -0.019445    1  \n",
              "1    0.560000 -0.010576 -0.000228  0.011102 -0.074188  0.012116 -0.000779    2  \n",
              "2    0.423375  0.009395 -0.025547 -0.035554 -0.025885  0.011198 -0.000163    0  \n",
              "3    0.605103 -0.008389 -0.051995 -0.056544 -0.020014  0.013964  0.008349    1  \n",
              "4    0.570905  0.003044 -0.006101 -0.009038 -0.049699  0.027615  0.021319    3  \n",
              "..        ...       ...       ...       ...       ...       ...       ...  ...  \n",
              "423  0.470224 -0.005500  0.013583  0.040782  0.003971  0.017743  0.026657    4  \n",
              "424  0.576764  0.016046 -0.025610  0.025426 -0.094371  0.008999 -0.004985    5  \n",
              "425  0.503516  0.019632  0.046315  0.074801  0.018078  0.022948 -0.016365    3  \n",
              "426  0.498566  0.051532  0.015525 -0.006052  0.018201  0.004361 -0.021649    3  \n",
              "427  0.527394 -0.001086  0.013948  0.013274 -0.056855 -0.001004 -0.001355    2  \n",
              "\n",
              "[428 rows x 261 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8fea3a78-95e5-4ecb-bfa3-f87b8aada834\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>...</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "      <th>257</th>\n",
              "      <th>258</th>\n",
              "      <th>0.1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-469.48477</td>\n",
              "      <td>88.400730</td>\n",
              "      <td>-7.127512</td>\n",
              "      <td>29.156132</td>\n",
              "      <td>5.335554</td>\n",
              "      <td>7.147227</td>\n",
              "      <td>-6.727572</td>\n",
              "      <td>-8.307674</td>\n",
              "      <td>-3.364513</td>\n",
              "      <td>...</td>\n",
              "      <td>0.635522</td>\n",
              "      <td>0.544438</td>\n",
              "      <td>0.532856</td>\n",
              "      <td>-0.018488</td>\n",
              "      <td>0.011973</td>\n",
              "      <td>-0.011038</td>\n",
              "      <td>0.079758</td>\n",
              "      <td>-0.021044</td>\n",
              "      <td>-0.019445</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>-434.88647</td>\n",
              "      <td>41.972150</td>\n",
              "      <td>-29.416862</td>\n",
              "      <td>18.537344</td>\n",
              "      <td>-4.156565</td>\n",
              "      <td>5.257353</td>\n",
              "      <td>-11.410935</td>\n",
              "      <td>-8.983023</td>\n",
              "      <td>-11.285996</td>\n",
              "      <td>...</td>\n",
              "      <td>0.648380</td>\n",
              "      <td>0.675295</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>-0.010576</td>\n",
              "      <td>-0.000228</td>\n",
              "      <td>0.011102</td>\n",
              "      <td>-0.074188</td>\n",
              "      <td>0.012116</td>\n",
              "      <td>-0.000779</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>-454.89886</td>\n",
              "      <td>45.067265</td>\n",
              "      <td>-0.193278</td>\n",
              "      <td>15.111545</td>\n",
              "      <td>3.080835</td>\n",
              "      <td>4.204241</td>\n",
              "      <td>-10.440731</td>\n",
              "      <td>-6.615343</td>\n",
              "      <td>-16.249382</td>\n",
              "      <td>...</td>\n",
              "      <td>0.575784</td>\n",
              "      <td>0.523305</td>\n",
              "      <td>0.423375</td>\n",
              "      <td>0.009395</td>\n",
              "      <td>-0.025547</td>\n",
              "      <td>-0.035554</td>\n",
              "      <td>-0.025885</td>\n",
              "      <td>0.011198</td>\n",
              "      <td>-0.000163</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>-447.12630</td>\n",
              "      <td>86.114920</td>\n",
              "      <td>4.772520</td>\n",
              "      <td>38.097256</td>\n",
              "      <td>8.324276</td>\n",
              "      <td>8.538317</td>\n",
              "      <td>-4.507682</td>\n",
              "      <td>-7.680664</td>\n",
              "      <td>-7.317249</td>\n",
              "      <td>...</td>\n",
              "      <td>0.615263</td>\n",
              "      <td>0.566964</td>\n",
              "      <td>0.605103</td>\n",
              "      <td>-0.008389</td>\n",
              "      <td>-0.051995</td>\n",
              "      <td>-0.056544</td>\n",
              "      <td>-0.020014</td>\n",
              "      <td>0.013964</td>\n",
              "      <td>0.008349</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>-451.45264</td>\n",
              "      <td>71.335200</td>\n",
              "      <td>12.223010</td>\n",
              "      <td>32.649555</td>\n",
              "      <td>3.997801</td>\n",
              "      <td>15.200773</td>\n",
              "      <td>-2.812883</td>\n",
              "      <td>-1.384373</td>\n",
              "      <td>-6.467040</td>\n",
              "      <td>...</td>\n",
              "      <td>0.719856</td>\n",
              "      <td>0.667424</td>\n",
              "      <td>0.570905</td>\n",
              "      <td>0.003044</td>\n",
              "      <td>-0.006101</td>\n",
              "      <td>-0.009038</td>\n",
              "      <td>-0.049699</td>\n",
              "      <td>0.027615</td>\n",
              "      <td>0.021319</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>423</th>\n",
              "      <td>423</td>\n",
              "      <td>-403.50656</td>\n",
              "      <td>30.989262</td>\n",
              "      <td>-12.756512</td>\n",
              "      <td>27.214573</td>\n",
              "      <td>-10.573180</td>\n",
              "      <td>-0.137991</td>\n",
              "      <td>-15.470394</td>\n",
              "      <td>-8.865036</td>\n",
              "      <td>-16.384440</td>\n",
              "      <td>...</td>\n",
              "      <td>0.531090</td>\n",
              "      <td>0.567376</td>\n",
              "      <td>0.470224</td>\n",
              "      <td>-0.005500</td>\n",
              "      <td>0.013583</td>\n",
              "      <td>0.040782</td>\n",
              "      <td>0.003971</td>\n",
              "      <td>0.017743</td>\n",
              "      <td>0.026657</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424</th>\n",
              "      <td>424</td>\n",
              "      <td>-418.62490</td>\n",
              "      <td>57.015880</td>\n",
              "      <td>6.383415</td>\n",
              "      <td>47.618423</td>\n",
              "      <td>-8.051490</td>\n",
              "      <td>4.213936</td>\n",
              "      <td>-15.029120</td>\n",
              "      <td>-2.448467</td>\n",
              "      <td>-10.805821</td>\n",
              "      <td>...</td>\n",
              "      <td>0.569263</td>\n",
              "      <td>0.551809</td>\n",
              "      <td>0.576764</td>\n",
              "      <td>0.016046</td>\n",
              "      <td>-0.025610</td>\n",
              "      <td>0.025426</td>\n",
              "      <td>-0.094371</td>\n",
              "      <td>0.008999</td>\n",
              "      <td>-0.004985</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>425</th>\n",
              "      <td>425</td>\n",
              "      <td>-427.36716</td>\n",
              "      <td>51.473750</td>\n",
              "      <td>4.835125</td>\n",
              "      <td>40.900140</td>\n",
              "      <td>6.937289</td>\n",
              "      <td>13.700687</td>\n",
              "      <td>-8.331753</td>\n",
              "      <td>-0.583414</td>\n",
              "      <td>-6.279562</td>\n",
              "      <td>...</td>\n",
              "      <td>0.638155</td>\n",
              "      <td>0.574542</td>\n",
              "      <td>0.503516</td>\n",
              "      <td>0.019632</td>\n",
              "      <td>0.046315</td>\n",
              "      <td>0.074801</td>\n",
              "      <td>0.018078</td>\n",
              "      <td>0.022948</td>\n",
              "      <td>-0.016365</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>426</th>\n",
              "      <td>426</td>\n",
              "      <td>-467.15588</td>\n",
              "      <td>52.217026</td>\n",
              "      <td>10.470471</td>\n",
              "      <td>47.414780</td>\n",
              "      <td>8.690019</td>\n",
              "      <td>15.601270</td>\n",
              "      <td>-2.032935</td>\n",
              "      <td>4.985774</td>\n",
              "      <td>-7.432734</td>\n",
              "      <td>...</td>\n",
              "      <td>0.523647</td>\n",
              "      <td>0.509723</td>\n",
              "      <td>0.498566</td>\n",
              "      <td>0.051532</td>\n",
              "      <td>0.015525</td>\n",
              "      <td>-0.006052</td>\n",
              "      <td>0.018201</td>\n",
              "      <td>0.004361</td>\n",
              "      <td>-0.021649</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>427</th>\n",
              "      <td>427</td>\n",
              "      <td>-437.97220</td>\n",
              "      <td>5.289146</td>\n",
              "      <td>-22.667547</td>\n",
              "      <td>25.394840</td>\n",
              "      <td>-28.545520</td>\n",
              "      <td>0.428451</td>\n",
              "      <td>-11.650926</td>\n",
              "      <td>-8.022680</td>\n",
              "      <td>-18.337156</td>\n",
              "      <td>...</td>\n",
              "      <td>0.495079</td>\n",
              "      <td>0.539387</td>\n",
              "      <td>0.527394</td>\n",
              "      <td>-0.001086</td>\n",
              "      <td>0.013948</td>\n",
              "      <td>0.013274</td>\n",
              "      <td>-0.056855</td>\n",
              "      <td>-0.001004</td>\n",
              "      <td>-0.001355</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>428 rows × 261 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8fea3a78-95e5-4ecb-bfa3-f87b8aada834')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8fea3a78-95e5-4ecb-bfa3-f87b8aada834 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8fea3a78-95e5-4ecb-bfa3-f87b8aada834');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-62823607-d68c-4ebf-8c4f-baa6286b56ca\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-62823607-d68c-4ebf-8c4f-baa6286b56ca')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-62823607-d68c-4ebf-8c4f-baa6286b56ca button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df1"
            }
          },
          "metadata": {},
          "execution_count": 472
        }
      ],
      "source": [
        "df1=pd.read_csv('/content/drive/MyDrive/DATASETS/EmoDB Dataset/train_fold5.csv')\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 473,
      "id": "a08acf9d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "a08acf9d",
        "outputId": "3f47939c-b59d-4c64-e0f8-07f0d770b71e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Unnamed: 0          0           1          2          3          4  \\\n",
              "0             0 -482.45233   62.835957  -0.348998  26.264595   2.978607   \n",
              "1             1 -491.33237   69.272530  12.908265  34.913530   7.287787   \n",
              "2             2 -433.93875   47.439816 -14.131547  33.306270   0.370407   \n",
              "3             3 -451.39062   54.114080 -15.474982  12.761942 -14.734682   \n",
              "4             4 -447.41296  105.425030   0.545184  40.437737   9.768583   \n",
              "..          ...        ...         ...        ...        ...        ...   \n",
              "102         102 -491.15110    8.158493 -39.897430   7.153264 -30.544424   \n",
              "103         103 -442.50560   16.904572 -26.406687  25.167395 -24.619476   \n",
              "104         104 -429.41583   51.232320  -4.127894  38.715740   2.253632   \n",
              "105         105 -478.05527    5.705422 -24.631056  27.622614 -19.235449   \n",
              "106         106 -526.19570   11.317784 -18.942173  29.540787 -28.057306   \n",
              "\n",
              "             5          6          7          8  ...       250       251  \\\n",
              "0     6.900927 -13.006243   0.273391  -9.196591  ...  0.675823  0.684034   \n",
              "1    10.195468  -5.368203   1.112940  -0.885312  ...  0.780847  0.746327   \n",
              "2    -1.469261 -12.553712  -9.810813 -14.061879  ...  0.666296  0.654239   \n",
              "3     3.685768 -11.450237 -12.940584 -20.427963  ...  0.607648  0.622860   \n",
              "4     4.426243 -11.221826  -9.262340  -3.457486  ...  0.682722  0.580366   \n",
              "..         ...        ...        ...        ...  ...       ...       ...   \n",
              "102  10.330252 -19.721329 -12.623847 -14.831120  ...  0.444168  0.486394   \n",
              "103  -4.223975 -19.451237  -7.989911 -21.999931  ...  0.407037  0.406521   \n",
              "104  -2.197497 -15.981533 -14.225066 -17.897070  ...  0.589742  0.609879   \n",
              "105  -7.040019 -13.293165  -9.629133 -14.067036  ...  0.590422  0.481957   \n",
              "106   1.299599 -16.251814  -7.512440 -23.191568  ...  0.528861  0.501070   \n",
              "\n",
              "          252       253       254       255       256       257       258  0.1  \n",
              "0    0.627376 -0.001574  0.012645 -0.027069  0.019313 -0.002252 -0.006220    0  \n",
              "1    0.655106  0.009772  0.000478 -0.041075 -0.024908 -0.000306  0.004096    3  \n",
              "2    0.584524  0.001197 -0.002374  0.009570  0.005788 -0.000300  0.000001    2  \n",
              "3    0.615351  0.009293  0.009108  0.034368 -0.004956  0.000385  0.004414    2  \n",
              "4    0.602522  0.019718 -0.030081 -0.054242  0.018702  0.000313  0.008927    5  \n",
              "..        ...       ...       ...       ...       ...       ...       ...  ...  \n",
              "102  0.480718  0.005411 -0.005025  0.028497 -0.069591  0.017052 -0.001056    2  \n",
              "103  0.464291  0.000847 -0.014598 -0.031204  0.019990 -0.013624 -0.004070    4  \n",
              "104  0.515997 -0.019026  0.000340 -0.022717 -0.020690 -0.002970 -0.001101    6  \n",
              "105  0.481437 -0.011216 -0.031485 -0.029936  0.042160 -0.013815  0.009839    0  \n",
              "106  0.496900  0.007856  0.005917 -0.045393 -0.004246 -0.001628  0.015051    2  \n",
              "\n",
              "[107 rows x 261 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9a9d5454-9a55-409c-a38d-739f04a0ddee\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>...</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "      <th>257</th>\n",
              "      <th>258</th>\n",
              "      <th>0.1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-482.45233</td>\n",
              "      <td>62.835957</td>\n",
              "      <td>-0.348998</td>\n",
              "      <td>26.264595</td>\n",
              "      <td>2.978607</td>\n",
              "      <td>6.900927</td>\n",
              "      <td>-13.006243</td>\n",
              "      <td>0.273391</td>\n",
              "      <td>-9.196591</td>\n",
              "      <td>...</td>\n",
              "      <td>0.675823</td>\n",
              "      <td>0.684034</td>\n",
              "      <td>0.627376</td>\n",
              "      <td>-0.001574</td>\n",
              "      <td>0.012645</td>\n",
              "      <td>-0.027069</td>\n",
              "      <td>0.019313</td>\n",
              "      <td>-0.002252</td>\n",
              "      <td>-0.006220</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>-491.33237</td>\n",
              "      <td>69.272530</td>\n",
              "      <td>12.908265</td>\n",
              "      <td>34.913530</td>\n",
              "      <td>7.287787</td>\n",
              "      <td>10.195468</td>\n",
              "      <td>-5.368203</td>\n",
              "      <td>1.112940</td>\n",
              "      <td>-0.885312</td>\n",
              "      <td>...</td>\n",
              "      <td>0.780847</td>\n",
              "      <td>0.746327</td>\n",
              "      <td>0.655106</td>\n",
              "      <td>0.009772</td>\n",
              "      <td>0.000478</td>\n",
              "      <td>-0.041075</td>\n",
              "      <td>-0.024908</td>\n",
              "      <td>-0.000306</td>\n",
              "      <td>0.004096</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>-433.93875</td>\n",
              "      <td>47.439816</td>\n",
              "      <td>-14.131547</td>\n",
              "      <td>33.306270</td>\n",
              "      <td>0.370407</td>\n",
              "      <td>-1.469261</td>\n",
              "      <td>-12.553712</td>\n",
              "      <td>-9.810813</td>\n",
              "      <td>-14.061879</td>\n",
              "      <td>...</td>\n",
              "      <td>0.666296</td>\n",
              "      <td>0.654239</td>\n",
              "      <td>0.584524</td>\n",
              "      <td>0.001197</td>\n",
              "      <td>-0.002374</td>\n",
              "      <td>0.009570</td>\n",
              "      <td>0.005788</td>\n",
              "      <td>-0.000300</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>-451.39062</td>\n",
              "      <td>54.114080</td>\n",
              "      <td>-15.474982</td>\n",
              "      <td>12.761942</td>\n",
              "      <td>-14.734682</td>\n",
              "      <td>3.685768</td>\n",
              "      <td>-11.450237</td>\n",
              "      <td>-12.940584</td>\n",
              "      <td>-20.427963</td>\n",
              "      <td>...</td>\n",
              "      <td>0.607648</td>\n",
              "      <td>0.622860</td>\n",
              "      <td>0.615351</td>\n",
              "      <td>0.009293</td>\n",
              "      <td>0.009108</td>\n",
              "      <td>0.034368</td>\n",
              "      <td>-0.004956</td>\n",
              "      <td>0.000385</td>\n",
              "      <td>0.004414</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>-447.41296</td>\n",
              "      <td>105.425030</td>\n",
              "      <td>0.545184</td>\n",
              "      <td>40.437737</td>\n",
              "      <td>9.768583</td>\n",
              "      <td>4.426243</td>\n",
              "      <td>-11.221826</td>\n",
              "      <td>-9.262340</td>\n",
              "      <td>-3.457486</td>\n",
              "      <td>...</td>\n",
              "      <td>0.682722</td>\n",
              "      <td>0.580366</td>\n",
              "      <td>0.602522</td>\n",
              "      <td>0.019718</td>\n",
              "      <td>-0.030081</td>\n",
              "      <td>-0.054242</td>\n",
              "      <td>0.018702</td>\n",
              "      <td>0.000313</td>\n",
              "      <td>0.008927</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>102</td>\n",
              "      <td>-491.15110</td>\n",
              "      <td>8.158493</td>\n",
              "      <td>-39.897430</td>\n",
              "      <td>7.153264</td>\n",
              "      <td>-30.544424</td>\n",
              "      <td>10.330252</td>\n",
              "      <td>-19.721329</td>\n",
              "      <td>-12.623847</td>\n",
              "      <td>-14.831120</td>\n",
              "      <td>...</td>\n",
              "      <td>0.444168</td>\n",
              "      <td>0.486394</td>\n",
              "      <td>0.480718</td>\n",
              "      <td>0.005411</td>\n",
              "      <td>-0.005025</td>\n",
              "      <td>0.028497</td>\n",
              "      <td>-0.069591</td>\n",
              "      <td>0.017052</td>\n",
              "      <td>-0.001056</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>103</td>\n",
              "      <td>-442.50560</td>\n",
              "      <td>16.904572</td>\n",
              "      <td>-26.406687</td>\n",
              "      <td>25.167395</td>\n",
              "      <td>-24.619476</td>\n",
              "      <td>-4.223975</td>\n",
              "      <td>-19.451237</td>\n",
              "      <td>-7.989911</td>\n",
              "      <td>-21.999931</td>\n",
              "      <td>...</td>\n",
              "      <td>0.407037</td>\n",
              "      <td>0.406521</td>\n",
              "      <td>0.464291</td>\n",
              "      <td>0.000847</td>\n",
              "      <td>-0.014598</td>\n",
              "      <td>-0.031204</td>\n",
              "      <td>0.019990</td>\n",
              "      <td>-0.013624</td>\n",
              "      <td>-0.004070</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>104</td>\n",
              "      <td>-429.41583</td>\n",
              "      <td>51.232320</td>\n",
              "      <td>-4.127894</td>\n",
              "      <td>38.715740</td>\n",
              "      <td>2.253632</td>\n",
              "      <td>-2.197497</td>\n",
              "      <td>-15.981533</td>\n",
              "      <td>-14.225066</td>\n",
              "      <td>-17.897070</td>\n",
              "      <td>...</td>\n",
              "      <td>0.589742</td>\n",
              "      <td>0.609879</td>\n",
              "      <td>0.515997</td>\n",
              "      <td>-0.019026</td>\n",
              "      <td>0.000340</td>\n",
              "      <td>-0.022717</td>\n",
              "      <td>-0.020690</td>\n",
              "      <td>-0.002970</td>\n",
              "      <td>-0.001101</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>105</td>\n",
              "      <td>-478.05527</td>\n",
              "      <td>5.705422</td>\n",
              "      <td>-24.631056</td>\n",
              "      <td>27.622614</td>\n",
              "      <td>-19.235449</td>\n",
              "      <td>-7.040019</td>\n",
              "      <td>-13.293165</td>\n",
              "      <td>-9.629133</td>\n",
              "      <td>-14.067036</td>\n",
              "      <td>...</td>\n",
              "      <td>0.590422</td>\n",
              "      <td>0.481957</td>\n",
              "      <td>0.481437</td>\n",
              "      <td>-0.011216</td>\n",
              "      <td>-0.031485</td>\n",
              "      <td>-0.029936</td>\n",
              "      <td>0.042160</td>\n",
              "      <td>-0.013815</td>\n",
              "      <td>0.009839</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>106</td>\n",
              "      <td>-526.19570</td>\n",
              "      <td>11.317784</td>\n",
              "      <td>-18.942173</td>\n",
              "      <td>29.540787</td>\n",
              "      <td>-28.057306</td>\n",
              "      <td>1.299599</td>\n",
              "      <td>-16.251814</td>\n",
              "      <td>-7.512440</td>\n",
              "      <td>-23.191568</td>\n",
              "      <td>...</td>\n",
              "      <td>0.528861</td>\n",
              "      <td>0.501070</td>\n",
              "      <td>0.496900</td>\n",
              "      <td>0.007856</td>\n",
              "      <td>0.005917</td>\n",
              "      <td>-0.045393</td>\n",
              "      <td>-0.004246</td>\n",
              "      <td>-0.001628</td>\n",
              "      <td>0.015051</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>107 rows × 261 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9a9d5454-9a55-409c-a38d-739f04a0ddee')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9a9d5454-9a55-409c-a38d-739f04a0ddee button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9a9d5454-9a55-409c-a38d-739f04a0ddee');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9297d94c-db2c-40c9-bd70-740fd6013227\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9297d94c-db2c-40c9-bd70-740fd6013227')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9297d94c-db2c-40c9-bd70-740fd6013227 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df2"
            }
          },
          "metadata": {},
          "execution_count": 473
        }
      ],
      "source": [
        "df2=pd.read_csv('/content/drive/MyDrive/DATASETS/EmoDB Dataset/test_fold5.csv')\n",
        "df2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 474,
      "id": "a3d750f7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3d750f7",
        "outputId": "47443e9e-292f-4c2c-e26d-1cc6aea3078c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-469.48477, -434.88647, -454.89886, -447.1263 , -451.45264,\n",
              "       -433.33972, -465.2206 , -407.61963, -478.92007, -445.90384,\n",
              "       -432.8532 , -438.46402, -451.66434, -415.74384, -444.03302,\n",
              "       -449.64343, -448.45706, -447.862  , -447.8849 , -414.28708,\n",
              "       -405.8289 , -414.1051 , -415.96158, -425.45212, -403.34033,\n",
              "       -434.169  , -452.2124 , -456.17935, -381.5805 , -454.38022,\n",
              "       -437.03818, -508.38065, -440.71924, -440.73105, -525.19977,\n",
              "       -448.62344, -474.65   , -456.05096, -390.37305, -423.79208,\n",
              "       -446.3631 , -457.93448, -436.3282 , -433.23328, -466.81842,\n",
              "       -494.3395 , -430.6762 , -448.18274, -410.33658, -521.504  ,\n",
              "       -453.46445, -436.69876, -493.1862 , -442.59415, -446.97107,\n",
              "       -454.48465, -468.04486, -416.0819 , -446.98584, -461.82983,\n",
              "       -478.63577, -483.07538, -434.9256 , -425.79813, -435.4896 ,\n",
              "       -464.13718, -461.44766, -464.07745, -503.61353, -453.04343,\n",
              "       -409.22214, -432.09225, -500.27402, -531.9213 , -455.4235 ,\n",
              "       -465.1539 , -424.5544 , -512.6878 , -469.68045, -444.93484,\n",
              "       -427.06485, -456.90128, -488.90784, -468.5587 , -456.18542,\n",
              "       -503.9724 , -439.49243, -447.12366, -437.33456, -504.06378,\n",
              "       -514.9054 , -418.71957, -503.28616, -460.4001 , -488.23538,\n",
              "       -432.9396 , -459.55142, -492.041  , -449.0555 , -407.6214 ,\n",
              "       -386.1615 , -463.09274, -458.41788, -447.04764, -405.84958,\n",
              "       -378.2114 , -390.5131 , -470.87045, -470.88403, -413.56833,\n",
              "       -403.73657, -405.75662, -377.2399 , -445.18192, -433.5633 ,\n",
              "       -433.1373 , -390.45312, -434.17294, -446.96768, -448.68094,\n",
              "       -434.1118 , -423.73795, -410.16275, -447.18866, -406.1605 ,\n",
              "       -429.79407, -420.2247 , -446.1366 , -428.3561 , -405.93726,\n",
              "       -420.10004, -389.66595, -514.8218 , -403.9027 , -402.585  ,\n",
              "       -434.36496, -472.72705, -364.07712, -409.9087 , -413.8373 ,\n",
              "       -359.38922, -391.26596, -370.994  , -449.62973, -373.46188,\n",
              "       -427.5882 , -437.44412, -400.4402 , -464.32104, -418.5449 ,\n",
              "       -521.55493, -421.95407, -440.67435, -497.26077, -463.6061 ,\n",
              "       -420.12646, -466.10834, -416.39185, -513.68   , -467.4495 ,\n",
              "       -457.78012, -454.6912 , -488.26974, -461.97726, -460.52005,\n",
              "       -443.02942, -460.19077, -451.02377, -452.5104 , -547.34924,\n",
              "       -470.54938, -426.89627, -421.20755, -443.90628, -408.07852,\n",
              "       -473.36694, -467.0893 , -407.5873 , -422.1737 , -473.6945 ,\n",
              "       -409.21838, -411.14822, -470.8944 , -453.5756 , -462.8844 ,\n",
              "       -513.1609 , -517.8032 , -469.223  , -479.2676 , -443.0332 ,\n",
              "       -410.65417, -513.6494 , -434.86633, -458.29306, -462.08606,\n",
              "       -435.5582 , -470.0053 , -482.287  , -416.1406 , -434.42883,\n",
              "       -474.72525, -501.81232, -413.52936, -444.86087, -443.34912,\n",
              "       -443.18646, -439.98254, -417.20303, -385.19785, -441.78366,\n",
              "       -454.7903 , -402.7788 , -467.03882, -428.71326, -438.01385,\n",
              "       -382.74545, -475.37103, -484.20596, -466.87482, -423.7153 ,\n",
              "       -420.7548 , -423.8322 , -452.5595 , -443.76602, -422.04797,\n",
              "       -392.5137 , -464.00873, -375.1225 , -409.2057 , -459.8976 ,\n",
              "       -431.6872 , -407.01892, -405.38095, -419.83594, -407.77988,\n",
              "       -395.2997 , -393.9526 , -403.3263 , -428.13434, -496.95926,\n",
              "       -421.93167, -471.28308, -473.18396, -399.32123, -413.09387,\n",
              "       -391.08258, -481.6817 , -380.43036, -365.09543, -410.75797,\n",
              "       -433.9074 , -405.67502, -416.07724, -461.75302, -438.8606 ,\n",
              "       -404.93027, -403.1864 , -448.45685, -413.7153 , -421.18414,\n",
              "       -447.04645, -387.3086 , -437.94302, -420.1675 , -421.31525,\n",
              "       -425.45074, -441.02634, -412.16312, -438.37827, -409.21655,\n",
              "       -414.18164, -453.3949 , -406.41467, -400.93872, -468.193  ,\n",
              "       -444.60718, -459.23962, -418.8131 , -460.4038 , -439.90536,\n",
              "       -465.9359 , -445.24417, -421.23648, -484.93997, -446.16428,\n",
              "       -367.194  , -481.92627, -522.2874 , -423.3954 , -502.14194,\n",
              "       -461.70255, -470.61038, -487.77856, -491.2516 , -443.2527 ,\n",
              "       -487.9756 , -446.5495 , -441.62894, -459.56256, -464.8163 ,\n",
              "       -431.91074, -454.7633 , -420.90936, -436.0145 , -489.41888,\n",
              "       -493.87128, -443.65494, -449.66565, -440.50952, -424.6715 ,\n",
              "       -427.8873 , -412.55634, -466.64697, -449.63315, -483.3338 ,\n",
              "       -478.16122, -441.50995, -440.85495, -454.05173, -442.1517 ,\n",
              "       -449.0907 , -462.45517, -469.98242, -405.81763, -432.5484 ,\n",
              "       -434.34827, -407.74323, -419.2449 , -469.46295, -477.94263,\n",
              "       -379.51093, -363.5735 , -414.5706 , -435.86343, -446.7613 ,\n",
              "       -442.37482, -485.03238, -471.95856, -440.10852, -404.7512 ,\n",
              "       -421.0112 , -439.35873, -460.42752, -412.05164, -372.68307,\n",
              "       -372.10165, -429.20193, -393.0507 , -403.01038, -394.76035,\n",
              "       -412.0556 , -332.69867, -411.78372, -414.32397, -428.96658,\n",
              "       -398.8699 , -395.0607 , -476.0034 , -452.18256, -456.82553,\n",
              "       -468.89825, -463.91916, -404.50937, -408.56897, -425.47263,\n",
              "       -423.54108, -393.92816, -437.02637, -412.13974, -471.60022,\n",
              "       -467.06317, -464.36145, -420.7758 , -416.08627, -432.17923,\n",
              "       -392.51147, -542.9345 , -404.80206, -417.062  , -432.88263,\n",
              "       -404.11523, -393.34085, -436.95248, -530.95605, -476.9987 ,\n",
              "       -411.51193, -498.8898 , -457.2986 , -492.2898 , -455.44485,\n",
              "       -441.25778, -411.96274, -457.94778, -524.4419 , -360.47025,\n",
              "       -358.59656, -458.9268 , -542.5666 , -396.28677, -423.83383,\n",
              "       -527.49243, -408.97797, -390.08295, -465.86484, -427.65176,\n",
              "       -462.9359 , -450.64505, -386.80893, -435.4015 , -406.23282,\n",
              "       -479.6645 , -427.612  , -441.11627, -485.59613, -464.9679 ,\n",
              "       -397.74173, -392.4652 , -447.6558 , -475.25198, -506.06097,\n",
              "       -392.73694, -431.76166, -505.94534, -403.50656, -418.6249 ,\n",
              "       -427.36716, -467.15588, -437.9722 ])"
            ]
          },
          "metadata": {},
          "execution_count": 474
        }
      ],
      "source": [
        "df1['0'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 475,
      "id": "72e890ce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72e890ce",
        "outputId": "5518a84e-ba35-41d8-d5ae-4c0890875bff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-482.45233, -491.33237, -433.93875, -451.39062, -447.41296,\n",
              "       -474.31326, -439.48215, -466.99695, -426.0529 , -482.82642,\n",
              "       -446.7974 , -453.8969 , -376.84555, -456.72043, -418.3227 ,\n",
              "       -423.0657 , -399.51852, -402.0103 , -474.8092 , -482.67584,\n",
              "       -456.9332 , -485.50922, -418.001  , -492.80527, -477.71838,\n",
              "       -414.93512, -453.48303, -439.2523 , -446.3131 , -470.00208,\n",
              "       -473.9847 , -448.96103, -401.80896, -345.723  , -412.15396,\n",
              "       -365.9771 , -412.62817, -414.99844, -464.94745, -481.4349 ,\n",
              "       -479.00146, -413.97577, -471.02194, -481.2874 , -441.25516,\n",
              "       -464.69373, -376.26874, -418.1163 , -405.00757, -435.22946,\n",
              "       -470.02222, -377.4015 , -457.7515 , -422.8841 , -446.06177,\n",
              "       -454.1528 , -455.91553, -453.7782 , -423.10638, -439.494  ,\n",
              "       -386.62506, -464.56476, -392.26648, -452.835  , -394.94235,\n",
              "       -389.36047, -412.8278 , -427.86765, -474.5434 , -442.24258,\n",
              "       -417.8694 , -448.38986, -426.17914, -504.21188, -466.36984,\n",
              "       -471.5478 , -421.56473, -429.95325, -484.456  , -428.66187,\n",
              "       -463.29285, -379.56708, -431.52588, -385.76328, -412.06152,\n",
              "       -459.4864 , -378.77286, -462.3047 , -386.1994 , -472.4163 ,\n",
              "       -415.02478, -449.23508, -403.8571 , -451.00165, -408.7152 ,\n",
              "       -426.08667, -467.7443 , -397.55426, -457.2802 , -465.24496,\n",
              "       -477.11566, -422.18552, -491.1511 , -442.5056 , -429.41583,\n",
              "       -478.05527, -526.1957 ])"
            ]
          },
          "metadata": {},
          "execution_count": 475
        }
      ],
      "source": [
        "df2['0'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 476,
      "id": "caeabe41",
      "metadata": {
        "id": "caeabe41"
      },
      "outputs": [],
      "source": [
        "x_train=df1.iloc[:,0:(df1.shape[1]-1)]\n",
        "y_train=df1.iloc[:,-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 477,
      "id": "08d5857c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "08d5857c",
        "outputId": "f176fbf0-0376-4b38-b22d-33ad9d1e1f1e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Unnamed: 0          0          1          2          3          4  \\\n",
              "0             0 -469.48477  88.400730  -7.127512  29.156132   5.335554   \n",
              "1             1 -434.88647  41.972150 -29.416862  18.537344  -4.156565   \n",
              "2             2 -454.89886  45.067265  -0.193278  15.111545   3.080835   \n",
              "3             3 -447.12630  86.114920   4.772520  38.097256   8.324276   \n",
              "4             4 -451.45264  71.335200  12.223010  32.649555   3.997801   \n",
              "..          ...        ...        ...        ...        ...        ...   \n",
              "423         423 -403.50656  30.989262 -12.756512  27.214573 -10.573180   \n",
              "424         424 -418.62490  57.015880   6.383415  47.618423  -8.051490   \n",
              "425         425 -427.36716  51.473750   4.835125  40.900140   6.937289   \n",
              "426         426 -467.15588  52.217026  10.470471  47.414780   8.690019   \n",
              "427         427 -437.97220   5.289146 -22.667547  25.394840 -28.545520   \n",
              "\n",
              "             5          6         7          8  ...       249       250  \\\n",
              "0     7.147227  -6.727572 -8.307674  -3.364513  ...  0.646426  0.635522   \n",
              "1     5.257353 -11.410935 -8.983023 -11.285996  ...  0.600825  0.648380   \n",
              "2     4.204241 -10.440731 -6.615343 -16.249382  ...  0.506934  0.575784   \n",
              "3     8.538317  -4.507682 -7.680664  -7.317249  ...  0.698287  0.615263   \n",
              "4    15.200773  -2.812883 -1.384373  -6.467040  ...  0.696758  0.719856   \n",
              "..         ...        ...       ...        ...  ...       ...       ...   \n",
              "423  -0.137991 -15.470394 -8.865036 -16.384440  ...  0.457819  0.531090   \n",
              "424   4.213936 -15.029120 -2.448467 -10.805821  ...  0.566316  0.569263   \n",
              "425  13.700687  -8.331753 -0.583414  -6.279562  ...  0.671834  0.638155   \n",
              "426  15.601270  -2.032935  4.985774  -7.432734  ...  0.531997  0.523647   \n",
              "427   0.428451 -11.650926 -8.022680 -18.337156  ...  0.510255  0.495079   \n",
              "\n",
              "          251       252       253       254       255       256       257  \\\n",
              "0    0.544438  0.532856 -0.018488  0.011973 -0.011038  0.079758 -0.021044   \n",
              "1    0.675295  0.560000 -0.010576 -0.000228  0.011102 -0.074188  0.012116   \n",
              "2    0.523305  0.423375  0.009395 -0.025547 -0.035554 -0.025885  0.011198   \n",
              "3    0.566964  0.605103 -0.008389 -0.051995 -0.056544 -0.020014  0.013964   \n",
              "4    0.667424  0.570905  0.003044 -0.006101 -0.009038 -0.049699  0.027615   \n",
              "..        ...       ...       ...       ...       ...       ...       ...   \n",
              "423  0.567376  0.470224 -0.005500  0.013583  0.040782  0.003971  0.017743   \n",
              "424  0.551809  0.576764  0.016046 -0.025610  0.025426 -0.094371  0.008999   \n",
              "425  0.574542  0.503516  0.019632  0.046315  0.074801  0.018078  0.022948   \n",
              "426  0.509723  0.498566  0.051532  0.015525 -0.006052  0.018201  0.004361   \n",
              "427  0.539387  0.527394 -0.001086  0.013948  0.013274 -0.056855 -0.001004   \n",
              "\n",
              "          258  \n",
              "0   -0.019445  \n",
              "1   -0.000779  \n",
              "2   -0.000163  \n",
              "3    0.008349  \n",
              "4    0.021319  \n",
              "..        ...  \n",
              "423  0.026657  \n",
              "424 -0.004985  \n",
              "425 -0.016365  \n",
              "426 -0.021649  \n",
              "427 -0.001355  \n",
              "\n",
              "[428 rows x 260 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c690150d-8f71-4e77-b20e-d119b6146107\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>...</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "      <th>257</th>\n",
              "      <th>258</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-469.48477</td>\n",
              "      <td>88.400730</td>\n",
              "      <td>-7.127512</td>\n",
              "      <td>29.156132</td>\n",
              "      <td>5.335554</td>\n",
              "      <td>7.147227</td>\n",
              "      <td>-6.727572</td>\n",
              "      <td>-8.307674</td>\n",
              "      <td>-3.364513</td>\n",
              "      <td>...</td>\n",
              "      <td>0.646426</td>\n",
              "      <td>0.635522</td>\n",
              "      <td>0.544438</td>\n",
              "      <td>0.532856</td>\n",
              "      <td>-0.018488</td>\n",
              "      <td>0.011973</td>\n",
              "      <td>-0.011038</td>\n",
              "      <td>0.079758</td>\n",
              "      <td>-0.021044</td>\n",
              "      <td>-0.019445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>-434.88647</td>\n",
              "      <td>41.972150</td>\n",
              "      <td>-29.416862</td>\n",
              "      <td>18.537344</td>\n",
              "      <td>-4.156565</td>\n",
              "      <td>5.257353</td>\n",
              "      <td>-11.410935</td>\n",
              "      <td>-8.983023</td>\n",
              "      <td>-11.285996</td>\n",
              "      <td>...</td>\n",
              "      <td>0.600825</td>\n",
              "      <td>0.648380</td>\n",
              "      <td>0.675295</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>-0.010576</td>\n",
              "      <td>-0.000228</td>\n",
              "      <td>0.011102</td>\n",
              "      <td>-0.074188</td>\n",
              "      <td>0.012116</td>\n",
              "      <td>-0.000779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>-454.89886</td>\n",
              "      <td>45.067265</td>\n",
              "      <td>-0.193278</td>\n",
              "      <td>15.111545</td>\n",
              "      <td>3.080835</td>\n",
              "      <td>4.204241</td>\n",
              "      <td>-10.440731</td>\n",
              "      <td>-6.615343</td>\n",
              "      <td>-16.249382</td>\n",
              "      <td>...</td>\n",
              "      <td>0.506934</td>\n",
              "      <td>0.575784</td>\n",
              "      <td>0.523305</td>\n",
              "      <td>0.423375</td>\n",
              "      <td>0.009395</td>\n",
              "      <td>-0.025547</td>\n",
              "      <td>-0.035554</td>\n",
              "      <td>-0.025885</td>\n",
              "      <td>0.011198</td>\n",
              "      <td>-0.000163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>-447.12630</td>\n",
              "      <td>86.114920</td>\n",
              "      <td>4.772520</td>\n",
              "      <td>38.097256</td>\n",
              "      <td>8.324276</td>\n",
              "      <td>8.538317</td>\n",
              "      <td>-4.507682</td>\n",
              "      <td>-7.680664</td>\n",
              "      <td>-7.317249</td>\n",
              "      <td>...</td>\n",
              "      <td>0.698287</td>\n",
              "      <td>0.615263</td>\n",
              "      <td>0.566964</td>\n",
              "      <td>0.605103</td>\n",
              "      <td>-0.008389</td>\n",
              "      <td>-0.051995</td>\n",
              "      <td>-0.056544</td>\n",
              "      <td>-0.020014</td>\n",
              "      <td>0.013964</td>\n",
              "      <td>0.008349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>-451.45264</td>\n",
              "      <td>71.335200</td>\n",
              "      <td>12.223010</td>\n",
              "      <td>32.649555</td>\n",
              "      <td>3.997801</td>\n",
              "      <td>15.200773</td>\n",
              "      <td>-2.812883</td>\n",
              "      <td>-1.384373</td>\n",
              "      <td>-6.467040</td>\n",
              "      <td>...</td>\n",
              "      <td>0.696758</td>\n",
              "      <td>0.719856</td>\n",
              "      <td>0.667424</td>\n",
              "      <td>0.570905</td>\n",
              "      <td>0.003044</td>\n",
              "      <td>-0.006101</td>\n",
              "      <td>-0.009038</td>\n",
              "      <td>-0.049699</td>\n",
              "      <td>0.027615</td>\n",
              "      <td>0.021319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>423</th>\n",
              "      <td>423</td>\n",
              "      <td>-403.50656</td>\n",
              "      <td>30.989262</td>\n",
              "      <td>-12.756512</td>\n",
              "      <td>27.214573</td>\n",
              "      <td>-10.573180</td>\n",
              "      <td>-0.137991</td>\n",
              "      <td>-15.470394</td>\n",
              "      <td>-8.865036</td>\n",
              "      <td>-16.384440</td>\n",
              "      <td>...</td>\n",
              "      <td>0.457819</td>\n",
              "      <td>0.531090</td>\n",
              "      <td>0.567376</td>\n",
              "      <td>0.470224</td>\n",
              "      <td>-0.005500</td>\n",
              "      <td>0.013583</td>\n",
              "      <td>0.040782</td>\n",
              "      <td>0.003971</td>\n",
              "      <td>0.017743</td>\n",
              "      <td>0.026657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424</th>\n",
              "      <td>424</td>\n",
              "      <td>-418.62490</td>\n",
              "      <td>57.015880</td>\n",
              "      <td>6.383415</td>\n",
              "      <td>47.618423</td>\n",
              "      <td>-8.051490</td>\n",
              "      <td>4.213936</td>\n",
              "      <td>-15.029120</td>\n",
              "      <td>-2.448467</td>\n",
              "      <td>-10.805821</td>\n",
              "      <td>...</td>\n",
              "      <td>0.566316</td>\n",
              "      <td>0.569263</td>\n",
              "      <td>0.551809</td>\n",
              "      <td>0.576764</td>\n",
              "      <td>0.016046</td>\n",
              "      <td>-0.025610</td>\n",
              "      <td>0.025426</td>\n",
              "      <td>-0.094371</td>\n",
              "      <td>0.008999</td>\n",
              "      <td>-0.004985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>425</th>\n",
              "      <td>425</td>\n",
              "      <td>-427.36716</td>\n",
              "      <td>51.473750</td>\n",
              "      <td>4.835125</td>\n",
              "      <td>40.900140</td>\n",
              "      <td>6.937289</td>\n",
              "      <td>13.700687</td>\n",
              "      <td>-8.331753</td>\n",
              "      <td>-0.583414</td>\n",
              "      <td>-6.279562</td>\n",
              "      <td>...</td>\n",
              "      <td>0.671834</td>\n",
              "      <td>0.638155</td>\n",
              "      <td>0.574542</td>\n",
              "      <td>0.503516</td>\n",
              "      <td>0.019632</td>\n",
              "      <td>0.046315</td>\n",
              "      <td>0.074801</td>\n",
              "      <td>0.018078</td>\n",
              "      <td>0.022948</td>\n",
              "      <td>-0.016365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>426</th>\n",
              "      <td>426</td>\n",
              "      <td>-467.15588</td>\n",
              "      <td>52.217026</td>\n",
              "      <td>10.470471</td>\n",
              "      <td>47.414780</td>\n",
              "      <td>8.690019</td>\n",
              "      <td>15.601270</td>\n",
              "      <td>-2.032935</td>\n",
              "      <td>4.985774</td>\n",
              "      <td>-7.432734</td>\n",
              "      <td>...</td>\n",
              "      <td>0.531997</td>\n",
              "      <td>0.523647</td>\n",
              "      <td>0.509723</td>\n",
              "      <td>0.498566</td>\n",
              "      <td>0.051532</td>\n",
              "      <td>0.015525</td>\n",
              "      <td>-0.006052</td>\n",
              "      <td>0.018201</td>\n",
              "      <td>0.004361</td>\n",
              "      <td>-0.021649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>427</th>\n",
              "      <td>427</td>\n",
              "      <td>-437.97220</td>\n",
              "      <td>5.289146</td>\n",
              "      <td>-22.667547</td>\n",
              "      <td>25.394840</td>\n",
              "      <td>-28.545520</td>\n",
              "      <td>0.428451</td>\n",
              "      <td>-11.650926</td>\n",
              "      <td>-8.022680</td>\n",
              "      <td>-18.337156</td>\n",
              "      <td>...</td>\n",
              "      <td>0.510255</td>\n",
              "      <td>0.495079</td>\n",
              "      <td>0.539387</td>\n",
              "      <td>0.527394</td>\n",
              "      <td>-0.001086</td>\n",
              "      <td>0.013948</td>\n",
              "      <td>0.013274</td>\n",
              "      <td>-0.056855</td>\n",
              "      <td>-0.001004</td>\n",
              "      <td>-0.001355</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>428 rows × 260 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c690150d-8f71-4e77-b20e-d119b6146107')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c690150d-8f71-4e77-b20e-d119b6146107 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c690150d-8f71-4e77-b20e-d119b6146107');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-404ce309-0abd-4db8-89d4-fad0797f303c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-404ce309-0abd-4db8-89d4-fad0797f303c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-404ce309-0abd-4db8-89d4-fad0797f303c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "x_train"
            }
          },
          "metadata": {},
          "execution_count": 477
        }
      ],
      "source": [
        "x_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 478,
      "id": "d2127f10",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2127f10",
        "outputId": "994192be-dfe8-4a1f-c4df-2b46f26bda8f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      1\n",
              "1      2\n",
              "2      0\n",
              "3      1\n",
              "4      3\n",
              "      ..\n",
              "423    4\n",
              "424    5\n",
              "425    3\n",
              "426    3\n",
              "427    2\n",
              "Name: 0.1, Length: 428, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 478
        }
      ],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 479,
      "id": "7311d746",
      "metadata": {
        "id": "7311d746"
      },
      "outputs": [],
      "source": [
        "x_test=df2.iloc[:,0:(df2.shape[1]-1)]\n",
        "y_test=df2.iloc[:,-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 480,
      "id": "ae86f98c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "ae86f98c",
        "outputId": "a3c8dcd4-a633-4145-cb00-725d34f3288d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Unnamed: 0          0           1          2          3          4  \\\n",
              "0             0 -482.45233   62.835957  -0.348998  26.264595   2.978607   \n",
              "1             1 -491.33237   69.272530  12.908265  34.913530   7.287787   \n",
              "2             2 -433.93875   47.439816 -14.131547  33.306270   0.370407   \n",
              "3             3 -451.39062   54.114080 -15.474982  12.761942 -14.734682   \n",
              "4             4 -447.41296  105.425030   0.545184  40.437737   9.768583   \n",
              "..          ...        ...         ...        ...        ...        ...   \n",
              "102         102 -491.15110    8.158493 -39.897430   7.153264 -30.544424   \n",
              "103         103 -442.50560   16.904572 -26.406687  25.167395 -24.619476   \n",
              "104         104 -429.41583   51.232320  -4.127894  38.715740   2.253632   \n",
              "105         105 -478.05527    5.705422 -24.631056  27.622614 -19.235449   \n",
              "106         106 -526.19570   11.317784 -18.942173  29.540787 -28.057306   \n",
              "\n",
              "             5          6          7          8  ...       249       250  \\\n",
              "0     6.900927 -13.006243   0.273391  -9.196591  ...  0.635609  0.675823   \n",
              "1    10.195468  -5.368203   1.112940  -0.885312  ...  0.742611  0.780847   \n",
              "2    -1.469261 -12.553712  -9.810813 -14.061879  ...  0.677009  0.666296   \n",
              "3     3.685768 -11.450237 -12.940584 -20.427963  ...  0.574641  0.607648   \n",
              "4     4.426243 -11.221826  -9.262340  -3.457486  ...  0.744625  0.682722   \n",
              "..         ...        ...        ...        ...  ...       ...       ...   \n",
              "102  10.330252 -19.721329 -12.623847 -14.831120  ...  0.452379  0.444168   \n",
              "103  -4.223975 -19.451237  -7.989911 -21.999931  ...  0.422272  0.407037   \n",
              "104  -2.197497 -15.981533 -14.225066 -17.897070  ...  0.556236  0.589742   \n",
              "105  -7.040019 -13.293165  -9.629133 -14.067036  ...  0.597085  0.590422   \n",
              "106   1.299599 -16.251814  -7.512440 -23.191568  ...  0.497285  0.528861   \n",
              "\n",
              "          251       252       253       254       255       256       257  \\\n",
              "0    0.684034  0.627376 -0.001574  0.012645 -0.027069  0.019313 -0.002252   \n",
              "1    0.746327  0.655106  0.009772  0.000478 -0.041075 -0.024908 -0.000306   \n",
              "2    0.654239  0.584524  0.001197 -0.002374  0.009570  0.005788 -0.000300   \n",
              "3    0.622860  0.615351  0.009293  0.009108  0.034368 -0.004956  0.000385   \n",
              "4    0.580366  0.602522  0.019718 -0.030081 -0.054242  0.018702  0.000313   \n",
              "..        ...       ...       ...       ...       ...       ...       ...   \n",
              "102  0.486394  0.480718  0.005411 -0.005025  0.028497 -0.069591  0.017052   \n",
              "103  0.406521  0.464291  0.000847 -0.014598 -0.031204  0.019990 -0.013624   \n",
              "104  0.609879  0.515997 -0.019026  0.000340 -0.022717 -0.020690 -0.002970   \n",
              "105  0.481957  0.481437 -0.011216 -0.031485 -0.029936  0.042160 -0.013815   \n",
              "106  0.501070  0.496900  0.007856  0.005917 -0.045393 -0.004246 -0.001628   \n",
              "\n",
              "          258  \n",
              "0   -0.006220  \n",
              "1    0.004096  \n",
              "2    0.000001  \n",
              "3    0.004414  \n",
              "4    0.008927  \n",
              "..        ...  \n",
              "102 -0.001056  \n",
              "103 -0.004070  \n",
              "104 -0.001101  \n",
              "105  0.009839  \n",
              "106  0.015051  \n",
              "\n",
              "[107 rows x 260 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-09fed548-8fad-43de-8b45-b4c4812fbd44\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>...</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "      <th>257</th>\n",
              "      <th>258</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-482.45233</td>\n",
              "      <td>62.835957</td>\n",
              "      <td>-0.348998</td>\n",
              "      <td>26.264595</td>\n",
              "      <td>2.978607</td>\n",
              "      <td>6.900927</td>\n",
              "      <td>-13.006243</td>\n",
              "      <td>0.273391</td>\n",
              "      <td>-9.196591</td>\n",
              "      <td>...</td>\n",
              "      <td>0.635609</td>\n",
              "      <td>0.675823</td>\n",
              "      <td>0.684034</td>\n",
              "      <td>0.627376</td>\n",
              "      <td>-0.001574</td>\n",
              "      <td>0.012645</td>\n",
              "      <td>-0.027069</td>\n",
              "      <td>0.019313</td>\n",
              "      <td>-0.002252</td>\n",
              "      <td>-0.006220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>-491.33237</td>\n",
              "      <td>69.272530</td>\n",
              "      <td>12.908265</td>\n",
              "      <td>34.913530</td>\n",
              "      <td>7.287787</td>\n",
              "      <td>10.195468</td>\n",
              "      <td>-5.368203</td>\n",
              "      <td>1.112940</td>\n",
              "      <td>-0.885312</td>\n",
              "      <td>...</td>\n",
              "      <td>0.742611</td>\n",
              "      <td>0.780847</td>\n",
              "      <td>0.746327</td>\n",
              "      <td>0.655106</td>\n",
              "      <td>0.009772</td>\n",
              "      <td>0.000478</td>\n",
              "      <td>-0.041075</td>\n",
              "      <td>-0.024908</td>\n",
              "      <td>-0.000306</td>\n",
              "      <td>0.004096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>-433.93875</td>\n",
              "      <td>47.439816</td>\n",
              "      <td>-14.131547</td>\n",
              "      <td>33.306270</td>\n",
              "      <td>0.370407</td>\n",
              "      <td>-1.469261</td>\n",
              "      <td>-12.553712</td>\n",
              "      <td>-9.810813</td>\n",
              "      <td>-14.061879</td>\n",
              "      <td>...</td>\n",
              "      <td>0.677009</td>\n",
              "      <td>0.666296</td>\n",
              "      <td>0.654239</td>\n",
              "      <td>0.584524</td>\n",
              "      <td>0.001197</td>\n",
              "      <td>-0.002374</td>\n",
              "      <td>0.009570</td>\n",
              "      <td>0.005788</td>\n",
              "      <td>-0.000300</td>\n",
              "      <td>0.000001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>-451.39062</td>\n",
              "      <td>54.114080</td>\n",
              "      <td>-15.474982</td>\n",
              "      <td>12.761942</td>\n",
              "      <td>-14.734682</td>\n",
              "      <td>3.685768</td>\n",
              "      <td>-11.450237</td>\n",
              "      <td>-12.940584</td>\n",
              "      <td>-20.427963</td>\n",
              "      <td>...</td>\n",
              "      <td>0.574641</td>\n",
              "      <td>0.607648</td>\n",
              "      <td>0.622860</td>\n",
              "      <td>0.615351</td>\n",
              "      <td>0.009293</td>\n",
              "      <td>0.009108</td>\n",
              "      <td>0.034368</td>\n",
              "      <td>-0.004956</td>\n",
              "      <td>0.000385</td>\n",
              "      <td>0.004414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>-447.41296</td>\n",
              "      <td>105.425030</td>\n",
              "      <td>0.545184</td>\n",
              "      <td>40.437737</td>\n",
              "      <td>9.768583</td>\n",
              "      <td>4.426243</td>\n",
              "      <td>-11.221826</td>\n",
              "      <td>-9.262340</td>\n",
              "      <td>-3.457486</td>\n",
              "      <td>...</td>\n",
              "      <td>0.744625</td>\n",
              "      <td>0.682722</td>\n",
              "      <td>0.580366</td>\n",
              "      <td>0.602522</td>\n",
              "      <td>0.019718</td>\n",
              "      <td>-0.030081</td>\n",
              "      <td>-0.054242</td>\n",
              "      <td>0.018702</td>\n",
              "      <td>0.000313</td>\n",
              "      <td>0.008927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>102</td>\n",
              "      <td>-491.15110</td>\n",
              "      <td>8.158493</td>\n",
              "      <td>-39.897430</td>\n",
              "      <td>7.153264</td>\n",
              "      <td>-30.544424</td>\n",
              "      <td>10.330252</td>\n",
              "      <td>-19.721329</td>\n",
              "      <td>-12.623847</td>\n",
              "      <td>-14.831120</td>\n",
              "      <td>...</td>\n",
              "      <td>0.452379</td>\n",
              "      <td>0.444168</td>\n",
              "      <td>0.486394</td>\n",
              "      <td>0.480718</td>\n",
              "      <td>0.005411</td>\n",
              "      <td>-0.005025</td>\n",
              "      <td>0.028497</td>\n",
              "      <td>-0.069591</td>\n",
              "      <td>0.017052</td>\n",
              "      <td>-0.001056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>103</td>\n",
              "      <td>-442.50560</td>\n",
              "      <td>16.904572</td>\n",
              "      <td>-26.406687</td>\n",
              "      <td>25.167395</td>\n",
              "      <td>-24.619476</td>\n",
              "      <td>-4.223975</td>\n",
              "      <td>-19.451237</td>\n",
              "      <td>-7.989911</td>\n",
              "      <td>-21.999931</td>\n",
              "      <td>...</td>\n",
              "      <td>0.422272</td>\n",
              "      <td>0.407037</td>\n",
              "      <td>0.406521</td>\n",
              "      <td>0.464291</td>\n",
              "      <td>0.000847</td>\n",
              "      <td>-0.014598</td>\n",
              "      <td>-0.031204</td>\n",
              "      <td>0.019990</td>\n",
              "      <td>-0.013624</td>\n",
              "      <td>-0.004070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>104</td>\n",
              "      <td>-429.41583</td>\n",
              "      <td>51.232320</td>\n",
              "      <td>-4.127894</td>\n",
              "      <td>38.715740</td>\n",
              "      <td>2.253632</td>\n",
              "      <td>-2.197497</td>\n",
              "      <td>-15.981533</td>\n",
              "      <td>-14.225066</td>\n",
              "      <td>-17.897070</td>\n",
              "      <td>...</td>\n",
              "      <td>0.556236</td>\n",
              "      <td>0.589742</td>\n",
              "      <td>0.609879</td>\n",
              "      <td>0.515997</td>\n",
              "      <td>-0.019026</td>\n",
              "      <td>0.000340</td>\n",
              "      <td>-0.022717</td>\n",
              "      <td>-0.020690</td>\n",
              "      <td>-0.002970</td>\n",
              "      <td>-0.001101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>105</td>\n",
              "      <td>-478.05527</td>\n",
              "      <td>5.705422</td>\n",
              "      <td>-24.631056</td>\n",
              "      <td>27.622614</td>\n",
              "      <td>-19.235449</td>\n",
              "      <td>-7.040019</td>\n",
              "      <td>-13.293165</td>\n",
              "      <td>-9.629133</td>\n",
              "      <td>-14.067036</td>\n",
              "      <td>...</td>\n",
              "      <td>0.597085</td>\n",
              "      <td>0.590422</td>\n",
              "      <td>0.481957</td>\n",
              "      <td>0.481437</td>\n",
              "      <td>-0.011216</td>\n",
              "      <td>-0.031485</td>\n",
              "      <td>-0.029936</td>\n",
              "      <td>0.042160</td>\n",
              "      <td>-0.013815</td>\n",
              "      <td>0.009839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>106</td>\n",
              "      <td>-526.19570</td>\n",
              "      <td>11.317784</td>\n",
              "      <td>-18.942173</td>\n",
              "      <td>29.540787</td>\n",
              "      <td>-28.057306</td>\n",
              "      <td>1.299599</td>\n",
              "      <td>-16.251814</td>\n",
              "      <td>-7.512440</td>\n",
              "      <td>-23.191568</td>\n",
              "      <td>...</td>\n",
              "      <td>0.497285</td>\n",
              "      <td>0.528861</td>\n",
              "      <td>0.501070</td>\n",
              "      <td>0.496900</td>\n",
              "      <td>0.007856</td>\n",
              "      <td>0.005917</td>\n",
              "      <td>-0.045393</td>\n",
              "      <td>-0.004246</td>\n",
              "      <td>-0.001628</td>\n",
              "      <td>0.015051</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>107 rows × 260 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-09fed548-8fad-43de-8b45-b4c4812fbd44')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-09fed548-8fad-43de-8b45-b4c4812fbd44 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-09fed548-8fad-43de-8b45-b4c4812fbd44');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c3a3d4d4-96c3-4b03-a9dd-a8e6b913d0e1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c3a3d4d4-96c3-4b03-a9dd-a8e6b913d0e1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c3a3d4d4-96c3-4b03-a9dd-a8e6b913d0e1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "x_test"
            }
          },
          "metadata": {},
          "execution_count": 480
        }
      ],
      "source": [
        "x_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 481,
      "id": "62579094",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62579094",
        "outputId": "aaff384e-7047-4647-9b58-b3129e34bc83"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      0\n",
              "1      3\n",
              "2      2\n",
              "3      2\n",
              "4      5\n",
              "      ..\n",
              "102    2\n",
              "103    4\n",
              "104    6\n",
              "105    0\n",
              "106    2\n",
              "Name: 0.1, Length: 107, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 481
        }
      ],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 482,
      "id": "3beb4ef2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "3beb4ef2",
        "outputId": "5497d042-d52f-4ce7-ab03-4ea7de719116"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: ylabel='count'>"
            ]
          },
          "metadata": {},
          "execution_count": 482
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "sns.countplot(df1['0'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 483,
      "id": "d58a5824",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "d58a5824",
        "outputId": "c7d118bd-988f-4c93-ff97-2c30edf96295"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: ylabel='count'>"
            ]
          },
          "metadata": {},
          "execution_count": 483
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkIAAAGdCAYAAAD+JxxnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABHjklEQVR4nO3dd3gVVf4G8Pf2e9N7r0DoEGpi6CV0EFYEBBYwIEiJglkVUCEKChaaLrhICeiCNAVFCEEpYUXQPJSA7EoRUSIlgJRIkASS7++P/OZsLkQXEAgw7+d55oE798zMmbkzc9975szEICICIiIiIh0ylncFiIiIiMoLgxARERHpFoMQERER6RaDEBEREekWgxARERHpFoMQERER6RaDEBEREekWgxARERHplrm8K3C3FRcX4/jx43B3d4fBYCjv6hAREdENEBH8+uuvCAkJgdF4+9pxdBeEjh8/jvDw8PKuBhEREd2CnJwchIWF3bb56S4Iubu7AyjZkB4eHuVcGyIiIroReXl5CA8PV9/jt4vugpB2OczDw4NBiIiI6D5zu7u1sLM0ERER6RaDEBEREekWgxARERHpFoMQERER6RaDEBEREekWgxARERHpFoMQERER6RaDEBEREekWgxARERHpFoMQERER6Va5BqF//etf6NKlC0JCQmAwGPDJJ5/8z2kyMzNRr1492Gw2VKpUCQsXLrzj9SQiIqIHU7kGofz8fMTGxmLWrFk3VP7IkSPo1KkTWrZsiezsbIwaNQpPPPEE1q9ff4drSkRERA+icv2jqx06dECHDh1uuPzs2bMRHR2NqVOnAgCqVauGrVu3Yvr06WjXrt2dqiYRERE9oO6rPkLbt29HYmKi07h27dph+/btvztNQUEB8vLynAYiIiIioJxbhG7WyZMnERgY6DQuMDAQeXl5+O233+BwOK6bZvLkyXjllVeuG9/spSUw2UrK73yrP+o/94HT+zcy7k5Pd617oU5cF67Lvbgu92KdHrR1uda9UCeui77Wpajgtz8sf6vuqxahWzF27FhcuHBBDTk5OeVdJSIiIrpH3FctQkFBQcjNzXUal5ubCw8PjzJbgwDAZrPBZrPdjeoRERHRfea+ahFKSEjAxo0bncZ98cUXSEhIKKcaERER0f2sXIPQxYsXkZ2djezsbAAlt8dnZ2fj6NGjAEoua/Xv31+VHzp0KH744Qc8//zz2L9/P959910sX74czzzzTHlUn4iIiO5z5RqEduzYgbp166Ju3boAgJSUFNStWxfjx48HAJw4cUKFIgCIjo7G2rVr8cUXXyA2NhZTp07FvHnzeOs8ERER3ZJy7SPUokULiMjvvl/WU6NbtGiB3bt338FaERERkV7cV32EiIiIiG4nBiEiIiLSLQYhIiIi0i0GISIiItItBiEiIiLSLQYhIiIi0i0GISIiItItBiEiIiLSLQYhIiIi0i0GISIiItItBiEiIiLSLQYhIiIi0i0GISIiItItBiEiIiLSLQYhIiIi0i0GISIiItItBiEiIiLSLQYhIiIi0i0GISIiItItBiEiIiLSLQYhIiIi0i0GISIiItItBiEiIiLSLQYhIiIi0i0GISIiItItBiEiIiLSLQYhIiIi0i0GISIiItItBiEiIiLSLQYhIiIi0i0GISIiItItBiEiIiLSLQYhIiIi0i0GISIiItItBiEiIiLSLQYhIiIi0i0GISIiItItBiEiIiLSLQYhIiIi0i0GISIiItItBiEiIiLSLQYhIiIi0i0GISIiItItBiEiIiLSLQYhIiIi0i0GISIiItItBiEiIiLSLQYhIiIi0i0GISIiItItBiEiIiLSLQYhIiIi0i0GISIiItItBiEiIiLSLQYhIiIi0i0GISIiItItBiEiIiLSLQYhIiIi0i0GISIiItItBiEiIiLSLQYhIiIi0q1yD0KzZs1CVFQU7HY74uPjkZWV9YflZ8yYgSpVqsDhcCA8PBzPPPMMLl++fJdqS0RERA+Scg1Cy5YtQ0pKClJTU7Fr1y7ExsaiXbt2OHXqVJnlP/zwQ4wZMwapqan47rvvMH/+fCxbtgwvvPDCXa45ERERPQjKNQhNmzYNgwcPRlJSEqpXr47Zs2fDxcUFaWlpZZbftm0bGjdujD59+iAqKgpt27ZF7969/2crEhEREVFZyi0IFRYWYufOnUhMTPxvZYxGJCYmYvv27WVO06hRI+zcuVMFnx9++AHp6eno2LHj7y6noKAAeXl5TgMRERERAJjLa8FnzpxBUVERAgMDncYHBgZi//79ZU7Tp08fnDlzBk2aNIGI4OrVqxg6dOgfXhqbPHkyXnnlldtadyIiInowlHtn6ZuRmZmJSZMm4d1338WuXbuwcuVKrF27FhMnTvzdacaOHYsLFy6oIScn5y7WmIiIiO5l5dYi5OfnB5PJhNzcXKfxubm5CAoKKnOacePGoV+/fnjiiScAALVq1UJ+fj6GDBmCF198EUbj9bnOZrPBZrPd/hUgIiKi+165tQhZrVbUr18fGzduVOOKi4uxceNGJCQklDnNpUuXrgs7JpMJACAid66yRERE9EAqtxYhAEhJScGAAQPQoEEDxMXFYcaMGcjPz0dSUhIAoH///ggNDcXkyZMBAF26dMG0adNQt25dxMfH4/vvv8e4cePQpUsXFYiIiIiIblS5BqFevXrh9OnTGD9+PE6ePIk6deogIyNDdaA+evSoUwvQSy+9BIPBgJdeegnHjh2Dv78/unTpgtdee628VoGIiIjuY+UahAAgOTkZycnJZb6XmZnp9NpsNiM1NRWpqal3oWZERET0oLuv7hojIiIiup0YhIiIiEi3GISIiIhItxiEiIiISLcYhIiIiEi3GISIiIhItxiEiIiISLcYhIiIiEi3GISIiIhItxiEiIiISLcYhIiIiEi3GISIiIhItxiEiIiISLcYhIiIiEi3GISIiIhItxiEiIiISLcYhIiIiEi3GISIiIhItxiEiIiISLcYhIiIiEi3GISIiIhItxiEiIiISLcYhIiIiEi3GISIiIhItxiEiIiISLcYhIiIiEi3GISIiIhItxiEiIiISLcYhIiIiEi3GISIiIhItxiEiIiISLcYhIiIiEi3GISIiIhItxiEiIiISLcYhIiIiEi3GISIiIhItxiEiIiISLcYhIiIiEi3GISIiIhItxiEiIiISLcYhIiIiEi3GISIiIhItxiEiIiISLcYhIiIiEi3GISIiIhItxiEiIiISLcYhIiIiEi3GISIiIhItxiEiIiISLcYhIiIiEi3GISIiIhItxiEiIiISLcYhIiIiEi3GISIiIhItxiEiIiISLcYhIiIiEi3GISIiIhItxiEiIiISLcYhIiIiEi3yj0IzZo1C1FRUbDb7YiPj0dWVtYflj9//jxGjBiB4OBg2Gw2VK5cGenp6XeptkRERPQgMZfnwpctW4aUlBTMnj0b8fHxmDFjBtq1a4cDBw4gICDguvKFhYVo06YNAgIC8NFHHyE0NBQ//fQTvLy87n7liYiI6L5XrkFo2rRpGDx4MJKSkgAAs2fPxtq1a5GWloYxY8ZcVz4tLQ1nz57Ftm3bYLFYAABRUVF3s8pERET0ACm3S2OFhYXYuXMnEhMT/1sZoxGJiYnYvn17mdOsXr0aCQkJGDFiBAIDA1GzZk1MmjQJRUVFv7ucgoIC5OXlOQ1EREREQDkGoTNnzqCoqAiBgYFO4wMDA3Hy5Mkyp/nhhx/w0UcfoaioCOnp6Rg3bhymTp2KV1999XeXM3nyZHh6eqohPDz8tq4HERER3b/KvbP0zSguLkZAQADmzJmD+vXro1evXnjxxRcxe/bs351m7NixuHDhghpycnLuYo2JiIjoXlZufYT8/PxgMpmQm5vrND43NxdBQUFlThMcHAyLxQKTyaTGVatWDSdPnkRhYSGsVut109hsNthstttbeSIiInoglFuLkNVqRf369bFx40Y1rri4GBs3bkRCQkKZ0zRu3Bjff/89iouL1biDBw8iODi4zBBERERE9EfK9dJYSkoK5s6di/fffx/fffcdhg0bhvz8fHUXWf/+/TF27FhVftiwYTh79ixGjhyJgwcPYu3atZg0aRJGjBhRXqtARERE97FyvX2+V69eOH36NMaPH4+TJ0+iTp06yMjIUB2ojx49CqPxv1ktPDwc69evxzPPPIPatWsjNDQUI0eOxOjRo8trFYiIiOg+Vq5BCACSk5ORnJxc5nuZmZnXjUtISMDXX399h2tFREREenBf3TVGREREdDsxCBEREZFu3VIQatWqFc6fP3/d+Ly8PLRq1erP1omIiIjorrilIJSZmYnCwsLrxl++fBlffvnln64UERER0d1wU52l9+7dq/7/n//8x+lPYRQVFSEjIwOhoaG3r3ZEREREd9BNBaE6derAYDDAYDCUeQnM4XDg73//+22rHBEREdGddFNB6MiRIxARVKhQAVlZWfD391fvWa1WBAQEOP35CyIiIqJ72U0FocjISABw+hMXRERERPerW36g4qFDh7B582acOnXqumA0fvz4P10xIiIiojvtloLQ3LlzMWzYMPj5+SEoKAgGg0G9ZzAYGISIiIjovnBLQejVV1/Fa6+9xr/xRURERPe1W3qO0Llz59CjR4/bXRciIiKiu+qWglCPHj3w+eef3+66EBEREd1Vt3RprFKlShg3bhy+/vpr1KpVCxaLxen9p59++rZUjoiIiOhOuqUgNGfOHLi5uWHLli3YsmWL03sGg4FBiIiIiO4LtxSEjhw5crvrQURERHTX3VIfISIiIqIHwS21CA0cOPAP309LS7ulyhARERHdTbcUhM6dO+f0+sqVK9i3bx/Onz9f5h9jJSIiIroX3VIQWrVq1XXjiouLMWzYMFSsWPFPV4qIiIjobrhtfYSMRiNSUlIwffr02zVLIiIiojvqtnaWPnz4MK5evXo7Z0lERER0x9zSpbGUlBSn1yKCEydOYO3atRgwYMBtqRgRERHRnXZLQWj37t1Or41GI/z9/TF16tT/eUcZERER0b3iloLQ5s2bb3c9iIiIiO66WwpCmtOnT+PAgQMAgCpVqsDf3/+2VIqIiIjobrilztL5+fkYOHAggoOD0axZMzRr1gwhISEYNGgQLl26dLvrSERERHRH3FIQSklJwZYtW/DZZ5/h/PnzOH/+PD799FNs2bIFf/vb3253HYmIiIjuiFu6NPbxxx/jo48+QosWLdS4jh07wuFwoGfPnvjHP/5xu+pHREREdMfcUovQpUuXEBgYeN34gIAAXhojIiKi+8YtBaGEhASkpqbi8uXLatxvv/2GV155BQkJCbetckRERER30i1dGpsxYwbat2+PsLAwxMbGAgD27NkDm82Gzz///LZWkIiIiOhOuaUgVKtWLRw6dAiLFy/G/v37AQC9e/dG37594XA4bmsFiYiIiO6UWwpCkydPRmBgIAYPHuw0Pi0tDadPn8bo0aNvS+WIiIiI7qRb6iP03nvvoWrVqteNr1GjBmbPnv2nK0VERER0N9xSEDp58iSCg4OvG+/v748TJ0786UoRERER3Q23FITCw8Px1VdfXTf+q6++QkhIyJ+uFBEREdHdcEt9hAYPHoxRo0bhypUraNWqFQBg48aNeP755/lkaSIiIrpv3FIQeu655/DLL79g+PDhKCwsBADY7XaMHj0aY8eOva0VJCIiIrpTbikIGQwGvPHGGxg3bhy+++47OBwOxMTEwGaz3e76EREREd0xtxSENG5ubmjYsOHtqgsRERHRXXVLnaWJiIiIHgQMQkRERKRbDEJERESkWwxCREREpFsMQkRERKRbDEJERESkWwxCREREpFsMQkRERKRbDEJERESkWwxCREREpFsMQkRERKRbDEJERESkWwxCREREpFsMQkRERKRbDEJERESkWwxCREREpFsMQkRERKRbDEJERESkW/dEEJo1axaioqJgt9sRHx+PrKysG5pu6dKlMBgM6Nat252tIBERET2Qyj0ILVu2DCkpKUhNTcWuXbsQGxuLdu3a4dSpU3843Y8//ohnn30WTZs2vUs1JSIiogdNuQehadOmYfDgwUhKSkL16tUxe/ZsuLi4IC0t7XenKSoqQt++ffHKK6+gQoUKd7G2RERE9CAp1yBUWFiInTt3IjExUY0zGo1ITEzE9u3bf3e6CRMmICAgAIMGDfqfyygoKEBeXp7TQERERASUcxA6c+YMioqKEBgY6DQ+MDAQJ0+eLHOarVu3Yv78+Zg7d+4NLWPy5Mnw9PRUQ3h4+J+uNxERET0Yyv3S2M349ddf0a9fP8ydOxd+fn43NM3YsWNx4cIFNeTk5NzhWhIREdH9wlyeC/fz84PJZEJubq7T+NzcXAQFBV1X/vDhw/jxxx/RpUsXNa64uBgAYDabceDAAVSsWNFpGpvNBpvNdgdqT0RERPe7cm0RslqtqF+/PjZu3KjGFRcXY+PGjUhISLiufNWqVfHtt98iOztbDQ8//DBatmyJ7OxsXvYiIiKim1KuLUIAkJKSggEDBqBBgwaIi4vDjBkzkJ+fj6SkJABA//79ERoaismTJ8Nut6NmzZpO03t5eQHAdeOJiIiI/pdyD0K9evXC6dOnMX78eJw8eRJ16tRBRkaG6kB99OhRGI33VVcmIiIiuk+UexACgOTkZCQnJ5f5XmZm5h9Ou3DhwttfISIiItIFNrUQERGRbjEIERERkW4xCBEREZFuMQgRERGRbjEIERERkW4xCBEREZFuMQgRERGRbjEIERERkW4xCBEREZFuMQgRERGRbjEIERERkW4xCBEREZFuMQgRERGRbjEIERERkW4xCBEREZFuMQgRERGRbjEIERERkW4xCBEREZFuMQgRERGRbjEIERERkW4xCBEREZFuMQgRERGRbjEIERERkW4xCBEREZFuMQgRERGRbjEIERERkW4xCBEREZFuMQgRERGRbjEIERERkW4xCBEREZFuMQgRERGRbjEIERERkW4xCBEREZFuMQgRERGRbjEIERERkW4xCBEREZFuMQgRERGRbjEIERERkW4xCBEREZFuMQgRERGRbjEIERERkW4xCBEREZFuMQgRERGRbjEIERERkW4xCBEREZFuMQgRERGRbjEIERERkW4xCBEREZFuMQgRERGRbjEIERERkW4xCBEREZFuMQgRERGRbjEIERERkW4xCBEREZFuMQgRERGRbjEIERERkW4xCBEREZFuMQgRERGRbjEIERERkW4xCBEREZFu3RNBaNasWYiKioLdbkd8fDyysrJ+t+zcuXPRtGlTeHt7w9vbG4mJiX9YnoiIiOj3lHsQWrZsGVJSUpCamopdu3YhNjYW7dq1w6lTp8osn5mZid69e2Pz5s3Yvn07wsPD0bZtWxw7duwu15yIiIjud+UehKZNm4bBgwcjKSkJ1atXx+zZs+Hi4oK0tLQyyy9evBjDhw9HnTp1ULVqVcybNw/FxcXYuHHjXa45ERER3e/KNQgVFhZi586dSExMVOOMRiMSExOxffv2G5rHpUuXcOXKFfj4+JT5fkFBAfLy8pwGIiIiIqCcg9CZM2dQVFSEwMBAp/GBgYE4efLkDc1j9OjRCAkJcQpTpU2ePBmenp5qCA8P/9P1JiIiogdDuV8a+zNef/11LF26FKtWrYLdbi+zzNixY3HhwgU15OTk3OVaEhER0b3KXJ4L9/Pzg8lkQm5urtP43NxcBAUF/eG0U6ZMweuvv44NGzagdu3av1vOZrPBZrPdlvoSERHRg6VcW4SsVivq16/v1NFZ6/ickJDwu9O9+eabmDhxIjIyMtCgQYO7UVUiIiJ6AJVrixAApKSkYMCAAWjQoAHi4uIwY8YM5OfnIykpCQDQv39/hIaGYvLkyQCAN954A+PHj8eHH36IqKgo1ZfIzc0Nbm5u5bYeREREdP8p9yDUq1cvnD59GuPHj8fJkydRp04dZGRkqA7UR48ehdH434arf/zjHygsLMSjjz7qNJ/U1FS8/PLLd7PqREREdJ8r9yAEAMnJyUhOTi7zvczMTKfXP/74452vEBEREenCfX3XGBEREdGfwSBEREREusUgRERERLrFIERERES6xSBEREREusUgRERERLrFIERERES6xSBEREREusUgRERERLrFIERERES6xSBEREREusUgRERERLrFIERERES6xSBEREREusUgRERERLrFIERERES6xSBEREREusUgRERERLrFIERERES6xSBEREREusUgRERERLrFIERERES6xSBEREREusUgRERERLrFIERERES6xSBEREREusUgRERERLrFIERERES6xSBEREREusUgRERERLrFIERERES6xSBEREREusUgRERERLrFIERERES6xSBEREREusUgRERERLrFIERERES6xSBEREREusUgRERERLrFIERERES6xSBEREREusUgRERERLrFIERERES6xSBEREREusUgRERERLrFIERERES6xSBEREREusUgRERERLrFIERERES6xSBEREREusUgRERERLrFIERERES6xSBEREREusUgRERERLrFIERERES6xSBEREREusUgRERERLrFIERERES6xSBEREREunVPBKFZs2YhKioKdrsd8fHxyMrK+sPyK1asQNWqVWG321GrVi2kp6ffpZoSERHRg6Tcg9CyZcuQkpKC1NRU7Nq1C7GxsWjXrh1OnTpVZvlt27ahd+/eGDRoEHbv3o1u3bqhW7du2Ldv312uOREREd3vyj0ITZs2DYMHD0ZSUhKqV6+O2bNnw8XFBWlpaWWWf/vtt9G+fXs899xzqFatGiZOnIh69eph5syZd7nmREREdL8zl+fCCwsLsXPnTowdO1aNMxqNSExMxPbt28ucZvv27UhJSXEa165dO3zyySdlli8oKEBBQYF6feHCBQBAUeFvalxeXh6KCn5zmu5Gxt3p6a51L9SJ68J1uRfX5V6s04O2Lte6F+rEddHXumjf2yLyh9PdNClHx44dEwCybds2p/HPPfecxMXFlTmNxWKRDz/80GncrFmzJCAgoMzyqampAoADBw4cOHDg8AAMOTk5tyeE/L9yvzR2p40dOxYXLlxQw7lz53D48GGcP38eOTk5AICcnBxcuHDhutc3Ou52lbnb092LdeK63Jt14rrcm3XiutybdeK63Jl5Hz16FDk5OQgJCcHtVK6Xxvz8/GAymZCbm+s0Pjc3F0FBQWVOExQUdFPlbTYbbDab0zgvLy8AgMFgAAB4eHjAw8NDvX/t6xsdd7vKsE5cl3u1TlyXe7NOXJd7s05cl9s7b09Pz+vG3Q7l2iJktVpRv359bNy4UY0rLi7Gxo0bkZCQUOY0CQkJTuUB4Isvvvjd8kRERES/p1xbhAAgJSUFAwYMQIMGDRAXF4cZM2YgPz8fSUlJAID+/fsjNDQUkydPBgCMHDkSzZs3x9SpU9GpUycsXboUO3bswJw5c8pzNYiIiOg+VO5BqFevXjh9+jTGjx+PkydPok6dOsjIyEBgYCCAkmuCRuN/G64aNWqEDz/8EC+99BJeeOEFxMTE4JNPPkHNmjVvetk2mw2pqanq0tm1r2903O0qwzpxXe7VOnFd7s06cV3uzTpxXe58nW4ng8jtvg+NiIiI6P7wwN81RkRERPR7GISIiIhItxiEiIiISLcYhIiIiEi/butzqu8jM2fOlMjISLHZbFK1alVp0qSJBAcHCwD561//Kg0aNBA3Nzfx9/eXrl27SmpqqtSqVUvc3d3F3d1dHnroIUlPT1fzmzx5cpmPAq9SpYr8/PPP0rdvX/Hx8RG73S4Wi6XMsu7u7mK326VChQoyYcIESU9Pl+joaDGZTAJAwsPDneq5cuVK6d+/v9hsNjWPqlWriru7uwAQPz8/ASABAQFiNBrFaDQKAHn44YclPDxcjEajGAwGASBPPvmkdO7cWVxcXASAVKpU6br6Va1aVRo1aiRms1lNV9YQERGh1tFkMondbhez2SwAxGq1SuvWreXpp58Wf39/NY3NZhOHw6Fet2nTRmJjY9W6GY1GcXd3Fw8PDzEYDGIwGMRqtUrFihWlZs2a4ubmJlarVQCIj4/PdXWqUKGC2i5/NLi4uKh1czgcEhAQoLa/2WwWNzc3tR3NZrNERERIcHCwGmexWMRut4vRaBSLxaIGbf0NBoOYzWaJjY2VESNGSK1atdT8rx0CAgKke/fu4urq+od1dnd3Fy8vL3FxcZG6devKww8/LADEbreLu7u79OjRQ55//nm1nV1dXaVGjRoCQEJDQ9V2adiwofrMwsPDJSEhQQCIh4eH2O128fPzk2rVqgkAefrpp6V9+/aqnmXVq2fPntKyZUux2+1/6nH62ud6s4P2mdzsoH1WfzQ4HA7x8/MTk8kkJpNJDAaDGI1GMZlMYrVaxW63S82aNeWxxx5T628wGMTFxUXc3NzUdjWbzWK1WtWxaDKZxNfXV0JDQ53mW7FiRVm7dq06Lss6/tzc3GT27NkSGhr6h8fn/Txox73FYhGHw6HORUajUSIjIyU1NVWaNGmijimr1Srh4eHqM7DZbOLp6el0nLu5uUlCQoL4+PiocXa7XerWrStRUVFqXiEhIdfVJzw8XGJiYv7n9o6Li1PHscFgEFdXV7Hb7arudevWlX79+qnzr8FgUN81ACQoKEjMZrPY7Xa1z5lMJgkODpawsDCn7VCrVi3ZvHmz1KxZ83f3FS8vL1m4cKFER0f/Yd2tVqvT8devXz+pUKGCU5natWs7fQe9+eabEh0d7VTG09NT/V/7big9jVbGZDKp46hx48YyYsQICQoKUnUs/Z2hDQMGDJC4uDix2+3i5eUlXbt2vek8oMsgtHTpUrFarZKWlib//ve/pX379mKz2WTBggUCQOrWrSsLFiyQffv2SXZ2tnTs2FH8/Pzk448/loMHD8qBAwfkhRdeEIvFIvv27ZOsrCyJioqSgIAA8fX1lRMnTqjh0KFDEhkZKY8//rh888038sMPP8iyZctk+/btqkxSUpIAkEmTJsmRI0dkxYoV4ubmJhUqVBA/Pz+ZOHGiAJBmzZqJ1WqVuXPnqp3SxcVFevToIdOmTROg5It8xIgRAkBeffVVASCPPPKIDB06VIW1gIAAee2112T48OEyY8YMtcN37txZoqKiBCgJQvXr15eRI0fK/PnzBYC4urpK9+7dZdCgQfLuu+8KABkyZIhTGe0kMnr0aMnIyFAnD4fDIStWrJDu3burg9nLy0vmz58vH374ofoi0OqekJAgRqNRhg8fLunp6TJnzhyxWq1iMBjk1VdfleXLl0uzZs3EaDSKj4+PjBo1Sho3bqwO/MaNG0tmZqZs2LBBGjRoIACkW7du8sknn8jHH38sbm5u4uXlJevWrZMNGzZITEyMACWBacWKFfLZZ5+pk+egQYPks88+U18wRqNRZsyYIbGxseLp6SlGo1G6du0qAKRly5bSo0cPMZlMMnLkSImMjJRq1apJixYtxGQyydtvvy0tW7YUd3d3MZlM0r17d2nWrJkAkMDAQDGZTLJo0SLZunWr9O7dWwBI165dZfXq1bJ48WLp0KGDmEwmef311+Xpp58Wo9GoTpLr1q2TQYMGqc+qb9++snfvXrWdLBaLPPLII/L++++LzWYTi8UijRo1UvuFw+GQqKgoefzxx2XWrFnqxPvoo4/KkSNH5IMPPhCHwyEWi0WaNm0qHTp0UCc2Hx8feeKJJ2TdunUSHh4uFSpUEKvVKpMnT5bFixerE3ZSUpKcOHFChgwZIkBJ6PDw8JCvv/5annnmGbUPb9q0STZt2iRhYWFiNBolKipKMjIyJCwsTEJDQ6Vp06YCQFavXi0dO3aUbt26qc8uOjpahZDKlSvL+vXrpU2bNuLm5iYBAQHSqlUrASCVK1eW2rVrS5UqVWTPnj3ywQcfiJubmzRt2lQqVaokmZmZ8uabb8oTTzwhlSpVkj179sizzz4rRqNRatSoIcHBwdK1a1f1ZeXp6SmJiYnSrFkzqV27tvTp00cMBoO0atVKFi1aJNWrVxcXFxfx8vISADJ79myJj48XT09Padu2rcybN09iY2PF399f/Pz8pHv37rJkyRLp2bOnOBwO8fb2Fm9vb/Hz85Pq1auL1WqVTp06SXp6unzzzTfy5ptvisFgkJo1a8oHH3wgH3/8sSQnJ0tAQID06tVLXnrpJQEgvr6+MnPmTMnKypKRI0cKAAkLC5P09HTp2rWr2u9fffVVqVevnoSFhUmfPn3U8b169Wpp0qSJAJCZM2fKihUrxNfXV2rVqqW+CNu0aSMhISGydOlSASB16tQRABIZGSkAZN68eWqbaz+Apk+fLpmZmTJq1CinMpGRkU6h5bHHHlPnMQDy6KOPSps2baRixYoqtLz00kuSnp6uzmfal/myZcskMDBQgJKgvnLlShUu7Xa7zJs3T9avX6++7Js3by7VqlUTb29vFVT/+c9/SlZWlrz22mtq/3/77bclMzNT4uPjBSj5MZ2VlSXDhw9XZSIjI2Xp0qUSFhamzlPvvPOO9OrVS4WCkJAQSUtLk+rVq4vNZlNh/p133pFGjRqp42HSpElSu3Zt8fLyEpPJJFFRUbJw4ULp0aOHWK1WMZlMYrPZ1Ofi6uoqRqNR5s6dK9u2bVPHn7e3t8ydO1eWL18uXbt2FaPRKF5eXvKXv/xFbTeLxeL0Q99gMEjr1q1lzJgxatzDDz+svoO0c3mrVq0kOTlZlWnYsKFYLBapXLmyOvY7dOggAwcOFKDkB5i7u7u8/vrrUrt2bXE4HGI0GqVDhw7q3G4ymWTcuHHqu3Pu3Lni7e0t//jHP+TAgQPy73//W5YtW3bTmUCXQSguLk5GjBihXhcVFUlISIj6sFetWuVU/tSpUwJAtmzZ4jTe29tbZs6cKTExMfLFF19IZGSk+Pn5OZUZPXq0NGnS5A/rExUVJe7u7lJcXKzGde3aVQwGg6xZs0ZERNWrXr168uKLL6pU/9Zbb6lptJ3rnXfeEQCye/fu69ZH2yl/+umn68b5+fnJvn37VBAqnay1k0JpZc3bZrPJhAkTRETkwIEDAkCFjC1btkhRUZF4e3sLAHnllVfUtF999ZUAUCfBQ4cOXbfN582bJwBk06ZNIiKyZ88ep7qfOHFCvX700UfVdPXq1btuXtd+ptqvpxdeeEFERNavX68O6P79+8v58+fVLxl3d3eZN2+efPfdd+r1s88+KwDk3LlzIlKyb8ybN0+WL18uVqtVrly5osZp9dbCmFbvzp07qzIiIvHx8erkXJpWpk6dOmI2m+WDDz5Q+2JoaKj6/EaOHCm//vqr+nUWGRkpw4YNk5iYGBVce/bsKenp6eqzbN68uSrz0ksvicFgkKeeekp+/fVXiYmJkffee0+Fle+//17NNzQ0VE33xRdfiLu7u8TFxanpvvjiC2nevLmqk3aS9/X1VcfM+vXrBShppRIRp21eoUIFERG1zTt27Oi0vVNTUyU2NlZERG3zcePGqXHaNjebzWp716hRQ+Lj41WZ+Ph4eemll5zmde28tW3eqVMnadKkidpPbDabVKpUSdVb+2KNjo5W8zl//ryYTCb1Jbt79+7rzg9ZWVnqS0Nz4cIFVfd9+/ZJZGSkBAUFiY+Pj9N+UbVqVfH393caV3r+Xbt2FVdXV3V8ioj0799fAMjgwYPV9u7QoYMYjUaZM2eO2t49e/ZUP2rOnTsnI0eOlIoVK6pz1vLly1XLVXFxsdreXbp0EZPJJMePHxegpLXW09NTTRcfHy9xcXFO87p23kajUUJDQ8XFxUWSkpLUflKjRg2JjIyUvn37qm1uMBictos2Xmsd3717t3Tq1ElNV3qbR0REqOnatWunzmfaNrdYLOLi4qLK9OrVS+x2u1SsWFGNu3beWrgAIGvWrFHnRO38/eKLL8rVq1dV0OvZs6eqd+mWmN27d8ulS5fEZDJJxYoV5cUXX1T1Lj2dtq8AkPbt20tkZKTUr19fDAaDmk5EpHv37k7TiYhcunRJjdN+2PXu3dvpOweAxMfHi4hIcXGxGjd48GAREadlX1umX79+4unpKefOnROg5Ie9RiujbbdPPvlEhaqQkBD597//7TQfEZErV65IaGjodefHW1HuD1S82woLC7Fz506MHTtWjTMajUhMTMT27dvLnObChQsAAB8fHwBAUVERVqxYgfz8fHz++efo1KkTEhMTAQDnz59HSEgI7HY7EhISkJWVhc6dO6NHjx7YsmULQkNDMXz4cAwePFjVJzc3FzabDYcOHULlypWxZ88efPXVVxAR2O12p7o4HA5s3bpVLUtbrqZy5crYtWvX/9wO2t9bKywsVNtg1KhRqFGjhiqTmZmJgIAAVdbX1xft2rXD7t27ER0d7TQ/7e+/xcTEYPXq1Rg4cCAuX74MAOoP5vn4+Dg9HLN03QMCAgAAJ06cAADk5eU5bfPSy/D390d+fj4WLFiAkJAQHD9+HOPGjXP6e3MbNmyAn58f/Pz8cODAAQDA008/jRMnTqBq1aoYNmyYmv/OnTuxb98+AMDWrVtx9uxZXLx4EVLyQwGPPfYYdu7ciStXrsBgMODSpUtISEhATEwMfH19cf78ebXdioqKsHTpUuTn5yMhIQHbtm2Du7s7PvroI+Tn5yM2Nhbz58+Hn58ffvnlF7zzzjuq3hs3bsRvv/2GyZMnY/fu3fjmm29gMpnw97//HS+88AKqVKmCVq1aIT8/Hx4eHsjOzkaDBg0wffp0tS8GBQXh2LFj6jMbMWIEYmNjceTIERQXFyMzMxOdOnXCwIEDMXz4cJw4cQIzZswAALRo0QIzZsxQZSIjI2EymWA0GjFixAi0bdsWe/fuhcFgQIsWLTBhwgQAgJubGw4cOID33nsPXl5eWL16NX799Vc4HA5UqFABFy9exMSJE9Vn+thjj6GgoAANGzbEt99+i6tXryIkJASXLl0CAPzyyy/qjypeuXJF7UPaceXi4oKff/4ZAFCtWjU4HA64urri8OHDCAkJUZ/Tr7/+ikOHDiE4OFjN22g0ol69egCAn3/+GQUFBbh8+TLMZjOKiopQWFiIEydO4OTJk7DZbPD19YWvry8OHz4Mf39/nDlzBj4+Pti8eTP69++PV155BcXFxSgoKEBAQIA6zgHg8uXLaNSokdOxbzKZ1DHdqlUrXLp0CU2aNFFltL+j1LBhQzXOYrEAAOLi4vDyyy8jJycHIgKj0QibzYaioiJ4eXnhl19+QUJCAoKCgnDq1Ck4HA7YbDYMGDAAXbp0wZo1a+Dm5ob58+dj4MCB8PPzw8qVKwEALVu2VPv4N998Azc3N3z11VcYPHgwwsPDsXbtWjz22GOYP38+CgsLsWjRIqSkpKi/1/jLL79ARDBw4EBcunQJCxYsQEREBNLT0/HYY48hODgYQMm51Gazwd/fXx2bLi4u8PX1RVBQECpXroxvv/0Wzz//PAwGA3bu3Ini4mLk5uaiSZMm2LRpk1pm7dq1sWzZMtSpUwd2ux0GgwEiguLiYhw8eBCVK1fGjz/+CKPRiF9//VWdG6Kjo7Fu3TrUqVMHANT5Mj8/HwcPHkRUVJTaV1u0aIEaNWqgsLAQRUVFuHr1Knx8fODn54cjR47AYDDg6NGj8PHxQUxMDPz8/JCTk4M6deogNzcXa9asgdFoRHFxMS5evKjOiQDg7e2NrVu34qeffoL8/+P89u/fD6Dkb2o1aNAA27ZtU+WvXr2KoqIi2O12bN26FS1atFDv7d+/H4WFhZg9e7Za3vHjxwFAnceOHDmCadOm4cyZM1i3bh0AID09HQEBAYiOjsaoUaMAAPv27cPBgwcBAFlZWfDx8VHfOQDQtWtXAMCRI0fUMVX6fW3dSpcBgD179gAAzp49q+p1reLiYpw9e1b9pQij0YimTZvCxcUFQMmf5froo4+wdu1a+Pr64tixYxAR1K1bVz2Q+a233rr5Byz/6Sh1nzl27JgAkG3btjmNf+655yQuLu66Vo6ioiLp1KmTNG7cWPbu3Suurq5iMpnE09NTnn/+ealZs6b89ttvIlLSqtCxY0fZs2ePZGRkqD4WNptNxo4dK7t27ZL33ntP7Ha7LFy4UEREli1bJkajUUaMGKH6jxgMBpk0aZIkJCRI8+bNVZ1HjhwpRqNRNS0CkOPHj6u6ApBGjRpJp06dymwR+u233wSANG3aVD777DNxdXVVv0RiYmLULzD8f+vPp59+Knv37pW0tDT1a2vKlCmye/du1Xo2ceJEERF54403BIAsWrRI/crUmqk9PT0lPj5eCgoKZNKkSaru127j0s3ebdu2lcaNG6syubm56hqwdq29cuXKEhISIoGBgWo+QMn19IyMDNm7d6+MHz9e1T0tLU127dolI0eOFIPBIPXq1RMRkaFDh4qbm5vEx8erz0wbHnroIfnmm2+crmc3btxY7Qfa5S2thUXbN9auXStbtmxR/Zm0S0raPCwWi7Rt21btU9p+8sILLzgty83NTSZMmCAOh0N9VnPmzJGePXuqpvrSfVpcXV3FxcVFQkNDpU2bNlK9enV1KcjLy0t8fX3V/uru7i7R0dGqxejcuXNSrVo18fX1lZycHImIiJCIiAipXr26Wranp6f4+/tL+/btVSvasGHDJDw8XDw9PSUtLU1dxzebzRIaGirbt2+XUaNGicFgkGbNmomPj49UqVJFzp07p/rLaH3JDAaD2O122bRpk6SkpKj1atGihTquXF1d1Tb68ssvJSMjQ6pWrSp+fn6Snp4u/v7+Ehoa6tRvzMXFRdzd3cXPz0+2bdsmQMmlSE9PT3n11Vdl6NChaj9xd3eXCRMmSN++fcVsNqtL1H/5y18kPDxc9aXSypf+t3379tKrVy/1vtlsdjr2S5ddsmSJ+uw6d+4s27dvl4iICLU9ru1Loc0rODhYzaNZs2YydepUp35arVu3liVLlqjLGyaTSV2OnTFjhjoutZYKb29v6dGjh8yZM0f10dCOQRGRihUrisFgkBUrVggASUtLE5PJJMeOHRMRkdOnT4uvr6/az4GSvpGNGjUSg8GgymnrvnjxYtm7d6+8/PLLqs5Tp06VXbt2qZa+L7/8UkREhg0b5rS+1/Znady4sbpcrY279lxao0YNqVixotqG2nbTygAl/Vy0VhJtcHd3dyoTGxsrjzzyiDrutHItW7Z0qkNsbKzT6/Hjx6tjQhunfabR0dHy5JNPqnk5HA45duyYXL16VX0fAZD169fL1atX1WW86OhoqVu37nXnKzc3N7WuDodDQkNDpW/fvqpfaWBgoFP/T4vFIuvXr1eX+bRtpPXp0foElh46d+4sZ8+elczMTDVOOwdrr7t06SIi/23pN5vNEhkZKZ6enmpcgwYNRERU63/p7VO9enV1ju/Zs6ccOXJEvdbOBdolNXd3d/noo49kx44d0rt3b/H19ZVffvnlZmKB/i6N3WwQGjp0qERGRkpOTo4UFBTIoUOHZMeOHTJ8+HAxGAyycuVKVVZr+tdoTYClm05FRJ566il56KGHRESkbdu2UrduXQkLC5MlS5bI3r175YMPPhAfHx958803Vf8RoORyR9++faVq1ao3HYQKCwulS5cuAkAWL14sFy9elEOHDql+Ub6+vpKbm6vmM3DgwOu2GQDZsGGD0/K0ZvcqVaoIUNJxrXLlyrJ69WrZs2ePxMbGqmm1TrjaCf7abRwbG6uCUHh4uOTk5IhISXNvQECA2O122bNnjxw8eFC2bNmiTi579uxR8wEgc+fOVfPu1q2bWv7333+vylksFhk+fLhcunRJrFareHt7y+OPPy4NGjSQ999/Xz788EPVobj0l15gYKDYbDZJT0+XHTt2SFBQkDgcDtVva/PmzTJmzBjx9fWVGjVqSNOmTVX/Fy8vL3nzzTelevXqqlPs7t271YngkUceEV9fX0lPT1eX2ux2uyqzY8cO8ff3F4fDIa6urjJmzBjp2bOnBAQEiMFgkKlTp8rLL78sLi4uahsbjUZ1fd1oNEqfPn3UtnF1dRWz2awuOX777bdisVikZ8+eEhcXJ+3bt5e4uDhxOByyevVqmTRpkri6uorD4RCHw6Euf8ydO1csFouat3Zp1mw2q89GRFTnVjc3N5kyZYokJyeLu7u7PPLII5KdnS0vv/yyU+DTQqTRaHS6LFu3bl21ftqlsXPnzomHh4dER0dL+/bt5dSpU+Lh4SGvvfaaqnfNmjXFw8NDre+iRYvUa+3krH2ZaM3ttWrVklGjRomHh4c4HA6ZMmWKDB48WABIcHCwZGdny1//+len/eSvf/2r+rHi6urqdOxrx4F2fFosFgkKCpL4+Hjp0qWL1K1bV4xGo7i5uanjUwsX2rwiIyPFZDJJUFCQOo9o4c5kMl23PLPZLFWqVJHk5GR56623xMvLS6pUqSIJCQlSs2ZNcTgcTh1g/f39pUOHDuryhoeHh1SsWFE2b94sAKRVq1bSuXNndWzGxcWJr6+vtG3bVh2bDRs2FIPBoMKUVhfty09EnLa5dmy2bdtW3N3dZcyYMXLp0iXx9PQULy8vdRkpMDBQ9TMsfRNBp06dJCgoSICSwF/6XFo6VC5fvlyGDx8uRqNRhgwZIs2bN5fIyEhxdXUVb29vmT59ukyYMEF9RgMGDJC9e/eKr6+vWK1W8fHxkSVLlsiGDRucApO2PK0P1NChQyUqKkoSExPFxcVFnQ/KCnPa4OPjoz5rk8kk3t7eTgHXZDJJrVq1nDoM16tXTzp16uR0Q4LFYpF27dqpeWkdqLV5aevm6+vrtDwvLy8JCQlxqp/ZbJbmzZs7dX7WwroWWjw8PCQoKEh9xjcThH7++WcV3ICSUPn555+r4NapUyenINSmTRt1OW/x4sWqHpcvXxYRkcuXL4ufn5/Mnj1bbobuglBBQYGYTKbr+gH1799f3W2jvTdixAgJCwuTH3744br5rFq1Su3UWstA6ddXr14VERGr1Sq1a9d2mvbdd9+VkJAQ+fHHH1U/iZkzZzqVmThxolSpUkVERC5evKjq1bNnT/WrSTuZarQk/fjjjzsFoRUrVki3bt2kdu3a1wW96dOnO6Xx0usRGRmpthlQ0nGu9A4GlPzy+9e//uV0sGr9mrTt17NnT2nVqpUMHDhQwsLC1En33LlzTts4IiJCtTJkZ2eLiEheXp4KH999951a9ogRI1RrQum737TXzZs3lxEjRqgDCoBkZGSo5XXs2FH69OkjiYmJapsAkH379jl9Dt7e3hITEyOffvqp+pKoVKmSDBkyREREIiIiJCYmRjp37qzWKS8vTzw9PSUkJES1voiItG7dWoYMGeLUgbD0r0uto+KQIUNUX62IiAi1LBGRnj17qg7a33zzjQCQt99+22le155oy7p7qqw71a4td22ZG70T6drpSh8fZZUpfcy0bt1a/P395emnn1bb3Gw2S8uWLdU2iIiIUF8GWhDKy8sTV1dXiYyMVNu8QYMGMmbMGNUCeO16aHcijhkzRn744Qf1fkxMjIwZM0Zt7z59+kh0dLTTNrdYLE7Htd1uF39/fwkODhYREV9fXwFKgmxpZrPZKQhFRERIQkKC2O12qV27tpw5c0Z8fHxUX5Tp06c71b30drPZbBISEuJ0jF4bhLRWNgDy9ddfqx8A2p2kn3zyiQwaNEjatWunwvzixYslLi5Ohg8fLj/++KMAJT+MtCCkTZeXlycJCQnSqFEjNU6jtQprn622bxkMBhVqv/zyS6djUzsfNm7cWPr06aNCDFDSWTgsLEydJ1u3bi2urq7Spk0biYmJcdrm1/ad0lobtW2uHf9ubm5qm3t4eEhAQIDa5r+3rxqNRrW9tZYid3d3tSztDk4tLHz99ddiNBolLCxMRErO5cePH5dBgwZJQECAtGzZUrZv3y5AyY+/jh07qjLNmjVTnb23bdsmx48fl8LCQtXaqZ2rtO+E119/vczjUTuX9+zZU9q2bas+05CQEKflPf/88+Lh4SEA1Gf94osvXvedk5CQIOfPn1d9lKxWq9SqVUtE/huEevfuLSIihw8fVuNq164tnp6ealxcXJzExMRIv379rptO64x/7TkSKLl7WURk06ZNatz+/fvVZxAXF6f6et4o3T1HyGq1on79+ti4caMaV1xcjI0bNyIhIQEAICJITk7GqlWrsGnTpuv6wwBA69at0bBhQ3Tp0gXZ2dmqv0bfvn2RnZ0Nk8mk+plcvHjRadqDBw8iMjISCxYsQEBAgLreX5rJZEJxcTEAwNXVFQBw8eJFrF+/Xl2j9fLycloPbd5aHwjNW2+9hUOHDmHDhg3XrUe/fv0AlPT/efLJJ5GdnQ0A6NatG9avX6+2GVDSn0i71q8JCAjA/PnzUb9+fQAlfWQMBoPT9vP09MShQ4eQkZGBtLQ0HDlyBCaTCb1791ZlCgoKcPToUdWfKDIyEhcuXEBMTAzOnTuHrKwsVK1a1emz+eyzz2C1WtGwYUP4+/tjzZo1AIBp06YhIiICq1atQmZmJvz8/AAACxYsUMs7duwY9u/fj3/9619o27YtKlasCADXfQ6a5s2bw2w24/Tp0/D19UVBQQEOHDiAo0ePwsPDQ/VlycvLQ9u2bWEwGNCyZUunPl5aX5Jnn30WNpsN0dHRav8BgOnTp6NatWooKCjAuXPnAJT0kSkoKHD6fC9fvozw8HA4HA4AJX+IuPS++NBDD8HHxwedO3fGypUrVR+gwMBANW7VqlUAgMaNG6u+PitXroTD4YCPjw+WLFmC7Oxs1K1bV02zadMmLFu2DEBJX7Tp06cDAEaNGoWYmBhVLi0tDQAQHh6OlStXquPDbrfDbrfjoYcewvLlywEANWvWdDpmRAR5eXmoWLEimjdvDpPJhKtXr6pjU9vm+fn5apvk5eWhdevWKCgoQHJyMux2Oy5evIjDhw8jODgYY8aMwY4dO2C1WuFwODB69GgAwOuvvw6j0Yjg4GBERUWp/SQ3N1ft5wcPHkRwcDBycnJQq1Yttc2vPa69vLyQn5+PqKgobNq0Cb/88otan9L1LCoqgtn8366ZCQkJ2L17NwwGAzZs2ABfX1/4+/vDZDIBKDk+9+7dC7PZDIvFguzsbISEhKBatWrw9PREZGQkAODUqVNl7rfasurXr49q1arhypUrOH78OKxWKwICAtCpUyd1rvn+++8BAKdPn8aOHTvQtWtXvPXWWwCApKQkNU9/f380bdoUbdu2hdVqRYsWLdS8NFofkdTUVGRnZ+PJJ58EAEydOhULFiwAAHzxxReqv09wcLA6H168eBGRkZGYP38+IiIiAJT01bl06ZI6PrX9Qvs8tG0OwOl4KWubX7p0Cbt27UJhYaHa5leuXFGfVb9+/eDp6Qm73Q5PT0+1zc1ms+p7Z7VaERsbC8D5nHH58mW4urri/PnzapsXFxc7ncuDg4Nx9epVnD17Fo899pjqB5mbm4uuXbvC1dUVrq6u+Prrr1W/SIfDAT8/P3Tr1g0nTpzAyy+/jBo1auDcuXPqO+GJJ57At99+i5CQEFitVowfPx4hISF4/vnnsXz5cqxfvx7du3dX+1bp5QUHB2Pfvn24ePEiIiIi1HdIQUGB03eO1WrFmTNnVB8moOR7oXPnzk773fnz5wHA6btT216+vr4AgL1796J+/fpqfyi9Ld944w31+o033kB6ejoAwGKxIDk5GUDJPm2xWGAwGFQf0ytXruDHH39Ux8UNu6nY9IBYunSp2Gw2WbhwofznP/+RpKQkcXNzU82djRo1Ejc3N1m2bJm6TS85OVk+//xzOXLkiOzdu1fGjBkjBoNBPv/8czXfsLAw6d69uxw5ckS++uorSUxMFE9PTzGbzfLaa6/JoUOHZPHixeLi4iIffPCBREREyOjRo2XAgAESGhoqa9askSNHjsjKlStVn4SZM2fKmjVrBCjp+1KxYkVZvXq1ACXNhq6urjJ+/HhZvny5ACXNlFr/He26r3ZL4qJFiwQouZw1a9YsWbBggVPz4ltvvaXuIKpUqZK89957smDBApk9e7ZK3ikpKTJv3jwZPXq0AJCkpCSxWq3qtveKFSuKh4eHuLi4yJIlS+Txxx8Xg8EgNptNUlNT1a+xypUri8FgkOnTp0tGRob4+vqK0WhUd40tXrxYvLy8xGg0yuLFi2Xt2rXy1FNPSWJiori7u8vMmTOlTZs2YrVaxd3dXdavX6/uBoqOjhY3NzdZsmSJLFy4UN2l5uLiIosWLZJRo0aJ0WhUz+xYvHixHD16VDw9PaVGjRqyatUqWbt2rboEqT2ewGKxqF+2r7zyitSuXVs1x/fo0UOAkrubtOWlpqbKypUrZdCgQaqla9y4cdK8eXP1C3XKlCnqWrvWrD5q1CgJCwtTt1n369dPNm/eLEOGDFHLf+KJJ+TgwYMSHh4uYWFhYjAYZOHChTJlyhT1OTz++OPyz3/+U3x8fNSzPh555BHZsWOHJCQkiJubm/Tp00e1BFSvXl0cDof07t1bTpw4IV9//bVERERIr1695KeffpKvvvpKunTpImaz2ekuEe3OkqSkJPn000+lQoUKYrfbxWq1yooVK+TQoUPqDjQAsm7dOiksLJRKlSqJ1WqVli1bSmZmpmops9vtsmrVKhk3bpyaZuLEibJ69WqpUqWKuLm5qUsdCxculAoVKqhLbhkZGTJnzhyJiooSm80mc+fOlY8++kj1JfHw8JAdO3YIANXS+N5778mcOXPUr2GLxSLvvvuuJCcni9VqVZcU3nnnHdmyZYs4HA7VypWSkqIeI4D//5Xr6empHnMAlFwqW7NmjbpMrH2GU6ZMUZ95mzZtZNu2bTJ9+nTVgvLcc8/JqlWr1OVxo9Eor732mgQFBalLFR07dpT3339ftRwAJY+J2LBhgwwYMECNe/jhh+XQoUNqn/bw8JAnn3xSFixYIBaLRUaOHCkhISHquWVNmzaVrKwssVqtEhgYKLt371Z9nHr06CG1atWS6tWry8GDB9UluoyMDPnhhx/kyy+/FIfDIXa7XXJzc6WoqEi1kPTu3Vt27Nghhw8fFn9/f/UojWXLlklISIg0atRI7Ha7Ohf7+/uLl5eXNG3aVDp16iSBgYGqb4jWd6lBgwbi6emp9k3tOFu3bp3UrVtX7VNAyS362nHVunVrycrKknfffVeMRqM4HA5JS0uT1atXq+O6TZs2cuTIEdUHysXFRebPny+LFi1St+HbbDaZM2eO6vOkXc599dVXZeXKlWKxWMTd3V3eeOMN2bJli+qi4OfnJ7NmzVKX/Y1Go7rtv2HDhqqPHQCZNm2a1KpVSz1KYuvWrfL++++Ln5+fREREyPz58+XTTz+Vtm3bisFgkOjoaPnss88kODhYYmNjJSQkRCpUqCALFy6USpUqqe00YsQI2bRpkzp/AyV3jWnfOSaTSUJCQtQ5Qusz9dRTT6nL99o5S3ukitZCP2jQIJkwYYIq07JlS9WSqs3rySefVI8YsFqt4urqKn/7298kNjZWnW9XrFgh69atE6Ckj+I333wjhw8flkWLFonD4RAXFxdZv3697N+/X7W0nT179qYygS6DkIjI3//+d4mIiBCr1erU5+aPBu06sb+/v7Ru3dopBImI+Pv7i6urq1itVgkNDZVevXrJ999/L5999pnUrFlTPbxxzpw56hbQAwcOSF5enowcOVIiIiLUAxVffPFFp1sWOdydofQtqxaLxelSodYRWftytlqt6gT/v4bSlzdsNpskJCRIu3btJDIyUi2zdBkXFxdp3ry5xMTEOD3QMSgoSOx2u5rOx8dHfVm4uLhI7dq1pVOnTuokEhMTI1OnTpVLly5JSEiI2Gw2cXFxkb/85S/qeSb/a9AeAqo9T6Zhw4aqLxxQEqC0Sz6VKlWS5557Tpo0aaKee+Li4iIJCQkSEBAgbm5uUlRUJCIiBw8evG57l76UYzabpXbt2hIREeE07kY/y9Kd0x0OhwQHB0tAQIBapoeHh1MZNzc3qVGjhqqDwWAQf39/8fDwEKPRqI7rjh07Stu2bcXT09PpMptWxmw2S9WqVeW9996TXr16OT1M9Ebrrj04VKvXCy+8oM4j2rxK9wvx8vKSKVOmyNNPP62WY7PZpHnz5mK1WtVzaSpVqqQ6olqtVqlSpYp069ZN3V4eFBQk9evXF29vb9WhV3vO2Y1uc21e69atE5H/PhZB21d8fHzUdh80aJCkpqaqaerUqSNffvmljB07Vl3+XL9+vTzyyCPi5+cnZrNZ7dulH2yqdWr/29/+Jg0bNnR6wOmN1r309nQ4HFK9enV1Ttb67JR+OKa7u7u0aNFCdSzW9hct3Grn8pEjR0rTpk2dLoGXvtTj5uYmQ4cOVY8aAMq+dP17Q+m+UjabTXr06CELFixQXRBK95HS1i0pKUmSkpLUvmk2m6VmzZo3tdw7OWgPVBw+fLgEBgaqfbpy5cri6ekpdrtdqlWrJhMnTpRRo0ZJQECAuLu7S2Ji4nXdG26E4f9PZkRERES6o7s+QkREREQaBiEiIiLSLQYhIiIi0i0GISIiItItBiEiIiLSLQYhIiIi0i0GISIiItItBiEiIiLSLQYhIiIi0i0GISIiItItBiEiIiLSLQYhIiIi0q3/AwxwClqbClTcAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "sns.countplot(df2['0'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 484,
      "id": "f4bf0f42",
      "metadata": {
        "id": "f4bf0f42"
      },
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 485,
      "id": "5bd3a370",
      "metadata": {
        "id": "5bd3a370"
      },
      "outputs": [],
      "source": [
        "# x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 486,
      "id": "7b9f98bc",
      "metadata": {
        "id": "7b9f98bc"
      },
      "outputs": [],
      "source": [
        "# x_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 487,
      "id": "c8d669e4",
      "metadata": {
        "id": "c8d669e4"
      },
      "outputs": [],
      "source": [
        "# y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 488,
      "id": "3e5f00f5",
      "metadata": {
        "id": "3e5f00f5"
      },
      "outputs": [],
      "source": [
        "# x_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 489,
      "id": "6d201bc1",
      "metadata": {
        "id": "6d201bc1"
      },
      "outputs": [],
      "source": [
        "# y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 490,
      "id": "000082a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "000082a7",
        "outputId": "17bafdc1-af43-47c1-8baa-60392e1e8707"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 3, 2, 5, 4, 1, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 490
        }
      ],
      "source": [
        "#Check unique values for y_test\n",
        "y_test.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 491,
      "id": "0c50dd63",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c50dd63",
        "outputId": "481cdaf7-021f-4682-db02-dbbee8152fb2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 0, 3, 4, 5, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 491
        }
      ],
      "source": [
        "#Check unique values for y_train\n",
        "y_train.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 492,
      "id": "2f69e286",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f69e286",
        "outputId": "2e71a9ac-62fa-4a18-b7a9-3daee969f3e7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 1.0726817042606516,\n",
              " 1: 0.9705215419501134,\n",
              " 2: 0.5994397759103641,\n",
              " 3: 1.2228571428571429,\n",
              " 4: 1.1116883116883116,\n",
              " 5: 0.9406593406593406,\n",
              " 6: 1.6984126984126984}"
            ]
          },
          "metadata": {},
          "execution_count": 492
        }
      ],
      "source": [
        "from sklearn.utils import compute_class_weight\n",
        "class_weights = compute_class_weight(\n",
        "                                        class_weight = \"balanced\",\n",
        "                                        classes = np.unique(y_train),\n",
        "                                        y = y_train\n",
        "                                    )\n",
        "class_weights = dict(zip(np.unique(y_train), class_weights))\n",
        "class_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 493,
      "id": "87b553a9",
      "metadata": {
        "id": "87b553a9"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "#Normalize the data\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(x_train)\n",
        "X_train_scalled = scaler.transform(x_train)\n",
        "X_test_scalled = scaler.transform(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 494,
      "id": "7b62969d",
      "metadata": {
        "id": "7b62969d"
      },
      "outputs": [],
      "source": [
        "#Import packages for CNN\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, Conv1D\n",
        "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, BatchNormalization, Flatten, MaxPooling1D\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.regularizers import l2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 495,
      "id": "c598744f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c598744f",
        "outputId": "64a37f2d-3f8d-46ff-8809-b95e223156ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[947, 635, 333, 655]\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "res = []\n",
        "for j in range(4):\n",
        "    res.append(random.randint(300, 1000))\n",
        "# res.sort(reverse=True)\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(260, 1)))\n",
        "model.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='sigmoid'))\n",
        "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# model.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='sigmoid'))\n",
        "# model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Dropout(0.2))\n",
        "\n",
        "# model.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='sigmoid'))\n",
        "# model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Dropout(0.3))\n",
        "##\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "model.add(LSTM(256))\n",
        "\n",
        "\n",
        "model.add(Dense(256, activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(256, activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "##\n",
        "# model.add(Dense(64, activation='sigmoid'))\n",
        "# # model.add(BatchNormalization())\n",
        "# model.add(Dropout(0.3))\n",
        "\n",
        "# model.add(Dense(32, activation='sigmoid'))\n",
        "# # model.add(BatchNormalization())\n",
        "# model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(7, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Q6P4AQ4Mrdt",
        "outputId": "01901bee-9767-49cf-965d-b050aff685ba"
      },
      "id": "_Q6P4AQ4Mrdt",
      "execution_count": 496,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_36 (Conv1D)          (None, 260, 256)          1536      \n",
            "                                                                 \n",
            " max_pooling1d_36 (MaxPooli  (None, 130, 256)          0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_36 (Ba  (None, 130, 256)          1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv1d_37 (Conv1D)          (None, 130, 256)          327936    \n",
            "                                                                 \n",
            " max_pooling1d_37 (MaxPooli  (None, 65, 256)           0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_37 (Ba  (None, 65, 256)           1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_36 (Dropout)        (None, 65, 256)           0         \n",
            "                                                                 \n",
            " conv1d_38 (Conv1D)          (None, 65, 256)           327936    \n",
            "                                                                 \n",
            " max_pooling1d_38 (MaxPooli  (None, 33, 256)           0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_38 (Ba  (None, 33, 256)           1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_37 (Dropout)        (None, 33, 256)           0         \n",
            "                                                                 \n",
            " conv1d_39 (Conv1D)          (None, 33, 128)           163968    \n",
            "                                                                 \n",
            " max_pooling1d_39 (MaxPooli  (None, 17, 128)           0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_39 (Ba  (None, 17, 128)           512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_38 (Dropout)        (None, 17, 128)           0         \n",
            "                                                                 \n",
            " lstm_18 (LSTM)              (None, 17, 256)           394240    \n",
            "                                                                 \n",
            " lstm_19 (LSTM)              (None, 256)               525312    \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dropout_39 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 7)                 1799      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1877895 (7.16 MB)\n",
            "Trainable params: 1876103 (7.16 MB)\n",
            "Non-trainable params: 1792 (7.00 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tempfile\n",
        "initial_weights = os.path.join(tempfile.mkdtemp(), 'initial_weights')\n",
        "model.save_weights(initial_weights)"
      ],
      "metadata": {
        "id": "JCEBlr_YMzuM"
      },
      "id": "JCEBlr_YMzuM",
      "execution_count": 497,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(initial_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8l3JMDetM3-D",
        "outputId": "0732b05b-4957-453d-cc81-574b43a9b33b"
      },
      "id": "8l3JMDetM3-D",
      "execution_count": 498,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7a51fe7c6ad0>"
            ]
          },
          "metadata": {},
          "execution_count": 498
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimiser = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=optimiser,\n",
        "              loss='sparse_categorical_crossentropy',                             #CategoricalCrossentropy\n",
        "              metrics=['SparseCategoricalAccuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WglAV3TMM7KM",
        "outputId": "e7629a47-c8b9-4606-9360-22eb233071a5"
      },
      "id": "WglAV3TMM7KM",
      "execution_count": 499,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_36 (Conv1D)          (None, 260, 256)          1536      \n",
            "                                                                 \n",
            " max_pooling1d_36 (MaxPooli  (None, 130, 256)          0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_36 (Ba  (None, 130, 256)          1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv1d_37 (Conv1D)          (None, 130, 256)          327936    \n",
            "                                                                 \n",
            " max_pooling1d_37 (MaxPooli  (None, 65, 256)           0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_37 (Ba  (None, 65, 256)           1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_36 (Dropout)        (None, 65, 256)           0         \n",
            "                                                                 \n",
            " conv1d_38 (Conv1D)          (None, 65, 256)           327936    \n",
            "                                                                 \n",
            " max_pooling1d_38 (MaxPooli  (None, 33, 256)           0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_38 (Ba  (None, 33, 256)           1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_37 (Dropout)        (None, 33, 256)           0         \n",
            "                                                                 \n",
            " conv1d_39 (Conv1D)          (None, 33, 128)           163968    \n",
            "                                                                 \n",
            " max_pooling1d_39 (MaxPooli  (None, 17, 128)           0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_39 (Ba  (None, 17, 128)           512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_38 (Dropout)        (None, 17, 128)           0         \n",
            "                                                                 \n",
            " lstm_18 (LSTM)              (None, 17, 256)           394240    \n",
            "                                                                 \n",
            " lstm_19 (LSTM)              (None, 256)               525312    \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dropout_39 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 7)                 1799      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1877895 (7.16 MB)\n",
            "Trainable params: 1876103 (7.16 MB)\n",
            "Non-trainable params: 1792 (7.00 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path='cnn_lstm_emodb3.ckpt'\n",
        "checkpoint_dir=os.path.dirname(checkpoint_path)\n",
        "callback1=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, monitor='val_sparse_categorical_accuracy', verbose=1,\n",
        "   save_best_only=True,save_weights_only=True,)\n",
        "callback2=tf.keras.callbacks.EarlyStopping(monitor='val_sparse_categorical_accuracy',min_delta=0, patience=50, verbose=0, mode='auto',baseline=None,restore_best_weights=True)\n",
        "cp_callback=[callback1,callback2]"
      ],
      "metadata": {
        "id": "Kg0_S4kRM8DV"
      },
      "id": "Kg0_S4kRM8DV",
      "execution_count": 500,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train_scalled, y_train, validation_data=(X_test_scalled, y_test), batch_size=64, epochs=900, verbose=1,class_weight=class_weights,callbacks=cp_callback)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giDUxkTnNGXy",
        "outputId": "468151c0-c2cb-40a8-cfea-ba864c13387c"
      },
      "id": "giDUxkTnNGXy",
      "execution_count": 501,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.9303 - sparse_categorical_accuracy: 0.1542\n",
            "Epoch 1: val_sparse_categorical_accuracy improved from -inf to 0.11215, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 8s 357ms/step - loss: 1.9303 - sparse_categorical_accuracy: 0.1542 - val_loss: 1.9428 - val_sparse_categorical_accuracy: 0.1121\n",
            "Epoch 2/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.8865 - sparse_categorical_accuracy: 0.2640\n",
            "Epoch 2: val_sparse_categorical_accuracy did not improve from 0.11215\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 1.8865 - sparse_categorical_accuracy: 0.2640 - val_loss: 1.9400 - val_sparse_categorical_accuracy: 0.1121\n",
            "Epoch 3/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.8423 - sparse_categorical_accuracy: 0.2944\n",
            "Epoch 3: val_sparse_categorical_accuracy did not improve from 0.11215\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 1.8423 - sparse_categorical_accuracy: 0.2944 - val_loss: 1.9384 - val_sparse_categorical_accuracy: 0.1121\n",
            "Epoch 4/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.8087 - sparse_categorical_accuracy: 0.2827\n",
            "Epoch 4: val_sparse_categorical_accuracy did not improve from 0.11215\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 1.8087 - sparse_categorical_accuracy: 0.2827 - val_loss: 1.9379 - val_sparse_categorical_accuracy: 0.1121\n",
            "Epoch 5/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.7694 - sparse_categorical_accuracy: 0.3014\n",
            "Epoch 5: val_sparse_categorical_accuracy did not improve from 0.11215\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 1.7694 - sparse_categorical_accuracy: 0.3014 - val_loss: 1.9375 - val_sparse_categorical_accuracy: 0.1121\n",
            "Epoch 6/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.7218 - sparse_categorical_accuracy: 0.3598\n",
            "Epoch 6: val_sparse_categorical_accuracy did not improve from 0.11215\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 1.7218 - sparse_categorical_accuracy: 0.3598 - val_loss: 1.9380 - val_sparse_categorical_accuracy: 0.1121\n",
            "Epoch 7/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.6605 - sparse_categorical_accuracy: 0.3879\n",
            "Epoch 7: val_sparse_categorical_accuracy improved from 0.11215 to 0.15888, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 55ms/step - loss: 1.6605 - sparse_categorical_accuracy: 0.3879 - val_loss: 1.9395 - val_sparse_categorical_accuracy: 0.1589\n",
            "Epoch 8/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.5859 - sparse_categorical_accuracy: 0.3925\n",
            "Epoch 8: val_sparse_categorical_accuracy improved from 0.15888 to 0.16822, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 54ms/step - loss: 1.5859 - sparse_categorical_accuracy: 0.3925 - val_loss: 1.9454 - val_sparse_categorical_accuracy: 0.1682\n",
            "Epoch 9/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.5074 - sparse_categorical_accuracy: 0.4229\n",
            "Epoch 9: val_sparse_categorical_accuracy improved from 0.16822 to 0.18692, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 49ms/step - loss: 1.5074 - sparse_categorical_accuracy: 0.4229 - val_loss: 1.9615 - val_sparse_categorical_accuracy: 0.1869\n",
            "Epoch 10/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.4174 - sparse_categorical_accuracy: 0.4720\n",
            "Epoch 10: val_sparse_categorical_accuracy did not improve from 0.18692\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 1.4174 - sparse_categorical_accuracy: 0.4720 - val_loss: 1.9826 - val_sparse_categorical_accuracy: 0.1869\n",
            "Epoch 11/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.3247 - sparse_categorical_accuracy: 0.5187\n",
            "Epoch 11: val_sparse_categorical_accuracy did not improve from 0.18692\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 1.3247 - sparse_categorical_accuracy: 0.5187 - val_loss: 2.0087 - val_sparse_categorical_accuracy: 0.1776\n",
            "Epoch 12/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.2288 - sparse_categorical_accuracy: 0.5584\n",
            "Epoch 12: val_sparse_categorical_accuracy did not improve from 0.18692\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 1.2288 - sparse_categorical_accuracy: 0.5584 - val_loss: 2.0200 - val_sparse_categorical_accuracy: 0.1589\n",
            "Epoch 13/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 1.1854 - sparse_categorical_accuracy: 0.5599\n",
            "Epoch 13: val_sparse_categorical_accuracy did not improve from 0.18692\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 1.1796 - sparse_categorical_accuracy: 0.5724 - val_loss: 1.9567 - val_sparse_categorical_accuracy: 0.1495\n",
            "Epoch 14/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.0805 - sparse_categorical_accuracy: 0.5584\n",
            "Epoch 14: val_sparse_categorical_accuracy did not improve from 0.18692\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 1.0805 - sparse_categorical_accuracy: 0.5584 - val_loss: 1.9054 - val_sparse_categorical_accuracy: 0.1495\n",
            "Epoch 15/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.0004 - sparse_categorical_accuracy: 0.6051\n",
            "Epoch 15: val_sparse_categorical_accuracy improved from 0.18692 to 0.28037, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 1.0004 - sparse_categorical_accuracy: 0.6051 - val_loss: 1.8933 - val_sparse_categorical_accuracy: 0.2804\n",
            "Epoch 16/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.9246 - sparse_categorical_accuracy: 0.6192\n",
            "Epoch 16: val_sparse_categorical_accuracy did not improve from 0.28037\n",
            "7/7 [==============================] - 0s 30ms/step - loss: 0.9246 - sparse_categorical_accuracy: 0.6192 - val_loss: 1.9003 - val_sparse_categorical_accuracy: 0.1776\n",
            "Epoch 17/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.8611 - sparse_categorical_accuracy: 0.6752\n",
            "Epoch 17: val_sparse_categorical_accuracy did not improve from 0.28037\n",
            "7/7 [==============================] - 0s 34ms/step - loss: 0.8611 - sparse_categorical_accuracy: 0.6752 - val_loss: 2.0980 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 18/900\n",
            "5/7 [====================>.........] - ETA: 0s - loss: 0.7769 - sparse_categorical_accuracy: 0.7375\n",
            "Epoch 18: val_sparse_categorical_accuracy did not improve from 0.28037\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.7923 - sparse_categorical_accuracy: 0.7173 - val_loss: 2.3190 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 19/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.7208 - sparse_categorical_accuracy: 0.7056\n",
            "Epoch 19: val_sparse_categorical_accuracy did not improve from 0.28037\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 0.7208 - sparse_categorical_accuracy: 0.7056 - val_loss: 2.5183 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 20/900\n",
            "5/7 [====================>.........] - ETA: 0s - loss: 0.6672 - sparse_categorical_accuracy: 0.7625\n",
            "Epoch 20: val_sparse_categorical_accuracy did not improve from 0.28037\n",
            "7/7 [==============================] - 0s 35ms/step - loss: 0.6886 - sparse_categorical_accuracy: 0.7383 - val_loss: 2.6082 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 21/900\n",
            "5/7 [====================>.........] - ETA: 0s - loss: 0.6230 - sparse_categorical_accuracy: 0.7344\n",
            "Epoch 21: val_sparse_categorical_accuracy did not improve from 0.28037\n",
            "7/7 [==============================] - 0s 35ms/step - loss: 0.6248 - sparse_categorical_accuracy: 0.7336 - val_loss: 3.0684 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 22/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.5908 - sparse_categorical_accuracy: 0.7578\n",
            "Epoch 22: val_sparse_categorical_accuracy did not improve from 0.28037\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.5969 - sparse_categorical_accuracy: 0.7500 - val_loss: 3.3866 - val_sparse_categorical_accuracy: 0.2523\n",
            "Epoch 23/900\n",
            "5/7 [====================>.........] - ETA: 0s - loss: 0.5365 - sparse_categorical_accuracy: 0.7625\n",
            "Epoch 23: val_sparse_categorical_accuracy did not improve from 0.28037\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.5308 - sparse_categorical_accuracy: 0.7757 - val_loss: 3.5657 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 24/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.4724 - sparse_categorical_accuracy: 0.8255\n",
            "Epoch 24: val_sparse_categorical_accuracy did not improve from 0.28037\n",
            "7/7 [==============================] - 0s 34ms/step - loss: 0.4814 - sparse_categorical_accuracy: 0.8178 - val_loss: 3.6426 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 25/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.4308 - sparse_categorical_accuracy: 0.8364\n",
            "Epoch 25: val_sparse_categorical_accuracy did not improve from 0.28037\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.4308 - sparse_categorical_accuracy: 0.8364 - val_loss: 3.8684 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 26/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.4086 - sparse_categorical_accuracy: 0.8411\n",
            "Epoch 26: val_sparse_categorical_accuracy did not improve from 0.28037\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.4086 - sparse_categorical_accuracy: 0.8411 - val_loss: 3.8699 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 27/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.3330 - sparse_categorical_accuracy: 0.8692\n",
            "Epoch 27: val_sparse_categorical_accuracy did not improve from 0.28037\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.3330 - sparse_categorical_accuracy: 0.8692 - val_loss: 4.3494 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 28/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.3366 - sparse_categorical_accuracy: 0.8692\n",
            "Epoch 28: val_sparse_categorical_accuracy did not improve from 0.28037\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.3366 - sparse_categorical_accuracy: 0.8692 - val_loss: 4.4044 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 29/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.2665 - sparse_categorical_accuracy: 0.8925\n",
            "Epoch 29: val_sparse_categorical_accuracy did not improve from 0.28037\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.2665 - sparse_categorical_accuracy: 0.8925 - val_loss: 4.8617 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 30/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.2758 - sparse_categorical_accuracy: 0.8785\n",
            "Epoch 30: val_sparse_categorical_accuracy did not improve from 0.28037\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.2758 - sparse_categorical_accuracy: 0.8785 - val_loss: 5.2180 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 31/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.2644 - sparse_categorical_accuracy: 0.8738\n",
            "Epoch 31: val_sparse_categorical_accuracy did not improve from 0.28037\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.2644 - sparse_categorical_accuracy: 0.8738 - val_loss: 4.6926 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 32/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.2550 - sparse_categorical_accuracy: 0.8972\n",
            "Epoch 32: val_sparse_categorical_accuracy did not improve from 0.28037\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.2550 - sparse_categorical_accuracy: 0.8972 - val_loss: 4.2130 - val_sparse_categorical_accuracy: 0.1495\n",
            "Epoch 33/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.2688 - sparse_categorical_accuracy: 0.8855\n",
            "Epoch 33: val_sparse_categorical_accuracy did not improve from 0.28037\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.2688 - sparse_categorical_accuracy: 0.8855 - val_loss: 5.4047 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 34/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.2354 - sparse_categorical_accuracy: 0.9112\n",
            "Epoch 34: val_sparse_categorical_accuracy did not improve from 0.28037\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.2354 - sparse_categorical_accuracy: 0.9112 - val_loss: 4.1862 - val_sparse_categorical_accuracy: 0.1589\n",
            "Epoch 35/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.2108 - sparse_categorical_accuracy: 0.9252\n",
            "Epoch 35: val_sparse_categorical_accuracy did not improve from 0.28037\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.2108 - sparse_categorical_accuracy: 0.9252 - val_loss: 4.5460 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 36/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1820 - sparse_categorical_accuracy: 0.9112\n",
            "Epoch 36: val_sparse_categorical_accuracy did not improve from 0.28037\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.1820 - sparse_categorical_accuracy: 0.9112 - val_loss: 4.8370 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 37/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1967 - sparse_categorical_accuracy: 0.9393\n",
            "Epoch 37: val_sparse_categorical_accuracy did not improve from 0.28037\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.1967 - sparse_categorical_accuracy: 0.9393 - val_loss: 5.9344 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 38/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1724 - sparse_categorical_accuracy: 0.9322\n",
            "Epoch 38: val_sparse_categorical_accuracy did not improve from 0.28037\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.1724 - sparse_categorical_accuracy: 0.9322 - val_loss: 5.6608 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 39/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1459 - sparse_categorical_accuracy: 0.9603\n",
            "Epoch 39: val_sparse_categorical_accuracy did not improve from 0.28037\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.1459 - sparse_categorical_accuracy: 0.9603 - val_loss: 5.1419 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 40/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1114 - sparse_categorical_accuracy: 0.9673\n",
            "Epoch 40: val_sparse_categorical_accuracy did not improve from 0.28037\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.1114 - sparse_categorical_accuracy: 0.9673 - val_loss: 5.5055 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 41/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0916 - sparse_categorical_accuracy: 0.9743\n",
            "Epoch 41: val_sparse_categorical_accuracy did not improve from 0.28037\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0916 - sparse_categorical_accuracy: 0.9743 - val_loss: 5.2409 - val_sparse_categorical_accuracy: 0.1402\n",
            "Epoch 42/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1172 - sparse_categorical_accuracy: 0.9509\n",
            "Epoch 42: val_sparse_categorical_accuracy did not improve from 0.28037\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.1172 - sparse_categorical_accuracy: 0.9509 - val_loss: 5.3953 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 43/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1027 - sparse_categorical_accuracy: 0.9650\n",
            "Epoch 43: val_sparse_categorical_accuracy did not improve from 0.28037\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.1027 - sparse_categorical_accuracy: 0.9650 - val_loss: 5.5051 - val_sparse_categorical_accuracy: 0.1402\n",
            "Epoch 44/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1229 - sparse_categorical_accuracy: 0.9579\n",
            "Epoch 44: val_sparse_categorical_accuracy did not improve from 0.28037\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.1229 - sparse_categorical_accuracy: 0.9579 - val_loss: 5.7760 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 45/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0819 - sparse_categorical_accuracy: 0.9720\n",
            "Epoch 45: val_sparse_categorical_accuracy did not improve from 0.28037\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0819 - sparse_categorical_accuracy: 0.9720 - val_loss: 5.2384 - val_sparse_categorical_accuracy: 0.1402\n",
            "Epoch 46/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0858 - sparse_categorical_accuracy: 0.9720\n",
            "Epoch 46: val_sparse_categorical_accuracy did not improve from 0.28037\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0858 - sparse_categorical_accuracy: 0.9720 - val_loss: 5.5450 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 47/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1468 - sparse_categorical_accuracy: 0.9486\n",
            "Epoch 47: val_sparse_categorical_accuracy did not improve from 0.28037\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.1468 - sparse_categorical_accuracy: 0.9486 - val_loss: 5.5686 - val_sparse_categorical_accuracy: 0.1402\n",
            "Epoch 48/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0912 - sparse_categorical_accuracy: 0.9603\n",
            "Epoch 48: val_sparse_categorical_accuracy did not improve from 0.28037\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0912 - sparse_categorical_accuracy: 0.9603 - val_loss: 6.2516 - val_sparse_categorical_accuracy: 0.1402\n",
            "Epoch 49/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0902 - sparse_categorical_accuracy: 0.9626\n",
            "Epoch 49: val_sparse_categorical_accuracy did not improve from 0.28037\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0902 - sparse_categorical_accuracy: 0.9626 - val_loss: 5.0077 - val_sparse_categorical_accuracy: 0.1495\n",
            "Epoch 50/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0750 - sparse_categorical_accuracy: 0.9813\n",
            "Epoch 50: val_sparse_categorical_accuracy did not improve from 0.28037\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0750 - sparse_categorical_accuracy: 0.9813 - val_loss: 4.9145 - val_sparse_categorical_accuracy: 0.1869\n",
            "Epoch 51/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1045 - sparse_categorical_accuracy: 0.9626\n",
            "Epoch 51: val_sparse_categorical_accuracy did not improve from 0.28037\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.1045 - sparse_categorical_accuracy: 0.9626 - val_loss: 5.1014 - val_sparse_categorical_accuracy: 0.2056\n",
            "Epoch 52/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1250 - sparse_categorical_accuracy: 0.9486\n",
            "Epoch 52: val_sparse_categorical_accuracy improved from 0.28037 to 0.31776, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 52ms/step - loss: 0.1250 - sparse_categorical_accuracy: 0.9486 - val_loss: 4.8673 - val_sparse_categorical_accuracy: 0.3178\n",
            "Epoch 53/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0999 - sparse_categorical_accuracy: 0.9556\n",
            "Epoch 53: val_sparse_categorical_accuracy did not improve from 0.31776\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0999 - sparse_categorical_accuracy: 0.9556 - val_loss: 5.1029 - val_sparse_categorical_accuracy: 0.2056\n",
            "Epoch 54/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0621 - sparse_categorical_accuracy: 0.9860\n",
            "Epoch 54: val_sparse_categorical_accuracy did not improve from 0.31776\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0621 - sparse_categorical_accuracy: 0.9860 - val_loss: 5.1669 - val_sparse_categorical_accuracy: 0.2150\n",
            "Epoch 55/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0579 - sparse_categorical_accuracy: 0.9673\n",
            "Epoch 55: val_sparse_categorical_accuracy did not improve from 0.31776\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0579 - sparse_categorical_accuracy: 0.9673 - val_loss: 5.0816 - val_sparse_categorical_accuracy: 0.2150\n",
            "Epoch 56/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0682 - sparse_categorical_accuracy: 0.9650\n",
            "Epoch 56: val_sparse_categorical_accuracy did not improve from 0.31776\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0682 - sparse_categorical_accuracy: 0.9650 - val_loss: 5.2962 - val_sparse_categorical_accuracy: 0.2150\n",
            "Epoch 57/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0685 - sparse_categorical_accuracy: 0.9696\n",
            "Epoch 57: val_sparse_categorical_accuracy did not improve from 0.31776\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0685 - sparse_categorical_accuracy: 0.9696 - val_loss: 4.9210 - val_sparse_categorical_accuracy: 0.2710\n",
            "Epoch 58/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0559 - sparse_categorical_accuracy: 0.9813\n",
            "Epoch 58: val_sparse_categorical_accuracy did not improve from 0.31776\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0559 - sparse_categorical_accuracy: 0.9813 - val_loss: 4.8981 - val_sparse_categorical_accuracy: 0.2617\n",
            "Epoch 59/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0938 - sparse_categorical_accuracy: 0.9626\n",
            "Epoch 59: val_sparse_categorical_accuracy did not improve from 0.31776\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0938 - sparse_categorical_accuracy: 0.9626 - val_loss: 5.1345 - val_sparse_categorical_accuracy: 0.2523\n",
            "Epoch 60/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0667 - sparse_categorical_accuracy: 0.9790\n",
            "Epoch 60: val_sparse_categorical_accuracy did not improve from 0.31776\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0667 - sparse_categorical_accuracy: 0.9790 - val_loss: 5.1504 - val_sparse_categorical_accuracy: 0.2336\n",
            "Epoch 61/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0716 - sparse_categorical_accuracy: 0.9720\n",
            "Epoch 61: val_sparse_categorical_accuracy did not improve from 0.31776\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0716 - sparse_categorical_accuracy: 0.9720 - val_loss: 5.1605 - val_sparse_categorical_accuracy: 0.2243\n",
            "Epoch 62/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0668 - sparse_categorical_accuracy: 0.9813\n",
            "Epoch 62: val_sparse_categorical_accuracy did not improve from 0.31776\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0668 - sparse_categorical_accuracy: 0.9813 - val_loss: 4.3400 - val_sparse_categorical_accuracy: 0.2897\n",
            "Epoch 63/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0532 - sparse_categorical_accuracy: 0.9790\n",
            "Epoch 63: val_sparse_categorical_accuracy did not improve from 0.31776\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0532 - sparse_categorical_accuracy: 0.9790 - val_loss: 4.8102 - val_sparse_categorical_accuracy: 0.2617\n",
            "Epoch 64/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0706 - sparse_categorical_accuracy: 0.9813\n",
            "Epoch 64: val_sparse_categorical_accuracy did not improve from 0.31776\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0706 - sparse_categorical_accuracy: 0.9813 - val_loss: 4.9304 - val_sparse_categorical_accuracy: 0.2430\n",
            "Epoch 65/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0567 - sparse_categorical_accuracy: 0.9766\n",
            "Epoch 65: val_sparse_categorical_accuracy did not improve from 0.31776\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0567 - sparse_categorical_accuracy: 0.9766 - val_loss: 5.1510 - val_sparse_categorical_accuracy: 0.2336\n",
            "Epoch 66/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0517 - sparse_categorical_accuracy: 0.9813\n",
            "Epoch 66: val_sparse_categorical_accuracy did not improve from 0.31776\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0517 - sparse_categorical_accuracy: 0.9813 - val_loss: 5.1232 - val_sparse_categorical_accuracy: 0.2056\n",
            "Epoch 67/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0728 - sparse_categorical_accuracy: 0.9813\n",
            "Epoch 67: val_sparse_categorical_accuracy did not improve from 0.31776\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0728 - sparse_categorical_accuracy: 0.9813 - val_loss: 5.3260 - val_sparse_categorical_accuracy: 0.2056\n",
            "Epoch 68/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0581 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 68: val_sparse_categorical_accuracy did not improve from 0.31776\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0581 - sparse_categorical_accuracy: 0.9907 - val_loss: 5.5250 - val_sparse_categorical_accuracy: 0.2056\n",
            "Epoch 69/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0386 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 69: val_sparse_categorical_accuracy did not improve from 0.31776\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0386 - sparse_categorical_accuracy: 0.9907 - val_loss: 5.0476 - val_sparse_categorical_accuracy: 0.2056\n",
            "Epoch 70/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0523 - sparse_categorical_accuracy: 0.9790\n",
            "Epoch 70: val_sparse_categorical_accuracy did not improve from 0.31776\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0523 - sparse_categorical_accuracy: 0.9790 - val_loss: 4.5095 - val_sparse_categorical_accuracy: 0.2430\n",
            "Epoch 71/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0427 - sparse_categorical_accuracy: 0.9883\n",
            "Epoch 71: val_sparse_categorical_accuracy did not improve from 0.31776\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0427 - sparse_categorical_accuracy: 0.9883 - val_loss: 4.1876 - val_sparse_categorical_accuracy: 0.2804\n",
            "Epoch 72/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0477 - sparse_categorical_accuracy: 0.9860\n",
            "Epoch 72: val_sparse_categorical_accuracy did not improve from 0.31776\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0477 - sparse_categorical_accuracy: 0.9860 - val_loss: 4.2526 - val_sparse_categorical_accuracy: 0.3084\n",
            "Epoch 73/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0547 - sparse_categorical_accuracy: 0.9790\n",
            "Epoch 73: val_sparse_categorical_accuracy did not improve from 0.31776\n",
            "7/7 [==============================] - 0s 30ms/step - loss: 0.0547 - sparse_categorical_accuracy: 0.9790 - val_loss: 4.2959 - val_sparse_categorical_accuracy: 0.3084\n",
            "Epoch 74/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0323 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 74: val_sparse_categorical_accuracy did not improve from 0.31776\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0323 - sparse_categorical_accuracy: 0.9907 - val_loss: 4.0237 - val_sparse_categorical_accuracy: 0.2804\n",
            "Epoch 75/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0243 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 75: val_sparse_categorical_accuracy improved from 0.31776 to 0.40187, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 53ms/step - loss: 0.0243 - sparse_categorical_accuracy: 0.9953 - val_loss: 3.1760 - val_sparse_categorical_accuracy: 0.4019\n",
            "Epoch 76/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0434 - sparse_categorical_accuracy: 0.9792\n",
            "Epoch 76: val_sparse_categorical_accuracy improved from 0.40187 to 0.42056, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0396 - sparse_categorical_accuracy: 0.9813 - val_loss: 3.1500 - val_sparse_categorical_accuracy: 0.4206\n",
            "Epoch 77/900\n",
            "5/7 [====================>.........] - ETA: 0s - loss: 0.0451 - sparse_categorical_accuracy: 0.9781\n",
            "Epoch 77: val_sparse_categorical_accuracy did not improve from 0.42056\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 0.0408 - sparse_categorical_accuracy: 0.9836 - val_loss: 3.1325 - val_sparse_categorical_accuracy: 0.4112\n",
            "Epoch 78/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0549 - sparse_categorical_accuracy: 0.9870\n",
            "Epoch 78: val_sparse_categorical_accuracy improved from 0.42056 to 0.42991, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0543 - sparse_categorical_accuracy: 0.9860 - val_loss: 3.1552 - val_sparse_categorical_accuracy: 0.4299\n",
            "Epoch 79/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0211 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 79: val_sparse_categorical_accuracy improved from 0.42991 to 0.46729, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0211 - sparse_categorical_accuracy: 0.9930 - val_loss: 2.9698 - val_sparse_categorical_accuracy: 0.4673\n",
            "Epoch 80/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0516 - sparse_categorical_accuracy: 0.9818\n",
            "Epoch 80: val_sparse_categorical_accuracy did not improve from 0.46729\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.0577 - sparse_categorical_accuracy: 0.9813 - val_loss: 2.6754 - val_sparse_categorical_accuracy: 0.4673\n",
            "Epoch 81/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0135 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 81: val_sparse_categorical_accuracy improved from 0.46729 to 0.47664, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 62ms/step - loss: 0.0135 - sparse_categorical_accuracy: 1.0000 - val_loss: 2.3857 - val_sparse_categorical_accuracy: 0.4766\n",
            "Epoch 82/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0228 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 82: val_sparse_categorical_accuracy improved from 0.47664 to 0.51402, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0228 - sparse_categorical_accuracy: 0.9930 - val_loss: 2.3557 - val_sparse_categorical_accuracy: 0.5140\n",
            "Epoch 83/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0365 - sparse_categorical_accuracy: 0.9896\n",
            "Epoch 83: val_sparse_categorical_accuracy improved from 0.51402 to 0.53271, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 0.0333 - sparse_categorical_accuracy: 0.9907 - val_loss: 2.3066 - val_sparse_categorical_accuracy: 0.5327\n",
            "Epoch 84/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0117 - sparse_categorical_accuracy: 0.9948\n",
            "Epoch 84: val_sparse_categorical_accuracy improved from 0.53271 to 0.54206, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 56ms/step - loss: 0.0150 - sparse_categorical_accuracy: 0.9930 - val_loss: 2.2567 - val_sparse_categorical_accuracy: 0.5421\n",
            "Epoch 85/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0296 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 85: val_sparse_categorical_accuracy improved from 0.54206 to 0.57944, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 54ms/step - loss: 0.0296 - sparse_categorical_accuracy: 0.9907 - val_loss: 2.1206 - val_sparse_categorical_accuracy: 0.5794\n",
            "Epoch 86/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0229 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 86: val_sparse_categorical_accuracy did not improve from 0.57944\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0229 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.8871 - val_sparse_categorical_accuracy: 0.5794\n",
            "Epoch 87/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0509 - sparse_categorical_accuracy: 0.9860\n",
            "Epoch 87: val_sparse_categorical_accuracy did not improve from 0.57944\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0509 - sparse_categorical_accuracy: 0.9860 - val_loss: 2.2999 - val_sparse_categorical_accuracy: 0.5607\n",
            "Epoch 88/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0597 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 88: val_sparse_categorical_accuracy did not improve from 0.57944\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0597 - sparse_categorical_accuracy: 0.9907 - val_loss: 2.3792 - val_sparse_categorical_accuracy: 0.5421\n",
            "Epoch 89/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0393 - sparse_categorical_accuracy: 0.9836\n",
            "Epoch 89: val_sparse_categorical_accuracy did not improve from 0.57944\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0393 - sparse_categorical_accuracy: 0.9836 - val_loss: 2.0049 - val_sparse_categorical_accuracy: 0.5701\n",
            "Epoch 90/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0458 - sparse_categorical_accuracy: 0.9790\n",
            "Epoch 90: val_sparse_categorical_accuracy did not improve from 0.57944\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0458 - sparse_categorical_accuracy: 0.9790 - val_loss: 1.8701 - val_sparse_categorical_accuracy: 0.5701\n",
            "Epoch 91/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0217 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 91: val_sparse_categorical_accuracy did not improve from 0.57944\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0217 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.8207 - val_sparse_categorical_accuracy: 0.5794\n",
            "Epoch 92/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0441 - sparse_categorical_accuracy: 0.9860\n",
            "Epoch 92: val_sparse_categorical_accuracy did not improve from 0.57944\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0441 - sparse_categorical_accuracy: 0.9860 - val_loss: 1.8545 - val_sparse_categorical_accuracy: 0.5794\n",
            "Epoch 93/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0143 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 93: val_sparse_categorical_accuracy did not improve from 0.57944\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0143 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.8938 - val_sparse_categorical_accuracy: 0.5607\n",
            "Epoch 94/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0290 - sparse_categorical_accuracy: 0.9860\n",
            "Epoch 94: val_sparse_categorical_accuracy did not improve from 0.57944\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0290 - sparse_categorical_accuracy: 0.9860 - val_loss: 2.0016 - val_sparse_categorical_accuracy: 0.5794\n",
            "Epoch 95/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0426 - sparse_categorical_accuracy: 0.9860\n",
            "Epoch 95: val_sparse_categorical_accuracy improved from 0.57944 to 0.67290, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 54ms/step - loss: 0.0426 - sparse_categorical_accuracy: 0.9860 - val_loss: 1.6168 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 96/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0219 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 96: val_sparse_categorical_accuracy did not improve from 0.67290\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0219 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.6648 - val_sparse_categorical_accuracy: 0.6542\n",
            "Epoch 97/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0673 - sparse_categorical_accuracy: 0.9696\n",
            "Epoch 97: val_sparse_categorical_accuracy did not improve from 0.67290\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0673 - sparse_categorical_accuracy: 0.9696 - val_loss: 1.6052 - val_sparse_categorical_accuracy: 0.6262\n",
            "Epoch 98/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0363 - sparse_categorical_accuracy: 0.9860\n",
            "Epoch 98: val_sparse_categorical_accuracy did not improve from 0.67290\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0363 - sparse_categorical_accuracy: 0.9860 - val_loss: 1.7498 - val_sparse_categorical_accuracy: 0.5888\n",
            "Epoch 99/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0621 - sparse_categorical_accuracy: 0.9790\n",
            "Epoch 99: val_sparse_categorical_accuracy did not improve from 0.67290\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0621 - sparse_categorical_accuracy: 0.9790 - val_loss: 2.0095 - val_sparse_categorical_accuracy: 0.5794\n",
            "Epoch 100/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0398 - sparse_categorical_accuracy: 0.9860\n",
            "Epoch 100: val_sparse_categorical_accuracy did not improve from 0.67290\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0398 - sparse_categorical_accuracy: 0.9860 - val_loss: 1.8708 - val_sparse_categorical_accuracy: 0.6168\n",
            "Epoch 101/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0404 - sparse_categorical_accuracy: 0.9883\n",
            "Epoch 101: val_sparse_categorical_accuracy did not improve from 0.67290\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0404 - sparse_categorical_accuracy: 0.9883 - val_loss: 1.8034 - val_sparse_categorical_accuracy: 0.6168\n",
            "Epoch 102/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0232 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 102: val_sparse_categorical_accuracy did not improve from 0.67290\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0232 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.8632 - val_sparse_categorical_accuracy: 0.6168\n",
            "Epoch 103/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0132 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 103: val_sparse_categorical_accuracy did not improve from 0.67290\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0132 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.8357 - val_sparse_categorical_accuracy: 0.6449\n",
            "Epoch 104/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0158 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 104: val_sparse_categorical_accuracy did not improve from 0.67290\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0158 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.6359 - val_sparse_categorical_accuracy: 0.6542\n",
            "Epoch 105/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0530 - sparse_categorical_accuracy: 0.9860\n",
            "Epoch 105: val_sparse_categorical_accuracy improved from 0.67290 to 0.68224, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 54ms/step - loss: 0.0530 - sparse_categorical_accuracy: 0.9860 - val_loss: 1.5635 - val_sparse_categorical_accuracy: 0.6822\n",
            "Epoch 106/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0282 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 106: val_sparse_categorical_accuracy did not improve from 0.68224\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0282 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.6544 - val_sparse_categorical_accuracy: 0.6542\n",
            "Epoch 107/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0184 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 107: val_sparse_categorical_accuracy did not improve from 0.68224\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0184 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.6240 - val_sparse_categorical_accuracy: 0.6449\n",
            "Epoch 108/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0127 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 108: val_sparse_categorical_accuracy did not improve from 0.68224\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0127 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.5411 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 109/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0195 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 109: val_sparse_categorical_accuracy did not improve from 0.68224\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0195 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.5128 - val_sparse_categorical_accuracy: 0.6636\n",
            "Epoch 110/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0156 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 110: val_sparse_categorical_accuracy improved from 0.68224 to 0.69159, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 52ms/step - loss: 0.0156 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.5726 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 111/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0266 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 111: val_sparse_categorical_accuracy did not improve from 0.69159\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0266 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.6083 - val_sparse_categorical_accuracy: 0.6822\n",
            "Epoch 112/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0095 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 112: val_sparse_categorical_accuracy improved from 0.69159 to 0.70093, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 54ms/step - loss: 0.0095 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.5242 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 113/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0077 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 113: val_sparse_categorical_accuracy improved from 0.70093 to 0.71028, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 52ms/step - loss: 0.0077 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.5272 - val_sparse_categorical_accuracy: 0.7103\n",
            "Epoch 114/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0142 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 114: val_sparse_categorical_accuracy did not improve from 0.71028\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0142 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.4585 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 115/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0063 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 115: val_sparse_categorical_accuracy did not improve from 0.71028\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0063 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.5712 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 116/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0112 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 116: val_sparse_categorical_accuracy improved from 0.71028 to 0.71963, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 52ms/step - loss: 0.0112 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.4345 - val_sparse_categorical_accuracy: 0.7196\n",
            "Epoch 117/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0115 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 117: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0115 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.4588 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 118/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0053 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 118: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0053 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.5729 - val_sparse_categorical_accuracy: 0.6542\n",
            "Epoch 119/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0146 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 119: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0146 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.5301 - val_sparse_categorical_accuracy: 0.6636\n",
            "Epoch 120/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0075 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 120: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0075 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.4188 - val_sparse_categorical_accuracy: 0.7103\n",
            "Epoch 121/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0093 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 121: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0093 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.4405 - val_sparse_categorical_accuracy: 0.6822\n",
            "Epoch 122/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0048 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 122: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0048 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.4876 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 123/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0090 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 123: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0090 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.4900 - val_sparse_categorical_accuracy: 0.6449\n",
            "Epoch 124/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0047 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 124: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0047 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.4564 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 125/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0038 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 125: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0038 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.4527 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 126/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0076 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 126: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0076 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.4823 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 127/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0023 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 127: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0023 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.5245 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 128/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0059 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 128: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0059 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.5478 - val_sparse_categorical_accuracy: 0.6822\n",
            "Epoch 129/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0115 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 129: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 30ms/step - loss: 0.0115 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.6318 - val_sparse_categorical_accuracy: 0.6636\n",
            "Epoch 130/900\n",
            "5/7 [====================>.........] - ETA: 0s - loss: 0.0499 - sparse_categorical_accuracy: 0.9937\n",
            "Epoch 130: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 34ms/step - loss: 0.0421 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.5371 - val_sparse_categorical_accuracy: 0.6636\n",
            "Epoch 131/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0084 - sparse_categorical_accuracy: 0.9974\n",
            "Epoch 131: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.0081 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.8227 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 132/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0184 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 132: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.0184 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.6553 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 133/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0218 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 133: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.0218 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.7224 - val_sparse_categorical_accuracy: 0.6822\n",
            "Epoch 134/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0125 - sparse_categorical_accuracy: 0.9974\n",
            "Epoch 134: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.0114 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.6268 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 135/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0029 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 135: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 0.0029 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.7733 - val_sparse_categorical_accuracy: 0.6636\n",
            "Epoch 136/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0072 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 136: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.0072 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.6492 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 137/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0053 - sparse_categorical_accuracy: 0.9974\n",
            "Epoch 137: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 0.0062 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.5740 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 138/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0041 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 138: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 0.0041 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.5381 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 139/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0235 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 139: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 0.0235 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.7943 - val_sparse_categorical_accuracy: 0.6822\n",
            "Epoch 140/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0168 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 140: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 0.0168 - sparse_categorical_accuracy: 0.9953 - val_loss: 2.0696 - val_sparse_categorical_accuracy: 0.6262\n",
            "Epoch 141/900\n",
            "5/7 [====================>.........] - ETA: 0s - loss: 0.0072 - sparse_categorical_accuracy: 0.9937\n",
            "Epoch 141: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 35ms/step - loss: 0.0214 - sparse_categorical_accuracy: 0.9907 - val_loss: 2.3746 - val_sparse_categorical_accuracy: 0.6168\n",
            "Epoch 142/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0155 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 142: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 36ms/step - loss: 0.0155 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.9344 - val_sparse_categorical_accuracy: 0.6636\n",
            "Epoch 143/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0056 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 143: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 37ms/step - loss: 0.0052 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.6268 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 144/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0169 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 144: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 0.0169 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.7256 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 145/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0309 - sparse_categorical_accuracy: 0.9896\n",
            "Epoch 145: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 0.0287 - sparse_categorical_accuracy: 0.9907 - val_loss: 2.0253 - val_sparse_categorical_accuracy: 0.6449\n",
            "Epoch 146/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0694 - sparse_categorical_accuracy: 0.9790\n",
            "Epoch 146: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0694 - sparse_categorical_accuracy: 0.9790 - val_loss: 1.8126 - val_sparse_categorical_accuracy: 0.6822\n",
            "Epoch 147/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0093 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 147: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0093 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.7945 - val_sparse_categorical_accuracy: 0.6542\n",
            "Epoch 148/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0234 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 148: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0234 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.8941 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 149/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0242 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 149: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0242 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.8523 - val_sparse_categorical_accuracy: 0.6822\n",
            "Epoch 150/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0262 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 150: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0262 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.7133 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 151/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0403 - sparse_categorical_accuracy: 0.9860\n",
            "Epoch 151: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0403 - sparse_categorical_accuracy: 0.9860 - val_loss: 1.8395 - val_sparse_categorical_accuracy: 0.6636\n",
            "Epoch 152/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0420 - sparse_categorical_accuracy: 0.9813\n",
            "Epoch 152: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0420 - sparse_categorical_accuracy: 0.9813 - val_loss: 1.8130 - val_sparse_categorical_accuracy: 0.6636\n",
            "Epoch 153/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0117 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 153: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0117 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.7855 - val_sparse_categorical_accuracy: 0.6636\n",
            "Epoch 154/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0146 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 154: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0146 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.6678 - val_sparse_categorical_accuracy: 0.6636\n",
            "Epoch 155/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0317 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 155: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0317 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.6584 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 156/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0312 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 156: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0312 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.8471 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 157/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0282 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 157: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0282 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.7139 - val_sparse_categorical_accuracy: 0.7103\n",
            "Epoch 158/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0251 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 158: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0251 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.7569 - val_sparse_categorical_accuracy: 0.6542\n",
            "Epoch 159/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0355 - sparse_categorical_accuracy: 0.9813\n",
            "Epoch 159: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0355 - sparse_categorical_accuracy: 0.9813 - val_loss: 1.7795 - val_sparse_categorical_accuracy: 0.6542\n",
            "Epoch 160/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0280 - sparse_categorical_accuracy: 0.9896\n",
            "Epoch 160: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 34ms/step - loss: 0.0271 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.6959 - val_sparse_categorical_accuracy: 0.6636\n",
            "Epoch 161/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0246 - sparse_categorical_accuracy: 0.9883\n",
            "Epoch 161: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.0246 - sparse_categorical_accuracy: 0.9883 - val_loss: 1.7531 - val_sparse_categorical_accuracy: 0.6636\n",
            "Epoch 162/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0249 - sparse_categorical_accuracy: 0.9896\n",
            "Epoch 162: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 0.0228 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.9695 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 163/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0568 - sparse_categorical_accuracy: 0.9860\n",
            "Epoch 163: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 0.0568 - sparse_categorical_accuracy: 0.9860 - val_loss: 1.8234 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 164/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0227 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 164: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 0.0227 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.6937 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 165/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0365 - sparse_categorical_accuracy: 0.9883\n",
            "Epoch 165: val_sparse_categorical_accuracy improved from 0.71963 to 0.72897, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0365 - sparse_categorical_accuracy: 0.9883 - val_loss: 1.6332 - val_sparse_categorical_accuracy: 0.7290\n",
            "Epoch 166/900\n",
            "5/7 [====================>.........] - ETA: 0s - loss: 0.0363 - sparse_categorical_accuracy: 0.9906\n",
            "Epoch 166: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 34ms/step - loss: 0.0296 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.6329 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 167/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0102 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 167: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 0.0102 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.6461 - val_sparse_categorical_accuracy: 0.7103\n",
            "Epoch 168/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0167 - sparse_categorical_accuracy: 0.9948\n",
            "Epoch 168: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 34ms/step - loss: 0.0154 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.5683 - val_sparse_categorical_accuracy: 0.7196\n",
            "Epoch 169/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0056 - sparse_categorical_accuracy: 0.9974\n",
            "Epoch 169: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 34ms/step - loss: 0.0051 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.6079 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 170/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0062 - sparse_categorical_accuracy: 0.9974\n",
            "Epoch 170: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 34ms/step - loss: 0.0082 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.5182 - val_sparse_categorical_accuracy: 0.6822\n",
            "Epoch 171/900\n",
            "5/7 [====================>.........] - ETA: 0s - loss: 0.0035 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 171: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 34ms/step - loss: 0.0032 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.5208 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 172/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0064 - sparse_categorical_accuracy: 0.9974\n",
            "Epoch 172: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 0.0060 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.5251 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 173/900\n",
            "5/7 [====================>.........] - ETA: 0s - loss: 0.0138 - sparse_categorical_accuracy: 0.9969\n",
            "Epoch 173: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 35ms/step - loss: 0.0229 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.5907 - val_sparse_categorical_accuracy: 0.6636\n",
            "Epoch 174/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0105 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 174: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.0105 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.6271 - val_sparse_categorical_accuracy: 0.6822\n",
            "Epoch 175/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0245 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 175: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0245 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.7589 - val_sparse_categorical_accuracy: 0.6542\n",
            "Epoch 176/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0082 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 176: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0082 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.8376 - val_sparse_categorical_accuracy: 0.6449\n",
            "Epoch 177/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0131 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 177: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0131 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.4784 - val_sparse_categorical_accuracy: 0.6822\n",
            "Epoch 178/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0146 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 178: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0146 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.5242 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 179/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0078 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 179: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0078 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.6523 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 180/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0026 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 180: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0026 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.6729 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 181/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0022 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 181: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0022 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.6813 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 182/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0163 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 182: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0163 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.6350 - val_sparse_categorical_accuracy: 0.7103\n",
            "Epoch 183/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0110 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 183: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0110 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.6556 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 184/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0143 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 184: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 30ms/step - loss: 0.0143 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.7127 - val_sparse_categorical_accuracy: 0.6542\n",
            "Epoch 185/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0100 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 185: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0100 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.7903 - val_sparse_categorical_accuracy: 0.6822\n",
            "Epoch 186/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0215 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 186: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0215 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.6673 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 187/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0062 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 187: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0062 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.7345 - val_sparse_categorical_accuracy: 0.7103\n",
            "Epoch 188/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0024 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 188: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0024 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.7351 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 189/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0034 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 189: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 30ms/step - loss: 0.0034 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.6869 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 190/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0043 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 190: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0043 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.6583 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 191/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0196 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 191: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0196 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.6810 - val_sparse_categorical_accuracy: 0.6822\n",
            "Epoch 192/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0313 - sparse_categorical_accuracy: 0.9870\n",
            "Epoch 192: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 34ms/step - loss: 0.0283 - sparse_categorical_accuracy: 0.9883 - val_loss: 1.8183 - val_sparse_categorical_accuracy: 0.6636\n",
            "Epoch 193/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0109 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 193: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 0.0109 - sparse_categorical_accuracy: 0.9930 - val_loss: 2.0335 - val_sparse_categorical_accuracy: 0.6636\n",
            "Epoch 194/900\n",
            "5/7 [====================>.........] - ETA: 0s - loss: 0.0188 - sparse_categorical_accuracy: 0.9937\n",
            "Epoch 194: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 34ms/step - loss: 0.0224 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.6939 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 195/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0235 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 195: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 0.0235 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.7257 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 196/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0245 - sparse_categorical_accuracy: 0.9948\n",
            "Epoch 196: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 0.0220 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.6300 - val_sparse_categorical_accuracy: 0.7196\n",
            "Epoch 197/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0116 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 197: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 30ms/step - loss: 0.0116 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.7685 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 198/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0100 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 198: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 34ms/step - loss: 0.0100 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.8689 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 199/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0093 - sparse_categorical_accuracy: 0.9974\n",
            "Epoch 199: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 35ms/step - loss: 0.0118 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.5845 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 200/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0043 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 200: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 0.0043 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.5224 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 201/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0114 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 201: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 0.0114 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.5687 - val_sparse_categorical_accuracy: 0.7290\n",
            "Epoch 202/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0121 - sparse_categorical_accuracy: 0.9974\n",
            "Epoch 202: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.0110 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.5838 - val_sparse_categorical_accuracy: 0.6822\n",
            "Epoch 203/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0277 - sparse_categorical_accuracy: 0.9922\n",
            "Epoch 203: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.0258 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.6189 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 204/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0323 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 204: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 34ms/step - loss: 0.0323 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.4483 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 205/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0160 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 205: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.0160 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.7145 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 206/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0176 - sparse_categorical_accuracy: 0.9948\n",
            "Epoch 206: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 0.0164 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.8550 - val_sparse_categorical_accuracy: 0.6636\n",
            "Epoch 207/900\n",
            "5/7 [====================>.........] - ETA: 0s - loss: 0.0458 - sparse_categorical_accuracy: 0.9844\n",
            "Epoch 207: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 0.0349 - sparse_categorical_accuracy: 0.9883 - val_loss: 1.4868 - val_sparse_categorical_accuracy: 0.7290\n",
            "Epoch 208/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0119 - sparse_categorical_accuracy: 0.9922\n",
            "Epoch 208: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 0.0107 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.4858 - val_sparse_categorical_accuracy: 0.7290\n",
            "Epoch 209/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0128 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 209: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0128 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.5151 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 210/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0116 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 210: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0116 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.5980 - val_sparse_categorical_accuracy: 0.7103\n",
            "Epoch 211/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0044 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 211: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0044 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.6757 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 212/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0060 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 212: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0060 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.6531 - val_sparse_categorical_accuracy: 0.7103\n",
            "Epoch 213/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0026 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 213: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0026 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.6063 - val_sparse_categorical_accuracy: 0.7196\n",
            "Epoch 214/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0035 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 214: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0035 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.5775 - val_sparse_categorical_accuracy: 0.7290\n",
            "Epoch 215/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0194 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 215: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 0.0194 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.4905 - val_sparse_categorical_accuracy: 0.7196\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 502,
      "id": "1b676d1c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "1b676d1c",
        "outputId": "9dfa5a42-af6c-4323-f9b4-e3bc64b941f1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Plot model accuracy over ephocs\n",
        "plt.plot(history.history['sparse_categorical_accuracy'])\n",
        "plt.plot(history.history['val_sparse_categorical_accuracy'])\n",
        "plt.ylim(0, 1)\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 503,
      "id": "df8c9ade",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df8c9ade",
        "outputId": "304866ad-0d66-4697-81e0-98ebf51ce014"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7a51ff31db40>"
            ]
          },
          "metadata": {},
          "execution_count": 503
        }
      ],
      "source": [
        "model.load_weights(checkpoint_path) #to load model with highest accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 504,
      "id": "2f08c66c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f08c66c",
        "outputId": "85049927-f096-40ba-8976-812d1790f74e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 7ms/step - loss: 1.6332 - sparse_categorical_accuracy: 0.7290\n",
            "Pre-training accuracy: 72.8972%\n"
          ]
        }
      ],
      "source": [
        "# Calculate pre-training accuracy\n",
        "score = model.evaluate(X_test_scalled, y_test, verbose=1)\n",
        "accuracy = 100*score[1]\n",
        "\n",
        "print(\"Pre-training accuracy: %.4f%%\" % accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 505,
      "id": "1bdfcfc8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bdfcfc8",
        "outputId": "8e2d0569-a69d-4287-fc97-f3288bd5becc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy:  1.0\n",
            "Testing Accuracy:  0.7289719581604004\n"
          ]
        }
      ],
      "source": [
        "# Evaluating the model on the training and testing set\n",
        "score = model.evaluate(X_train_scalled, y_train, verbose=0)\n",
        "print(\"Training Accuracy: \", score[1])\n",
        "\n",
        "score = model.evaluate(X_test_scalled, y_test, verbose=0)\n",
        "print(\"Testing Accuracy: \", score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 506,
      "id": "eaac1550",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaac1550",
        "outputId": "d7104066-c2aa-467b-c232-d1efdd3c13ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 1s 5ms/step\n"
          ]
        }
      ],
      "source": [
        "#Get predictions from model\n",
        "y_test_predictions = model.predict(X_test_scalled) # it will give the prediction data of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 507,
      "id": "8a9df249",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a9df249",
        "outputId": "db40ef53-33d6-48d9-e86f-89998a655190"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(107, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 507
        }
      ],
      "source": [
        "y_test_predictions.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 508,
      "id": "dda064ae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dda064ae",
        "outputId": "0bb15316-3178-46cc-81f0-f82ceadaea8d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9.97451842e-01, 4.94421402e-05, 1.61163887e-04, 2.53955591e-06,\n",
              "        1.19194250e-04, 1.52257940e-04, 2.06351560e-03],\n",
              "       [9.45459135e-07, 2.27173041e-05, 9.30974275e-05, 9.96654510e-01,\n",
              "        2.69154063e-03, 5.31735259e-06, 5.31850732e-04],\n",
              "       [9.84661747e-03, 9.06066707e-05, 5.45320660e-03, 1.74177723e-04,\n",
              "        9.82326448e-01, 3.71116221e-06, 2.10523326e-03],\n",
              "       [3.98076780e-04, 4.56579016e-07, 9.99582708e-01, 1.31493175e-06,\n",
              "        9.99517215e-06, 5.20213121e-07, 6.93859693e-06],\n",
              "       [5.47017680e-06, 9.72770452e-01, 2.12646069e-06, 2.20306581e-04,\n",
              "        1.27394815e-05, 2.69244742e-02, 6.44793690e-05],\n",
              "       [9.98758478e-05, 1.62816917e-07, 9.99855518e-01, 2.72777970e-06,\n",
              "        3.06107358e-05, 2.10126430e-07, 1.09161556e-05],\n",
              "       [4.66461461e-06, 2.58006480e-06, 8.18391345e-06, 1.98375583e-05,\n",
              "        9.99957800e-01, 4.59819951e-08, 6.92652065e-06],\n",
              "       [9.95962441e-01, 7.05556071e-04, 1.62456654e-05, 2.16722901e-06,\n",
              "        8.85257978e-05, 3.05695808e-03, 1.68110404e-04],\n",
              "       [4.71647596e-04, 1.03688865e-06, 9.99289155e-01, 3.54572421e-06,\n",
              "        2.21739814e-04, 1.73555952e-06, 1.10698220e-05],\n",
              "       [1.82193384e-01, 1.83641721e-04, 3.02844524e-01, 1.36110198e-03,\n",
              "        4.65904355e-01, 3.82411854e-05, 4.74747345e-02],\n",
              "       [1.41068494e-06, 9.98788059e-01, 3.09418908e-07, 8.30566132e-05,\n",
              "        1.37961270e-05, 1.04006880e-03, 7.33238485e-05],\n",
              "       [1.86900841e-08, 1.20465404e-06, 5.16464843e-06, 9.99908566e-01,\n",
              "        7.72936910e-05, 1.57035387e-07, 7.69638245e-06],\n",
              "       [2.34545703e-04, 4.18653227e-02, 6.18410486e-05, 4.89829690e-04,\n",
              "        1.33539972e-04, 9.56961095e-01, 2.53851438e-04],\n",
              "       [1.49747432e-08, 6.17194564e-06, 3.24611779e-06, 9.99939203e-01,\n",
              "        4.45508340e-05, 2.27014937e-07, 6.62153707e-06],\n",
              "       [9.32984874e-02, 1.45334797e-03, 4.28556959e-04, 3.06644244e-04,\n",
              "        1.62177763e-04, 8.19312394e-01, 8.50383863e-02],\n",
              "       [1.25845225e-04, 9.44987237e-01, 1.33424446e-05, 2.55567575e-04,\n",
              "        1.18248958e-04, 5.28970174e-02, 1.60283362e-03],\n",
              "       [9.47508868e-03, 3.55117628e-03, 3.52433883e-04, 2.47582328e-04,\n",
              "        3.52297298e-04, 5.75016618e-01, 4.11004841e-01],\n",
              "       [2.45726928e-01, 4.98146343e-04, 4.22170386e-04, 5.23082454e-05,\n",
              "        1.48429628e-03, 1.69788441e-03, 7.50118375e-01],\n",
              "       [2.58303044e-06, 9.93264437e-01, 4.63290746e-07, 4.33260138e-05,\n",
              "        1.14171098e-05, 6.66420860e-03, 1.35298278e-05],\n",
              "       [9.96084332e-01, 5.65010487e-06, 3.75746773e-03, 1.55299369e-06,\n",
              "        2.98750329e-05, 1.11886540e-04, 9.20575258e-06],\n",
              "       [1.21341316e-06, 9.94337618e-01, 1.74118739e-07, 2.04433727e-05,\n",
              "        4.73678847e-06, 5.62821515e-03, 7.62329364e-06],\n",
              "       [2.55183142e-04, 3.30259326e-07, 9.99704301e-01, 3.61511820e-06,\n",
              "        2.14032552e-05, 5.95054473e-07, 1.45912300e-05],\n",
              "       [3.54151161e-05, 1.44590638e-04, 1.13178135e-06, 3.06164065e-06,\n",
              "        7.73934175e-07, 9.99801934e-01, 1.32649093e-05],\n",
              "       [1.66457639e-07, 5.63904405e-06, 1.59784267e-05, 9.99844909e-01,\n",
              "        1.21369521e-04, 1.18970831e-06, 1.06784955e-05],\n",
              "       [2.57009547e-02, 5.67805819e-06, 9.72622216e-01, 5.15314823e-05,\n",
              "        1.94517648e-04, 1.61297485e-05, 1.40902749e-03],\n",
              "       [1.04441904e-06, 9.98422146e-01, 1.82917438e-06, 8.60073313e-04,\n",
              "        8.82770928e-06, 6.96137082e-04, 9.84407779e-06],\n",
              "       [2.67055526e-07, 9.99417186e-01, 2.59437655e-07, 1.00780810e-04,\n",
              "        3.31920614e-06, 4.76310728e-04, 1.90497656e-06],\n",
              "       [2.66188385e-06, 9.60435748e-01, 1.04107303e-05, 3.85287255e-02,\n",
              "        5.44936775e-05, 6.35911012e-04, 3.32186522e-04],\n",
              "       [7.93735206e-04, 1.97295003e-06, 9.99143839e-01, 5.46343063e-06,\n",
              "        2.71995250e-05, 3.42848443e-06, 2.43037484e-05],\n",
              "       [9.63602543e-01, 4.41402481e-05, 2.85287807e-03, 1.37961561e-05,\n",
              "        7.10230670e-05, 2.82333436e-04, 3.31333317e-02],\n",
              "       [3.94742489e-01, 1.94988737e-04, 4.42449091e-04, 1.89830862e-05,\n",
              "        2.44605675e-04, 3.86139931e-04, 6.03970349e-01],\n",
              "       [1.55737057e-01, 1.36811540e-01, 1.35694980e-03, 5.57423523e-03,\n",
              "        6.78520918e-01, 7.98180047e-03, 1.40174329e-02],\n",
              "       [8.35816445e-06, 9.87721478e-07, 4.55091031e-05, 1.12832804e-05,\n",
              "        9.99929905e-01, 3.78352013e-07, 3.54932467e-06],\n",
              "       [4.89531249e-06, 4.95106178e-06, 3.11499956e-04, 2.54801242e-04,\n",
              "        9.99143481e-01, 1.71204340e-06, 2.78763502e-04],\n",
              "       [7.17271132e-06, 5.33815546e-06, 1.59083174e-05, 1.01943997e-05,\n",
              "        1.92072930e-05, 2.57087431e-05, 9.99916434e-01],\n",
              "       [5.03652791e-06, 1.08204037e-03, 2.96491082e-04, 1.97794259e-01,\n",
              "        8.00437093e-01, 6.19645070e-05, 3.23202781e-04],\n",
              "       [8.61185799e-06, 1.08947759e-04, 8.19953812e-07, 4.25493772e-06,\n",
              "        5.55564554e-07, 9.99871492e-01, 5.26574922e-06],\n",
              "       [1.03130753e-06, 7.11260554e-07, 4.43114004e-06, 6.51780329e-06,\n",
              "        9.99986768e-01, 2.07619184e-08, 6.29140402e-07],\n",
              "       [9.68478143e-01, 8.67718791e-06, 3.10291369e-02, 8.19985144e-06,\n",
              "        2.54302198e-04, 2.65748931e-05, 1.94976528e-04],\n",
              "       [3.45051708e-03, 1.27584178e-06, 9.96502042e-01, 3.49989091e-06,\n",
              "        2.54490569e-05, 3.27413818e-06, 1.40985758e-05],\n",
              "       [6.15645968e-06, 2.46666675e-03, 6.31879957e-05, 9.94522929e-01,\n",
              "        1.72319953e-04, 2.46719387e-03, 3.01522930e-04],\n",
              "       [2.50807905e-04, 3.63996122e-07, 9.99731719e-01, 1.25454835e-06,\n",
              "        5.85713542e-06, 9.83568725e-07, 8.85736335e-06],\n",
              "       [8.68541804e-07, 9.98556554e-01, 7.43421253e-07, 1.96769513e-04,\n",
              "        1.04075098e-05, 1.22805394e-03, 6.60880642e-06],\n",
              "       [3.89893400e-03, 5.63752279e-02, 9.45227686e-04, 4.91639413e-03,\n",
              "        8.35686505e-01, 8.04670330e-04, 9.73729938e-02],\n",
              "       [2.05050260e-01, 1.40879038e-05, 7.94530690e-01, 1.01638916e-05,\n",
              "        2.85840273e-04, 1.98320522e-05, 8.92395910e-05],\n",
              "       [1.25343007e-07, 1.71634713e-06, 1.71196516e-05, 9.99914289e-01,\n",
              "        4.83447620e-05, 1.16913736e-06, 1.72839973e-05],\n",
              "       [1.72629257e-06, 6.78151650e-07, 9.12880478e-06, 7.37054961e-06,\n",
              "        9.99979615e-01, 6.66397071e-08, 1.34398545e-06],\n",
              "       [1.94701453e-04, 3.10114456e-06, 9.99308228e-01, 4.22639241e-05,\n",
              "        2.84722279e-04, 3.90392870e-06, 1.63051445e-04],\n",
              "       [8.94564655e-05, 7.73787008e-07, 9.99731839e-01, 8.62066281e-06,\n",
              "        1.37923242e-04, 8.60478167e-07, 3.05447829e-05],\n",
              "       [1.02402898e-03, 1.04402952e-05, 2.13122834e-03, 5.61520974e-05,\n",
              "        9.96669114e-01, 5.88477860e-07, 1.08534005e-04],\n",
              "       [1.89442531e-07, 6.74092689e-06, 1.39737604e-05, 9.99615908e-01,\n",
              "        3.49957903e-04, 2.00774434e-06, 1.12805128e-05],\n",
              "       [4.10178582e-05, 1.01834528e-04, 3.20620097e-06, 3.09198981e-06,\n",
              "        5.08048061e-07, 9.99838114e-01, 1.22719675e-05],\n",
              "       [3.58065069e-02, 6.96857333e-01, 1.23971287e-04, 4.49416082e-04,\n",
              "        4.20994172e-03, 2.61272281e-01, 1.28062000e-03],\n",
              "       [4.19107346e-06, 9.96289849e-01, 2.37471590e-06, 1.64525351e-04,\n",
              "        2.58547934e-05, 3.48846195e-03, 2.48291690e-05],\n",
              "       [5.41703157e-05, 1.49471276e-07, 9.99908566e-01, 2.17460047e-06,\n",
              "        1.92143034e-05, 1.41591343e-07, 1.55678317e-05],\n",
              "       [9.99969482e-01, 1.51839640e-06, 1.31829338e-05, 8.72475283e-08,\n",
              "        3.60838453e-06, 1.01912865e-05, 1.97584245e-06],\n",
              "       [1.07375917e-03, 1.30101921e-06, 9.98825610e-01, 9.73214446e-06,\n",
              "        7.22536206e-05, 1.11543875e-06, 1.61507396e-05],\n",
              "       [8.90321285e-03, 6.44663078e-06, 9.87730563e-01, 1.06391170e-04,\n",
              "        2.77975318e-03, 5.14026078e-06, 4.68403421e-04],\n",
              "       [1.30805393e-08, 2.32874590e-05, 1.38576206e-06, 9.99949813e-01,\n",
              "        2.38450721e-05, 2.76246226e-07, 1.37646714e-06],\n",
              "       [7.91816681e-04, 5.92107938e-07, 9.99191463e-01, 1.35033247e-06,\n",
              "        6.69926112e-06, 9.80747700e-07, 7.09920914e-06],\n",
              "       [5.12952624e-07, 3.82336481e-07, 1.24642668e-06, 7.02507862e-07,\n",
              "        1.45487447e-05, 3.08791584e-07, 9.99982238e-01],\n",
              "       [9.47773099e-01, 9.50679823e-05, 3.47334146e-02, 1.60325275e-04,\n",
              "        1.63254887e-03, 1.62548400e-04, 1.54429451e-02],\n",
              "       [4.62248352e-07, 5.85759949e-07, 1.78375080e-06, 1.14673494e-05,\n",
              "        9.99982238e-01, 3.02009013e-08, 3.45059789e-06],\n",
              "       [3.62858933e-04, 5.58948219e-01, 7.31698601e-05, 3.57165299e-02,\n",
              "        3.67079687e-04, 3.54284555e-01, 5.02475277e-02],\n",
              "       [6.78007609e-07, 9.98928607e-01, 6.29056331e-07, 8.48185518e-05,\n",
              "        3.78220716e-06, 9.71924805e-04, 9.49509285e-06],\n",
              "       [8.45429622e-06, 3.91246203e-06, 7.08789230e-06, 9.11753978e-06,\n",
              "        9.99969363e-01, 2.39954375e-07, 1.75384946e-06],\n",
              "       [1.76235392e-06, 9.93978739e-01, 1.12001828e-06, 7.98538749e-05,\n",
              "        6.69650990e-06, 5.92383510e-03, 8.01578881e-06],\n",
              "       [1.42678209e-05, 1.61530716e-05, 1.42755857e-06, 4.07000653e-06,\n",
              "        1.97055691e-04, 2.28367435e-06, 9.99764740e-01],\n",
              "       [2.41225243e-05, 5.02687320e-03, 5.13880138e-07, 6.41272572e-06,\n",
              "        2.36401092e-06, 9.94927645e-01, 1.21064304e-05],\n",
              "       [8.93030432e-08, 6.27231348e-05, 3.89309525e-06, 9.99904156e-01,\n",
              "        1.76488993e-05, 1.41171745e-06, 9.98407540e-06],\n",
              "       [7.28543819e-05, 9.98750966e-06, 1.47510555e-05, 1.10426990e-05,\n",
              "        9.99882817e-01, 3.59610169e-07, 8.21337017e-06],\n",
              "       [1.45843005e-05, 5.79246938e-01, 3.24206076e-06, 4.59057657e-04,\n",
              "        6.94997789e-06, 4.20225024e-01, 4.42142809e-05],\n",
              "       [6.13117936e-06, 1.84022406e-06, 5.31048681e-06, 2.95839072e-06,\n",
              "        1.55179657e-06, 1.86476097e-06, 9.99980330e-01],\n",
              "       [6.79252893e-02, 1.55011858e-05, 9.30942655e-01, 8.45634495e-05,\n",
              "        6.74015086e-04, 3.80534038e-05, 3.19909304e-04],\n",
              "       [2.67078892e-08, 1.22035908e-06, 6.47635079e-06, 9.99856949e-01,\n",
              "        1.06675048e-04, 2.08183536e-07, 2.84885555e-05],\n",
              "       [2.48940574e-04, 2.55970633e-07, 9.99713480e-01, 1.88440117e-06,\n",
              "        1.79345934e-05, 2.87925388e-07, 1.71700522e-05],\n",
              "       [3.90566092e-05, 9.42577302e-01, 5.67351663e-06, 4.76132613e-04,\n",
              "        3.59017322e-05, 5.48900887e-02, 1.97584531e-03],\n",
              "       [2.23679250e-04, 4.55128458e-07, 9.99620318e-01, 2.46743662e-06,\n",
              "        1.38203570e-04, 3.60009466e-07, 1.44350088e-05],\n",
              "       [1.03691626e-07, 4.58573686e-06, 8.55083726e-06, 9.99894977e-01,\n",
              "        7.47597223e-05, 8.78306878e-07, 1.62018350e-05],\n",
              "       [1.21288744e-04, 1.74523990e-07, 9.99848723e-01, 8.40354915e-07,\n",
              "        2.30548194e-05, 1.76259206e-07, 5.71052351e-06],\n",
              "       [2.20570737e-03, 5.69657971e-07, 9.97778833e-01, 8.65932407e-07,\n",
              "        6.77044181e-06, 1.40641453e-06, 5.89246338e-06],\n",
              "       [8.34856837e-05, 1.30624085e-05, 4.61530835e-05, 1.49174975e-05,\n",
              "        9.99837279e-01, 3.61482671e-06, 1.51951815e-06],\n",
              "       [4.25370038e-03, 2.04169510e-06, 9.95641828e-01, 2.86888530e-06,\n",
              "        8.57043706e-05, 3.12090629e-06, 1.07866499e-05],\n",
              "       [8.28580407e-04, 1.64824782e-03, 4.87925827e-05, 9.40340105e-05,\n",
              "        4.61283751e-04, 9.96651947e-01, 2.67044379e-04],\n",
              "       [1.89932998e-05, 5.95978834e-02, 3.47329956e-06, 4.85223754e-05,\n",
              "        4.70041823e-06, 9.40306187e-01, 2.01382682e-05],\n",
              "       [1.74045825e-04, 8.28222767e-07, 9.99503136e-01, 1.08210706e-05,\n",
              "        2.95697624e-04, 4.13452227e-07, 1.50131837e-05],\n",
              "       [2.26467341e-06, 2.52064411e-03, 8.38074702e-05, 9.90269899e-01,\n",
              "        6.62916107e-03, 1.76512447e-04, 3.17713333e-04],\n",
              "       [3.98200355e-05, 3.55767241e-07, 9.99815404e-01, 1.23344726e-05,\n",
              "        8.42885711e-05, 2.76670448e-07, 4.75925735e-05],\n",
              "       [2.22246285e-06, 1.17576204e-01, 3.34533033e-05, 8.80750418e-01,\n",
              "        8.58669926e-04, 6.49284688e-04, 1.29772423e-04],\n",
              "       [1.78670249e-04, 1.35338632e-03, 4.98276111e-03, 3.53020936e-01,\n",
              "        2.09469218e-02, 9.80182202e-04, 6.18537068e-01],\n",
              "       [4.44182224e-04, 4.02705598e-04, 3.66068016e-05, 1.02555641e-04,\n",
              "        9.95671391e-01, 4.50677135e-06, 3.33797350e-03],\n",
              "       [9.99821246e-01, 2.43112618e-06, 1.37149604e-04, 1.65629345e-07,\n",
              "        5.47590662e-06, 1.41114333e-05, 1.94662389e-05],\n",
              "       [8.40901066e-06, 6.14966929e-01, 4.29262000e-05, 3.74693900e-01,\n",
              "        9.07649472e-03, 9.86166415e-04, 2.25160678e-04],\n",
              "       [9.76189676e-06, 9.73914862e-01, 8.76619379e-06, 5.08365268e-03,\n",
              "        4.39531541e-05, 2.07961462e-02, 1.42961784e-04],\n",
              "       [1.80621617e-04, 6.51839946e-06, 6.66241124e-02, 1.58494717e-04,\n",
              "        9.32983875e-01, 1.24405938e-06, 4.51186861e-05],\n",
              "       [4.34036701e-05, 1.44765920e-07, 9.99921918e-01, 1.31181650e-06,\n",
              "        8.34039656e-06, 1.71657874e-07, 2.47721164e-05],\n",
              "       [5.23642925e-08, 1.02460433e-06, 2.02777173e-05, 9.99754369e-01,\n",
              "        1.97745874e-04, 3.31945984e-07, 2.62422182e-05],\n",
              "       [2.06348923e-05, 5.11324515e-06, 1.72023683e-05, 9.33038791e-06,\n",
              "        9.99945998e-01, 1.76591669e-07, 1.53129736e-06],\n",
              "       [2.19883800e-01, 7.64013967e-03, 2.08950847e-01, 1.27742272e-02,\n",
              "        9.98684950e-03, 5.26065290e-01, 1.46988397e-02],\n",
              "       [2.12031770e-02, 4.64857112e-06, 9.78673875e-01, 5.87759450e-06,\n",
              "        7.40541800e-05, 7.49503761e-06, 3.09339412e-05],\n",
              "       [1.33445123e-04, 8.04927822e-07, 9.99663830e-01, 1.73100652e-05,\n",
              "        1.53834670e-04, 4.59184207e-07, 3.02471253e-05],\n",
              "       [2.86989984e-06, 9.98482764e-01, 7.09172866e-07, 2.02369542e-04,\n",
              "        8.85021727e-05, 1.19736698e-03, 2.55102677e-05],\n",
              "       [4.87483144e-02, 1.18753496e-05, 9.50971305e-01, 2.39817527e-05,\n",
              "        1.60593292e-04, 3.40915067e-05, 4.98838344e-05],\n",
              "       [3.15495491e-01, 9.31521354e-05, 6.69377565e-01, 8.75314654e-05,\n",
              "        1.46998912e-02, 1.11444293e-04, 1.34915274e-04],\n",
              "       [5.75812737e-06, 4.02412024e-06, 9.85255974e-06, 6.98413214e-06,\n",
              "        3.42179601e-06, 4.04591628e-06, 9.99965906e-01],\n",
              "       [9.99641657e-01, 5.88386001e-06, 2.67357274e-04, 5.62178741e-07,\n",
              "        6.13366319e-06, 6.62882449e-05, 1.21548619e-05],\n",
              "       [5.50402736e-04, 2.05384686e-06, 9.99339879e-01, 2.12007071e-05,\n",
              "        4.84251177e-05, 4.10378425e-06, 3.39990402e-05]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 508
        }
      ],
      "source": [
        "y_test_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 509,
      "id": "996eb306",
      "metadata": {
        "id": "996eb306"
      },
      "outputs": [],
      "source": [
        "y_test_predictions=np.argmax(y_test_predictions,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 510,
      "id": "5f9002eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f9002eb",
        "outputId": "f6e09399-6c66-42fa-9393-4248e6559900"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 3, 4, 2, 1, 2, 4, 0, 2, 4, 1, 3, 5, 3, 5, 1, 5, 6, 1, 0, 1, 2,\n",
              "       5, 3, 2, 1, 1, 1, 2, 0, 6, 4, 4, 4, 6, 4, 5, 4, 0, 2, 3, 2, 1, 4,\n",
              "       2, 3, 4, 2, 2, 4, 3, 5, 1, 1, 2, 0, 2, 2, 3, 2, 6, 0, 4, 1, 1, 4,\n",
              "       1, 6, 5, 3, 4, 1, 6, 2, 3, 2, 1, 2, 3, 2, 2, 4, 2, 5, 5, 2, 3, 2,\n",
              "       3, 6, 4, 0, 1, 1, 4, 2, 3, 4, 5, 2, 2, 1, 2, 2, 6, 0, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 510
        }
      ],
      "source": [
        "y_test_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 511,
      "id": "2a98a885",
      "metadata": {
        "id": "2a98a885"
      },
      "outputs": [],
      "source": [
        "# df.replace({ 'happyness': 0, 'neutral': 1,'anger': 2,'sadness': 3, 'fear':4,'boredom':5,'disgust':6}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 512,
      "id": "d0b8e93d",
      "metadata": {
        "id": "d0b8e93d"
      },
      "outputs": [],
      "source": [
        "emotions={\n",
        " 0: 'happyness',\n",
        " 1: 'neutral',\n",
        " 2: 'anger',\n",
        " 3: 'sadness',\n",
        " 4: 'fear',\n",
        " 5: 'boredom',\n",
        " 6: 'disgust',\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 513,
      "id": "a32e6964",
      "metadata": {
        "id": "a32e6964"
      },
      "outputs": [],
      "source": [
        "label=[]\n",
        "for i in y_test_predictions:\n",
        "    label1=emotions[i]\n",
        "    label.append(label1)\n",
        "label\n",
        "y_pred_acc=np.array(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 514,
      "id": "c92b0963",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c92b0963",
        "outputId": "fccad90b-c790-4a77-d83c-cf810d046ed4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['happyness', 'sadness', 'fear', 'anger', 'neutral', 'anger',\n",
              "       'fear', 'happyness', 'anger', 'fear', 'neutral', 'sadness',\n",
              "       'boredom', 'sadness', 'boredom', 'neutral', 'boredom', 'disgust',\n",
              "       'neutral', 'happyness', 'neutral', 'anger', 'boredom', 'sadness',\n",
              "       'anger', 'neutral', 'neutral', 'neutral', 'anger', 'happyness',\n",
              "       'disgust', 'fear', 'fear', 'fear', 'disgust', 'fear', 'boredom',\n",
              "       'fear', 'happyness', 'anger', 'sadness', 'anger', 'neutral',\n",
              "       'fear', 'anger', 'sadness', 'fear', 'anger', 'anger', 'fear',\n",
              "       'sadness', 'boredom', 'neutral', 'neutral', 'anger', 'happyness',\n",
              "       'anger', 'anger', 'sadness', 'anger', 'disgust', 'happyness',\n",
              "       'fear', 'neutral', 'neutral', 'fear', 'neutral', 'disgust',\n",
              "       'boredom', 'sadness', 'fear', 'neutral', 'disgust', 'anger',\n",
              "       'sadness', 'anger', 'neutral', 'anger', 'sadness', 'anger',\n",
              "       'anger', 'fear', 'anger', 'boredom', 'boredom', 'anger', 'sadness',\n",
              "       'anger', 'sadness', 'disgust', 'fear', 'happyness', 'neutral',\n",
              "       'neutral', 'fear', 'anger', 'sadness', 'fear', 'boredom', 'anger',\n",
              "       'anger', 'neutral', 'anger', 'anger', 'disgust', 'happyness',\n",
              "       'anger'], dtype='<U9')"
            ]
          },
          "metadata": {},
          "execution_count": 514
        }
      ],
      "source": [
        "y_pred_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 515,
      "id": "59bdabc1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59bdabc1",
        "outputId": "07379cb0-dc59-4697-d41e-f2cc72414a1c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      0\n",
              "1      3\n",
              "2      2\n",
              "3      2\n",
              "4      5\n",
              "      ..\n",
              "102    2\n",
              "103    4\n",
              "104    6\n",
              "105    0\n",
              "106    2\n",
              "Name: 0.1, Length: 107, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 515
        }
      ],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 516,
      "id": "0dec7231",
      "metadata": {
        "id": "0dec7231"
      },
      "outputs": [],
      "source": [
        "emotion={\n",
        " 0: 'happyness',\n",
        " 1: 'neutral',\n",
        " 2: 'anger',\n",
        " 3: 'sadness',\n",
        " 4: 'fear',\n",
        " 5: 'boredom',\n",
        " 6: 'disgust',\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 517,
      "id": "06e8caee",
      "metadata": {
        "id": "06e8caee"
      },
      "outputs": [],
      "source": [
        "label_test=[]\n",
        "for i in y_test:\n",
        "    label_test.append(emotion[i])\n",
        "label_test\n",
        "y_true_accu=np.array(label_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 518,
      "id": "9562d2b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9562d2b0",
        "outputId": "49ee6d38-88fb-4bb3-c6e7-396399ac4f19"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['happyness', 'sadness', 'anger', 'anger', 'boredom', 'fear',\n",
              "       'happyness', 'boredom', 'anger', 'fear', 'neutral', 'sadness',\n",
              "       'boredom', 'sadness', 'boredom', 'neutral', 'boredom', 'neutral',\n",
              "       'boredom', 'happyness', 'neutral', 'anger', 'boredom', 'sadness',\n",
              "       'disgust', 'neutral', 'boredom', 'neutral', 'anger', 'disgust',\n",
              "       'disgust', 'neutral', 'anger', 'fear', 'disgust', 'fear',\n",
              "       'boredom', 'fear', 'happyness', 'anger', 'sadness', 'anger',\n",
              "       'neutral', 'happyness', 'anger', 'sadness', 'fear', 'anger',\n",
              "       'anger', 'fear', 'sadness', 'boredom', 'happyness', 'neutral',\n",
              "       'anger', 'happyness', 'anger', 'happyness', 'sadness', 'anger',\n",
              "       'disgust', 'happyness', 'fear', 'disgust', 'neutral', 'fear',\n",
              "       'neutral', 'disgust', 'boredom', 'sadness', 'fear', 'neutral',\n",
              "       'disgust', 'happyness', 'sadness', 'anger', 'neutral', 'anger',\n",
              "       'sadness', 'anger', 'anger', 'fear', 'anger', 'fear', 'neutral',\n",
              "       'happyness', 'boredom', 'anger', 'boredom', 'neutral', 'disgust',\n",
              "       'happyness', 'boredom', 'neutral', 'anger', 'anger', 'sadness',\n",
              "       'fear', 'boredom', 'happyness', 'anger', 'boredom', 'anger',\n",
              "       'fear', 'disgust', 'happyness', 'anger'], dtype='<U9')"
            ]
          },
          "metadata": {},
          "execution_count": 518
        }
      ],
      "source": [
        "y_true_accu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 519,
      "id": "8def2194",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8def2194",
        "outputId": "e5619cf7-9db1-4364-bf76-6baf80df30f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 72.90%\n"
          ]
        }
      ],
      "source": [
        "#DataFlair - Calculate the accuracy of our model\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy=accuracy_score(y_true=y_true_accu, y_pred=y_pred_acc)\n",
        "\n",
        "#DataFlair - Print the accuracy\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 520,
      "id": "0397dc13",
      "metadata": {
        "id": "0397dc13"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm=confusion_matrix(y_true=y_true_accu, y_pred=y_pred_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 521,
      "id": "7cee98a1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cee98a1",
        "outputId": "c96605e4-0091-4c4d-88e0-940c4e6ffa2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.76      0.88      0.81        25\n",
            "     boredom       0.80      0.50      0.62        16\n",
            "     disgust       0.75      0.60      0.67        10\n",
            "        fear       0.61      0.79      0.69        14\n",
            "   happyness       0.78      0.50      0.61        14\n",
            "     neutral       0.63      0.75      0.69        16\n",
            "     sadness       0.86      1.00      0.92        12\n",
            "\n",
            "    accuracy                           0.73       107\n",
            "   macro avg       0.74      0.72      0.71       107\n",
            "weighted avg       0.74      0.73      0.72       107\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true_accu,y_pred_acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 521,
      "id": "73b3090a",
      "metadata": {
        "id": "73b3090a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 521,
      "id": "4537d18f",
      "metadata": {
        "id": "4537d18f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}