{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 210,
      "id": "e1f95dfd",
      "metadata": {
        "id": "e1f95dfd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import seaborn as sns\n",
        "import librosa\n",
        "import librosa.display\n",
        "import IPython.display as pld\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xe7uVNwQ3Ppw",
        "outputId": "fdda6c51-092f-49e0-d186-2e8cffeba409"
      },
      "id": "Xe7uVNwQ3Ppw",
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "id": "603a1332",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "603a1332",
        "outputId": "1781c47b-bb97-4284-bb38-472226e0794b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Unnamed: 0          0          1          2          3          4  \\\n",
              "0             0 -482.45233  62.835957  -0.348998  26.264595   2.978607   \n",
              "1             1 -434.88647  41.972150 -29.416862  18.537344  -4.156565   \n",
              "2             2 -454.89886  45.067265  -0.193278  15.111545   3.080835   \n",
              "3             3 -451.45264  71.335200  12.223010  32.649555   3.997801   \n",
              "4             4 -478.92007  56.887730  -9.496516  12.799419  -0.196528   \n",
              "..          ...        ...        ...        ...        ...        ...   \n",
              "423         423 -478.05527   5.705422 -24.631056  27.622614 -19.235449   \n",
              "424         424 -427.36716  51.473750   4.835125  40.900140   6.937289   \n",
              "425         425 -467.15588  52.217026  10.470471  47.414780   8.690019   \n",
              "426         426 -526.19570  11.317784 -18.942173  29.540787 -28.057306   \n",
              "427         427 -437.97220   5.289146 -22.667547  25.394840 -28.545520   \n",
              "\n",
              "             5          6         7          8  ...       250       251  \\\n",
              "0     6.900927 -13.006243  0.273391  -9.196591  ...  0.675823  0.684034   \n",
              "1     5.257353 -11.410935 -8.983023 -11.285996  ...  0.648380  0.675295   \n",
              "2     4.204241 -10.440731 -6.615343 -16.249382  ...  0.575784  0.523305   \n",
              "3    15.200773  -2.812883 -1.384373  -6.467040  ...  0.719856  0.667424   \n",
              "4     3.286758  -9.909594 -2.337262  -7.694031  ...  0.664169  0.700484   \n",
              "..         ...        ...       ...        ...  ...       ...       ...   \n",
              "423  -7.040019 -13.293165 -9.629133 -14.067036  ...  0.590422  0.481957   \n",
              "424  13.700687  -8.331753 -0.583414  -6.279562  ...  0.638155  0.574542   \n",
              "425  15.601270  -2.032935  4.985774  -7.432734  ...  0.523647  0.509723   \n",
              "426   1.299599 -16.251814 -7.512440 -23.191568  ...  0.528861  0.501070   \n",
              "427   0.428451 -11.650926 -8.022680 -18.337156  ...  0.495079  0.539387   \n",
              "\n",
              "          252       253       254       255       256       257       258  0.1  \n",
              "0    0.627376 -0.001574  0.012645 -0.027069  0.019313 -0.002252 -0.006220    0  \n",
              "1    0.560000 -0.010576 -0.000228  0.011102 -0.074188  0.012116 -0.000779    2  \n",
              "2    0.423375  0.009395 -0.025547 -0.035554 -0.025885  0.011198 -0.000163    0  \n",
              "3    0.570905  0.003044 -0.006101 -0.009038 -0.049699  0.027615  0.021319    3  \n",
              "4    0.594266 -0.029971  0.019514 -0.009344 -0.040153  0.013369 -0.006417    0  \n",
              "..        ...       ...       ...       ...       ...       ...       ...  ...  \n",
              "423  0.481437 -0.011216 -0.031485 -0.029936  0.042160 -0.013815  0.009839    0  \n",
              "424  0.503516  0.019632  0.046315  0.074801  0.018078  0.022948 -0.016365    3  \n",
              "425  0.498566  0.051532  0.015525 -0.006052  0.018201  0.004361 -0.021649    3  \n",
              "426  0.496900  0.007856  0.005917 -0.045393 -0.004246 -0.001628  0.015051    2  \n",
              "427  0.527394 -0.001086  0.013948  0.013274 -0.056855 -0.001004 -0.001355    2  \n",
              "\n",
              "[428 rows x 261 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-460d8fdf-ca21-495d-9f66-5a9a1c028645\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>...</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "      <th>257</th>\n",
              "      <th>258</th>\n",
              "      <th>0.1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-482.45233</td>\n",
              "      <td>62.835957</td>\n",
              "      <td>-0.348998</td>\n",
              "      <td>26.264595</td>\n",
              "      <td>2.978607</td>\n",
              "      <td>6.900927</td>\n",
              "      <td>-13.006243</td>\n",
              "      <td>0.273391</td>\n",
              "      <td>-9.196591</td>\n",
              "      <td>...</td>\n",
              "      <td>0.675823</td>\n",
              "      <td>0.684034</td>\n",
              "      <td>0.627376</td>\n",
              "      <td>-0.001574</td>\n",
              "      <td>0.012645</td>\n",
              "      <td>-0.027069</td>\n",
              "      <td>0.019313</td>\n",
              "      <td>-0.002252</td>\n",
              "      <td>-0.006220</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>-434.88647</td>\n",
              "      <td>41.972150</td>\n",
              "      <td>-29.416862</td>\n",
              "      <td>18.537344</td>\n",
              "      <td>-4.156565</td>\n",
              "      <td>5.257353</td>\n",
              "      <td>-11.410935</td>\n",
              "      <td>-8.983023</td>\n",
              "      <td>-11.285996</td>\n",
              "      <td>...</td>\n",
              "      <td>0.648380</td>\n",
              "      <td>0.675295</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>-0.010576</td>\n",
              "      <td>-0.000228</td>\n",
              "      <td>0.011102</td>\n",
              "      <td>-0.074188</td>\n",
              "      <td>0.012116</td>\n",
              "      <td>-0.000779</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>-454.89886</td>\n",
              "      <td>45.067265</td>\n",
              "      <td>-0.193278</td>\n",
              "      <td>15.111545</td>\n",
              "      <td>3.080835</td>\n",
              "      <td>4.204241</td>\n",
              "      <td>-10.440731</td>\n",
              "      <td>-6.615343</td>\n",
              "      <td>-16.249382</td>\n",
              "      <td>...</td>\n",
              "      <td>0.575784</td>\n",
              "      <td>0.523305</td>\n",
              "      <td>0.423375</td>\n",
              "      <td>0.009395</td>\n",
              "      <td>-0.025547</td>\n",
              "      <td>-0.035554</td>\n",
              "      <td>-0.025885</td>\n",
              "      <td>0.011198</td>\n",
              "      <td>-0.000163</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>-451.45264</td>\n",
              "      <td>71.335200</td>\n",
              "      <td>12.223010</td>\n",
              "      <td>32.649555</td>\n",
              "      <td>3.997801</td>\n",
              "      <td>15.200773</td>\n",
              "      <td>-2.812883</td>\n",
              "      <td>-1.384373</td>\n",
              "      <td>-6.467040</td>\n",
              "      <td>...</td>\n",
              "      <td>0.719856</td>\n",
              "      <td>0.667424</td>\n",
              "      <td>0.570905</td>\n",
              "      <td>0.003044</td>\n",
              "      <td>-0.006101</td>\n",
              "      <td>-0.009038</td>\n",
              "      <td>-0.049699</td>\n",
              "      <td>0.027615</td>\n",
              "      <td>0.021319</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>-478.92007</td>\n",
              "      <td>56.887730</td>\n",
              "      <td>-9.496516</td>\n",
              "      <td>12.799419</td>\n",
              "      <td>-0.196528</td>\n",
              "      <td>3.286758</td>\n",
              "      <td>-9.909594</td>\n",
              "      <td>-2.337262</td>\n",
              "      <td>-7.694031</td>\n",
              "      <td>...</td>\n",
              "      <td>0.664169</td>\n",
              "      <td>0.700484</td>\n",
              "      <td>0.594266</td>\n",
              "      <td>-0.029971</td>\n",
              "      <td>0.019514</td>\n",
              "      <td>-0.009344</td>\n",
              "      <td>-0.040153</td>\n",
              "      <td>0.013369</td>\n",
              "      <td>-0.006417</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>423</th>\n",
              "      <td>423</td>\n",
              "      <td>-478.05527</td>\n",
              "      <td>5.705422</td>\n",
              "      <td>-24.631056</td>\n",
              "      <td>27.622614</td>\n",
              "      <td>-19.235449</td>\n",
              "      <td>-7.040019</td>\n",
              "      <td>-13.293165</td>\n",
              "      <td>-9.629133</td>\n",
              "      <td>-14.067036</td>\n",
              "      <td>...</td>\n",
              "      <td>0.590422</td>\n",
              "      <td>0.481957</td>\n",
              "      <td>0.481437</td>\n",
              "      <td>-0.011216</td>\n",
              "      <td>-0.031485</td>\n",
              "      <td>-0.029936</td>\n",
              "      <td>0.042160</td>\n",
              "      <td>-0.013815</td>\n",
              "      <td>0.009839</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424</th>\n",
              "      <td>424</td>\n",
              "      <td>-427.36716</td>\n",
              "      <td>51.473750</td>\n",
              "      <td>4.835125</td>\n",
              "      <td>40.900140</td>\n",
              "      <td>6.937289</td>\n",
              "      <td>13.700687</td>\n",
              "      <td>-8.331753</td>\n",
              "      <td>-0.583414</td>\n",
              "      <td>-6.279562</td>\n",
              "      <td>...</td>\n",
              "      <td>0.638155</td>\n",
              "      <td>0.574542</td>\n",
              "      <td>0.503516</td>\n",
              "      <td>0.019632</td>\n",
              "      <td>0.046315</td>\n",
              "      <td>0.074801</td>\n",
              "      <td>0.018078</td>\n",
              "      <td>0.022948</td>\n",
              "      <td>-0.016365</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>425</th>\n",
              "      <td>425</td>\n",
              "      <td>-467.15588</td>\n",
              "      <td>52.217026</td>\n",
              "      <td>10.470471</td>\n",
              "      <td>47.414780</td>\n",
              "      <td>8.690019</td>\n",
              "      <td>15.601270</td>\n",
              "      <td>-2.032935</td>\n",
              "      <td>4.985774</td>\n",
              "      <td>-7.432734</td>\n",
              "      <td>...</td>\n",
              "      <td>0.523647</td>\n",
              "      <td>0.509723</td>\n",
              "      <td>0.498566</td>\n",
              "      <td>0.051532</td>\n",
              "      <td>0.015525</td>\n",
              "      <td>-0.006052</td>\n",
              "      <td>0.018201</td>\n",
              "      <td>0.004361</td>\n",
              "      <td>-0.021649</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>426</th>\n",
              "      <td>426</td>\n",
              "      <td>-526.19570</td>\n",
              "      <td>11.317784</td>\n",
              "      <td>-18.942173</td>\n",
              "      <td>29.540787</td>\n",
              "      <td>-28.057306</td>\n",
              "      <td>1.299599</td>\n",
              "      <td>-16.251814</td>\n",
              "      <td>-7.512440</td>\n",
              "      <td>-23.191568</td>\n",
              "      <td>...</td>\n",
              "      <td>0.528861</td>\n",
              "      <td>0.501070</td>\n",
              "      <td>0.496900</td>\n",
              "      <td>0.007856</td>\n",
              "      <td>0.005917</td>\n",
              "      <td>-0.045393</td>\n",
              "      <td>-0.004246</td>\n",
              "      <td>-0.001628</td>\n",
              "      <td>0.015051</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>427</th>\n",
              "      <td>427</td>\n",
              "      <td>-437.97220</td>\n",
              "      <td>5.289146</td>\n",
              "      <td>-22.667547</td>\n",
              "      <td>25.394840</td>\n",
              "      <td>-28.545520</td>\n",
              "      <td>0.428451</td>\n",
              "      <td>-11.650926</td>\n",
              "      <td>-8.022680</td>\n",
              "      <td>-18.337156</td>\n",
              "      <td>...</td>\n",
              "      <td>0.495079</td>\n",
              "      <td>0.539387</td>\n",
              "      <td>0.527394</td>\n",
              "      <td>-0.001086</td>\n",
              "      <td>0.013948</td>\n",
              "      <td>0.013274</td>\n",
              "      <td>-0.056855</td>\n",
              "      <td>-0.001004</td>\n",
              "      <td>-0.001355</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>428 rows × 261 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-460d8fdf-ca21-495d-9f66-5a9a1c028645')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-460d8fdf-ca21-495d-9f66-5a9a1c028645 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-460d8fdf-ca21-495d-9f66-5a9a1c028645');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-667753ef-2c2d-4370-8512-6bd6ea002c57\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-667753ef-2c2d-4370-8512-6bd6ea002c57')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-667753ef-2c2d-4370-8512-6bd6ea002c57 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df1"
            }
          },
          "metadata": {},
          "execution_count": 212
        }
      ],
      "source": [
        "df1=pd.read_csv('/content/drive/MyDrive/DATASETS/EmoDB Dataset/train_fold4.csv')\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "id": "a08acf9d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "a08acf9d",
        "outputId": "726e5dbe-dcf7-490c-cd86-02ffb0c7d6a9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Unnamed: 0          0          1          2          3          4  \\\n",
              "0             0 -469.48477  88.400730  -7.127512  29.156132   5.335554   \n",
              "1             1 -447.12630  86.114920   4.772520  38.097256   8.324276   \n",
              "2             2 -433.33972  29.850212  -8.025759  23.749544   1.409119   \n",
              "3             3 -465.22060  37.216230 -27.930758  18.472618  -4.044474   \n",
              "4             4 -407.61963  39.778786  -9.883366   9.930613  -1.744454   \n",
              "..          ...        ...        ...        ...        ...        ...   \n",
              "102         102 -435.40150  17.910067 -32.667010  20.082544 -26.186550   \n",
              "103         103 -479.66450   7.133218 -38.469020   7.929045 -32.369520   \n",
              "104         104 -475.25198  48.588770  -2.106867  34.655080   3.924164   \n",
              "105         105 -505.94534  -1.201753 -28.506739  24.164959 -38.382088   \n",
              "106         106 -418.62490  57.015880   6.383415  47.618423  -8.051490   \n",
              "\n",
              "            5          6          7          8  ...       250       251  \\\n",
              "0    7.147227  -6.727572  -8.307674  -3.364513  ...  0.635522  0.544438   \n",
              "1    8.538317  -4.507682  -7.680664  -7.317249  ...  0.615263  0.566964   \n",
              "2    8.578079  -4.816757  -7.580163 -22.596510  ...  0.574034  0.558508   \n",
              "3    3.874021 -12.937754 -11.636196 -20.833626  ...  0.525122  0.552413   \n",
              "4    6.709172 -12.445559  -3.181297 -17.876530  ...  0.621760  0.691176   \n",
              "..        ...        ...        ...        ...  ...       ...       ...   \n",
              "102  4.677832 -12.905549  -9.618332 -18.023060  ...  0.536450  0.517381   \n",
              "103  4.264574 -14.345546  -6.981534 -17.207087  ...  0.540806  0.580027   \n",
              "104 -7.597296 -17.605679 -13.560740 -15.469123  ...  0.650717  0.574044   \n",
              "105  1.337496 -13.754961  -5.442271 -14.897131  ...  0.499615  0.552792   \n",
              "106  4.213936 -15.029120  -2.448467 -10.805821  ...  0.569263  0.551809   \n",
              "\n",
              "          252       253       254       255       256       257       258  0.1  \n",
              "0    0.532856 -0.018488  0.011973 -0.011038  0.079758 -0.021044 -0.019445    1  \n",
              "1    0.605103 -0.008389 -0.051995 -0.056544 -0.020014  0.013964  0.008349    1  \n",
              "2    0.499983 -0.009560  0.001801 -0.004441 -0.018945  0.011005 -0.011105    2  \n",
              "3    0.580766  0.016770  0.010151  0.024988  0.011547 -0.001119  0.005264    2  \n",
              "4    0.645503  0.023878  0.001674  0.000677 -0.006433 -0.002290 -0.001868    4  \n",
              "..        ...       ...       ...       ...       ...       ...       ...  ...  \n",
              "102  0.519304 -0.007135 -0.007543  0.027781 -0.004844  0.001824  0.008224    0  \n",
              "103  0.603574 -0.018925 -0.009150  0.035872  0.004143 -0.000523 -0.005385    2  \n",
              "104  0.481354 -0.014255  0.003728 -0.059519 -0.000201  0.013722 -0.000998    6  \n",
              "105  0.578515 -0.030600 -0.005539  0.033227  0.001151 -0.006986 -0.002342    2  \n",
              "106  0.576764  0.016046 -0.025610  0.025426 -0.094371  0.008999 -0.004985    5  \n",
              "\n",
              "[107 rows x 261 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c7ad3ae1-5e82-417b-a96f-a458e89a9268\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>...</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "      <th>257</th>\n",
              "      <th>258</th>\n",
              "      <th>0.1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-469.48477</td>\n",
              "      <td>88.400730</td>\n",
              "      <td>-7.127512</td>\n",
              "      <td>29.156132</td>\n",
              "      <td>5.335554</td>\n",
              "      <td>7.147227</td>\n",
              "      <td>-6.727572</td>\n",
              "      <td>-8.307674</td>\n",
              "      <td>-3.364513</td>\n",
              "      <td>...</td>\n",
              "      <td>0.635522</td>\n",
              "      <td>0.544438</td>\n",
              "      <td>0.532856</td>\n",
              "      <td>-0.018488</td>\n",
              "      <td>0.011973</td>\n",
              "      <td>-0.011038</td>\n",
              "      <td>0.079758</td>\n",
              "      <td>-0.021044</td>\n",
              "      <td>-0.019445</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>-447.12630</td>\n",
              "      <td>86.114920</td>\n",
              "      <td>4.772520</td>\n",
              "      <td>38.097256</td>\n",
              "      <td>8.324276</td>\n",
              "      <td>8.538317</td>\n",
              "      <td>-4.507682</td>\n",
              "      <td>-7.680664</td>\n",
              "      <td>-7.317249</td>\n",
              "      <td>...</td>\n",
              "      <td>0.615263</td>\n",
              "      <td>0.566964</td>\n",
              "      <td>0.605103</td>\n",
              "      <td>-0.008389</td>\n",
              "      <td>-0.051995</td>\n",
              "      <td>-0.056544</td>\n",
              "      <td>-0.020014</td>\n",
              "      <td>0.013964</td>\n",
              "      <td>0.008349</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>-433.33972</td>\n",
              "      <td>29.850212</td>\n",
              "      <td>-8.025759</td>\n",
              "      <td>23.749544</td>\n",
              "      <td>1.409119</td>\n",
              "      <td>8.578079</td>\n",
              "      <td>-4.816757</td>\n",
              "      <td>-7.580163</td>\n",
              "      <td>-22.596510</td>\n",
              "      <td>...</td>\n",
              "      <td>0.574034</td>\n",
              "      <td>0.558508</td>\n",
              "      <td>0.499983</td>\n",
              "      <td>-0.009560</td>\n",
              "      <td>0.001801</td>\n",
              "      <td>-0.004441</td>\n",
              "      <td>-0.018945</td>\n",
              "      <td>0.011005</td>\n",
              "      <td>-0.011105</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>-465.22060</td>\n",
              "      <td>37.216230</td>\n",
              "      <td>-27.930758</td>\n",
              "      <td>18.472618</td>\n",
              "      <td>-4.044474</td>\n",
              "      <td>3.874021</td>\n",
              "      <td>-12.937754</td>\n",
              "      <td>-11.636196</td>\n",
              "      <td>-20.833626</td>\n",
              "      <td>...</td>\n",
              "      <td>0.525122</td>\n",
              "      <td>0.552413</td>\n",
              "      <td>0.580766</td>\n",
              "      <td>0.016770</td>\n",
              "      <td>0.010151</td>\n",
              "      <td>0.024988</td>\n",
              "      <td>0.011547</td>\n",
              "      <td>-0.001119</td>\n",
              "      <td>0.005264</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>-407.61963</td>\n",
              "      <td>39.778786</td>\n",
              "      <td>-9.883366</td>\n",
              "      <td>9.930613</td>\n",
              "      <td>-1.744454</td>\n",
              "      <td>6.709172</td>\n",
              "      <td>-12.445559</td>\n",
              "      <td>-3.181297</td>\n",
              "      <td>-17.876530</td>\n",
              "      <td>...</td>\n",
              "      <td>0.621760</td>\n",
              "      <td>0.691176</td>\n",
              "      <td>0.645503</td>\n",
              "      <td>0.023878</td>\n",
              "      <td>0.001674</td>\n",
              "      <td>0.000677</td>\n",
              "      <td>-0.006433</td>\n",
              "      <td>-0.002290</td>\n",
              "      <td>-0.001868</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>102</td>\n",
              "      <td>-435.40150</td>\n",
              "      <td>17.910067</td>\n",
              "      <td>-32.667010</td>\n",
              "      <td>20.082544</td>\n",
              "      <td>-26.186550</td>\n",
              "      <td>4.677832</td>\n",
              "      <td>-12.905549</td>\n",
              "      <td>-9.618332</td>\n",
              "      <td>-18.023060</td>\n",
              "      <td>...</td>\n",
              "      <td>0.536450</td>\n",
              "      <td>0.517381</td>\n",
              "      <td>0.519304</td>\n",
              "      <td>-0.007135</td>\n",
              "      <td>-0.007543</td>\n",
              "      <td>0.027781</td>\n",
              "      <td>-0.004844</td>\n",
              "      <td>0.001824</td>\n",
              "      <td>0.008224</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>103</td>\n",
              "      <td>-479.66450</td>\n",
              "      <td>7.133218</td>\n",
              "      <td>-38.469020</td>\n",
              "      <td>7.929045</td>\n",
              "      <td>-32.369520</td>\n",
              "      <td>4.264574</td>\n",
              "      <td>-14.345546</td>\n",
              "      <td>-6.981534</td>\n",
              "      <td>-17.207087</td>\n",
              "      <td>...</td>\n",
              "      <td>0.540806</td>\n",
              "      <td>0.580027</td>\n",
              "      <td>0.603574</td>\n",
              "      <td>-0.018925</td>\n",
              "      <td>-0.009150</td>\n",
              "      <td>0.035872</td>\n",
              "      <td>0.004143</td>\n",
              "      <td>-0.000523</td>\n",
              "      <td>-0.005385</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>104</td>\n",
              "      <td>-475.25198</td>\n",
              "      <td>48.588770</td>\n",
              "      <td>-2.106867</td>\n",
              "      <td>34.655080</td>\n",
              "      <td>3.924164</td>\n",
              "      <td>-7.597296</td>\n",
              "      <td>-17.605679</td>\n",
              "      <td>-13.560740</td>\n",
              "      <td>-15.469123</td>\n",
              "      <td>...</td>\n",
              "      <td>0.650717</td>\n",
              "      <td>0.574044</td>\n",
              "      <td>0.481354</td>\n",
              "      <td>-0.014255</td>\n",
              "      <td>0.003728</td>\n",
              "      <td>-0.059519</td>\n",
              "      <td>-0.000201</td>\n",
              "      <td>0.013722</td>\n",
              "      <td>-0.000998</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>105</td>\n",
              "      <td>-505.94534</td>\n",
              "      <td>-1.201753</td>\n",
              "      <td>-28.506739</td>\n",
              "      <td>24.164959</td>\n",
              "      <td>-38.382088</td>\n",
              "      <td>1.337496</td>\n",
              "      <td>-13.754961</td>\n",
              "      <td>-5.442271</td>\n",
              "      <td>-14.897131</td>\n",
              "      <td>...</td>\n",
              "      <td>0.499615</td>\n",
              "      <td>0.552792</td>\n",
              "      <td>0.578515</td>\n",
              "      <td>-0.030600</td>\n",
              "      <td>-0.005539</td>\n",
              "      <td>0.033227</td>\n",
              "      <td>0.001151</td>\n",
              "      <td>-0.006986</td>\n",
              "      <td>-0.002342</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>106</td>\n",
              "      <td>-418.62490</td>\n",
              "      <td>57.015880</td>\n",
              "      <td>6.383415</td>\n",
              "      <td>47.618423</td>\n",
              "      <td>-8.051490</td>\n",
              "      <td>4.213936</td>\n",
              "      <td>-15.029120</td>\n",
              "      <td>-2.448467</td>\n",
              "      <td>-10.805821</td>\n",
              "      <td>...</td>\n",
              "      <td>0.569263</td>\n",
              "      <td>0.551809</td>\n",
              "      <td>0.576764</td>\n",
              "      <td>0.016046</td>\n",
              "      <td>-0.025610</td>\n",
              "      <td>0.025426</td>\n",
              "      <td>-0.094371</td>\n",
              "      <td>0.008999</td>\n",
              "      <td>-0.004985</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>107 rows × 261 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c7ad3ae1-5e82-417b-a96f-a458e89a9268')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c7ad3ae1-5e82-417b-a96f-a458e89a9268 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c7ad3ae1-5e82-417b-a96f-a458e89a9268');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-93e9ff84-96ae-40c7-a83e-a8e6d5b2aabf\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-93e9ff84-96ae-40c7-a83e-a8e6d5b2aabf')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-93e9ff84-96ae-40c7-a83e-a8e6d5b2aabf button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df2"
            }
          },
          "metadata": {},
          "execution_count": 213
        }
      ],
      "source": [
        "df2=pd.read_csv('/content/drive/MyDrive/DATASETS/EmoDB Dataset/test_fold4.csv')\n",
        "df2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "id": "a3d750f7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3d750f7",
        "outputId": "c48d2e55-caed-4e17-ef39-937b5325adc5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-482.45233, -434.88647, -454.89886, -451.45264, -478.92007,\n",
              "       -445.90384, -432.8532 , -438.46402, -451.66434, -415.74384,\n",
              "       -444.03302, -449.64343, -448.45706, -447.862  , -414.28708,\n",
              "       -405.8289 , -414.1051 , -403.34033, -434.169  , -452.2124 ,\n",
              "       -491.33237, -433.93875, -456.17935, -454.38022, -508.38065,\n",
              "       -440.71924, -440.73105, -451.39062, -447.41296, -448.62344,\n",
              "       -474.31326, -390.37305, -423.79208, -446.3631 , -457.93448,\n",
              "       -433.23328, -439.48215, -466.99695, -466.81842, -494.3395 ,\n",
              "       -426.0529 , -482.82642, -448.18274, -410.33658, -446.7974 ,\n",
              "       -453.8969 , -521.504  , -453.46445, -376.84555, -436.69876,\n",
              "       -446.97107, -454.48465, -468.04486, -456.72043, -446.98584,\n",
              "       -418.3227 , -423.0657 , -461.82983, -478.63577, -483.07538,\n",
              "       -434.9256 , -425.79813, -399.51852, -402.0103 , -461.44766,\n",
              "       -474.8092 , -464.07745, -503.61353, -453.04343, -482.67584,\n",
              "       -409.22214, -432.09225, -500.27402, -531.9213 , -455.4235 ,\n",
              "       -465.1539 , -424.5544 , -456.9332 , -512.6878 , -469.68045,\n",
              "       -485.50922, -444.93484, -418.001  , -456.90128, -492.80527,\n",
              "       -488.90784, -477.71838, -456.18542, -503.9724 , -447.12366,\n",
              "       -437.33456, -504.06378, -514.9054 , -414.93512, -503.28616,\n",
              "       -453.48303, -488.23538, -432.9396 , -459.55142, -492.041  ,\n",
              "       -449.0555 , -407.6214 , -386.1615 , -463.09274, -458.41788,\n",
              "       -447.04764, -439.2523 , -446.3131 , -405.84958, -378.2114 ,\n",
              "       -390.5131 , -470.87045, -470.00208, -470.88403, -413.56833,\n",
              "       -403.73657, -405.75662, -377.2399 , -445.18192, -473.9847 ,\n",
              "       -433.5633 , -433.1373 , -434.17294, -446.96768, -448.68094,\n",
              "       -434.1118 , -423.73795, -410.16275, -447.18866, -406.1605 ,\n",
              "       -448.96103, -446.1366 , -401.80896, -428.3561 , -389.66595,\n",
              "       -514.8218 , -345.723  , -403.9027 , -402.585  , -472.72705,\n",
              "       -364.07712, -412.15396, -409.9087 , -359.38922, -391.26596,\n",
              "       -370.994  , -449.62973, -373.46188, -427.5882 , -437.44412,\n",
              "       -365.9771 , -400.4402 , -464.32104, -418.5449 , -412.62817,\n",
              "       -521.55493, -414.99844, -440.67435, -463.6061 , -420.12646,\n",
              "       -416.39185, -513.68   , -454.6912 , -460.52005, -443.02942,\n",
              "       -464.94745, -452.5104 , -470.54938, -426.89627, -421.20755,\n",
              "       -481.4349 , -408.07852, -407.5873 , -473.6945 , -409.21838,\n",
              "       -411.14822, -479.00146, -413.97577, -462.8844 , -471.02194,\n",
              "       -513.1609 , -517.8032 , -469.223  , -481.2874 , -443.0332 ,\n",
              "       -410.65417, -513.6494 , -441.25516, -434.86633, -458.29306,\n",
              "       -462.08606, -435.5582 , -464.69373, -434.42883, -474.72525,\n",
              "       -376.26874, -501.81232, -413.52936, -444.86087, -418.1163 ,\n",
              "       -405.00757, -435.22946, -443.34912, -443.18646, -470.02222,\n",
              "       -439.98254, -417.20303, -377.4015 , -385.19785, -441.78366,\n",
              "       -454.7903 , -402.7788 , -467.03882, -457.7515 , -422.8841 ,\n",
              "       -428.71326, -438.01385, -446.06177, -382.74545, -475.37103,\n",
              "       -484.20596, -423.7153 , -420.7548 , -423.8322 , -452.5595 ,\n",
              "       -392.5137 , -464.00873, -375.1225 , -459.8976 , -431.6872 ,\n",
              "       -407.01892, -454.1528 , -405.38095, -455.91553, -453.7782 ,\n",
              "       -423.10638, -439.494  , -403.3263 , -386.62506, -428.13434,\n",
              "       -421.93167, -471.28308, -473.18396, -464.56476, -399.32123,\n",
              "       -413.09387, -481.6817 , -380.43036, -365.09543, -410.75797,\n",
              "       -405.67502, -416.07724, -461.75302, -438.8606 , -404.93027,\n",
              "       -392.26648, -452.835  , -448.45685, -413.7153 , -394.94235,\n",
              "       -421.18414, -447.04645, -389.36047, -387.3086 , -437.94302,\n",
              "       -420.1675 , -421.31525, -425.45074, -441.02634, -412.16312,\n",
              "       -409.21655, -412.8278 , -414.18164, -453.3949 , -406.41467,\n",
              "       -400.93872, -427.86765, -468.193  , -444.60718, -459.23962,\n",
              "       -418.8131 , -460.4038 , -439.90536, -465.9359 , -445.24417,\n",
              "       -421.23648, -446.16428, -367.194  , -474.5434 , -442.24258,\n",
              "       -522.2874 , -423.3954 , -417.8694 , -461.70255, -470.61038,\n",
              "       -487.77856, -491.2516 , -448.38986, -443.2527 , -487.9756 ,\n",
              "       -446.5495 , -441.62894, -459.56256, -426.17914, -504.21188,\n",
              "       -431.91074, -454.7633 , -466.36984, -471.5478 , -420.90936,\n",
              "       -489.41888, -493.87128, -421.56473, -429.95325, -443.65494,\n",
              "       -449.66565, -440.50952, -484.456  , -428.66187, -424.6715 ,\n",
              "       -466.64697, -449.63315, -463.29285, -379.56708, -483.3338 ,\n",
              "       -478.16122, -440.85495, -431.52588, -385.76328, -442.1517 ,\n",
              "       -449.0907 , -412.06152, -462.45517, -469.98242, -405.81763,\n",
              "       -459.4864 , -378.77286, -432.5484 , -462.3047 , -434.34827,\n",
              "       -407.74323, -386.1994 , -472.4163 , -419.2449 , -379.51093,\n",
              "       -363.5735 , -414.5706 , -435.86343, -446.7613 , -442.37482,\n",
              "       -415.02478, -449.23508, -440.10852, -404.7512 , -421.0112 ,\n",
              "       -460.42752, -403.8571 , -412.05164, -372.68307, -393.0507 ,\n",
              "       -394.76035, -451.00165, -408.7152 , -426.08667, -332.69867,\n",
              "       -411.78372, -467.7443 , -428.96658, -398.8699 , -395.0607 ,\n",
              "       -476.0034 , -452.18256, -456.82553, -463.91916, -397.55426,\n",
              "       -404.50937, -423.54108, -393.92816, -437.02637, -457.2802 ,\n",
              "       -412.13974, -471.60022, -464.36145, -420.7758 , -432.17923,\n",
              "       -404.80206, -417.062  , -465.24496, -393.34085, -436.95248,\n",
              "       -530.95605, -411.51193, -498.8898 , -457.2986 , -492.2898 ,\n",
              "       -455.44485, -477.11566, -411.96274, -457.94778, -360.47025,\n",
              "       -422.18552, -358.59656, -458.9268 , -542.5666 , -527.49243,\n",
              "       -465.86484, -427.65176, -386.80893, -406.23282, -427.612  ,\n",
              "       -441.11627, -485.59613, -464.9679 , -397.74173, -392.4652 ,\n",
              "       -447.6558 , -491.1511 , -442.5056 , -506.06097, -392.73694,\n",
              "       -431.76166, -403.50656, -429.41583, -478.05527, -427.36716,\n",
              "       -467.15588, -526.1957 , -437.9722 ])"
            ]
          },
          "metadata": {},
          "execution_count": 214
        }
      ],
      "source": [
        "df1['0'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "id": "72e890ce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72e890ce",
        "outputId": "f8970cdd-893f-4018-a526-53a1e99b5d10"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-469.48477, -447.1263 , -433.33972, -465.2206 , -407.61963,\n",
              "       -447.8849 , -415.96158, -425.45212, -381.5805 , -437.03818,\n",
              "       -525.19977, -474.65   , -456.05096, -436.3282 , -430.6762 ,\n",
              "       -493.1862 , -442.59415, -416.0819 , -435.4896 , -464.13718,\n",
              "       -427.06485, -468.5587 , -439.49243, -418.71957, -460.4001 ,\n",
              "       -390.45312, -429.79407, -420.2247 , -405.93726, -420.10004,\n",
              "       -434.36496, -413.8373 , -421.95407, -497.26077, -466.10834,\n",
              "       -467.4495 , -457.78012, -488.26974, -461.97726, -460.19077,\n",
              "       -451.02377, -547.34924, -443.90628, -473.36694, -467.0893 ,\n",
              "       -422.1737 , -470.8944 , -453.5756 , -479.2676 , -470.0053 ,\n",
              "       -482.287  , -416.1406 , -466.87482, -443.76602, -422.04797,\n",
              "       -409.2057 , -419.83594, -407.77988, -395.2997 , -393.9526 ,\n",
              "       -496.95926, -391.08258, -433.9074 , -403.1864 , -438.37827,\n",
              "       -484.93997, -481.92627, -502.14194, -464.8163 , -436.0145 ,\n",
              "       -427.8873 , -412.55634, -441.50995, -454.05173, -469.46295,\n",
              "       -477.94263, -485.03238, -471.95856, -439.35873, -372.10165,\n",
              "       -429.20193, -403.01038, -412.0556 , -414.32397, -468.89825,\n",
              "       -408.56897, -425.47263, -467.06317, -416.08627, -392.51147,\n",
              "       -542.9345 , -432.88263, -404.11523, -476.9987 , -441.25778,\n",
              "       -524.4419 , -396.28677, -423.83383, -408.97797, -390.08295,\n",
              "       -462.9359 , -450.64505, -435.4015 , -479.6645 , -475.25198,\n",
              "       -505.94534, -418.6249 ])"
            ]
          },
          "metadata": {},
          "execution_count": 215
        }
      ],
      "source": [
        "df2['0'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "id": "caeabe41",
      "metadata": {
        "id": "caeabe41"
      },
      "outputs": [],
      "source": [
        "x_train=df1.iloc[:,0:(df1.shape[1]-1)]\n",
        "y_train=df1.iloc[:,-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "id": "08d5857c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "08d5857c",
        "outputId": "d7855a46-3117-4f57-8325-2d062c65cf90"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Unnamed: 0          0          1          2          3          4  \\\n",
              "0             0 -482.45233  62.835957  -0.348998  26.264595   2.978607   \n",
              "1             1 -434.88647  41.972150 -29.416862  18.537344  -4.156565   \n",
              "2             2 -454.89886  45.067265  -0.193278  15.111545   3.080835   \n",
              "3             3 -451.45264  71.335200  12.223010  32.649555   3.997801   \n",
              "4             4 -478.92007  56.887730  -9.496516  12.799419  -0.196528   \n",
              "..          ...        ...        ...        ...        ...        ...   \n",
              "423         423 -478.05527   5.705422 -24.631056  27.622614 -19.235449   \n",
              "424         424 -427.36716  51.473750   4.835125  40.900140   6.937289   \n",
              "425         425 -467.15588  52.217026  10.470471  47.414780   8.690019   \n",
              "426         426 -526.19570  11.317784 -18.942173  29.540787 -28.057306   \n",
              "427         427 -437.97220   5.289146 -22.667547  25.394840 -28.545520   \n",
              "\n",
              "             5          6         7          8  ...       249       250  \\\n",
              "0     6.900927 -13.006243  0.273391  -9.196591  ...  0.635609  0.675823   \n",
              "1     5.257353 -11.410935 -8.983023 -11.285996  ...  0.600825  0.648380   \n",
              "2     4.204241 -10.440731 -6.615343 -16.249382  ...  0.506934  0.575784   \n",
              "3    15.200773  -2.812883 -1.384373  -6.467040  ...  0.696758  0.719856   \n",
              "4     3.286758  -9.909594 -2.337262  -7.694031  ...  0.649602  0.664169   \n",
              "..         ...        ...       ...        ...  ...       ...       ...   \n",
              "423  -7.040019 -13.293165 -9.629133 -14.067036  ...  0.597085  0.590422   \n",
              "424  13.700687  -8.331753 -0.583414  -6.279562  ...  0.671834  0.638155   \n",
              "425  15.601270  -2.032935  4.985774  -7.432734  ...  0.531997  0.523647   \n",
              "426   1.299599 -16.251814 -7.512440 -23.191568  ...  0.497285  0.528861   \n",
              "427   0.428451 -11.650926 -8.022680 -18.337156  ...  0.510255  0.495079   \n",
              "\n",
              "          251       252       253       254       255       256       257  \\\n",
              "0    0.684034  0.627376 -0.001574  0.012645 -0.027069  0.019313 -0.002252   \n",
              "1    0.675295  0.560000 -0.010576 -0.000228  0.011102 -0.074188  0.012116   \n",
              "2    0.523305  0.423375  0.009395 -0.025547 -0.035554 -0.025885  0.011198   \n",
              "3    0.667424  0.570905  0.003044 -0.006101 -0.009038 -0.049699  0.027615   \n",
              "4    0.700484  0.594266 -0.029971  0.019514 -0.009344 -0.040153  0.013369   \n",
              "..        ...       ...       ...       ...       ...       ...       ...   \n",
              "423  0.481957  0.481437 -0.011216 -0.031485 -0.029936  0.042160 -0.013815   \n",
              "424  0.574542  0.503516  0.019632  0.046315  0.074801  0.018078  0.022948   \n",
              "425  0.509723  0.498566  0.051532  0.015525 -0.006052  0.018201  0.004361   \n",
              "426  0.501070  0.496900  0.007856  0.005917 -0.045393 -0.004246 -0.001628   \n",
              "427  0.539387  0.527394 -0.001086  0.013948  0.013274 -0.056855 -0.001004   \n",
              "\n",
              "          258  \n",
              "0   -0.006220  \n",
              "1   -0.000779  \n",
              "2   -0.000163  \n",
              "3    0.021319  \n",
              "4   -0.006417  \n",
              "..        ...  \n",
              "423  0.009839  \n",
              "424 -0.016365  \n",
              "425 -0.021649  \n",
              "426  0.015051  \n",
              "427 -0.001355  \n",
              "\n",
              "[428 rows x 260 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3343b817-88ac-449e-ba95-a1a8bce3fe46\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>...</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "      <th>257</th>\n",
              "      <th>258</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-482.45233</td>\n",
              "      <td>62.835957</td>\n",
              "      <td>-0.348998</td>\n",
              "      <td>26.264595</td>\n",
              "      <td>2.978607</td>\n",
              "      <td>6.900927</td>\n",
              "      <td>-13.006243</td>\n",
              "      <td>0.273391</td>\n",
              "      <td>-9.196591</td>\n",
              "      <td>...</td>\n",
              "      <td>0.635609</td>\n",
              "      <td>0.675823</td>\n",
              "      <td>0.684034</td>\n",
              "      <td>0.627376</td>\n",
              "      <td>-0.001574</td>\n",
              "      <td>0.012645</td>\n",
              "      <td>-0.027069</td>\n",
              "      <td>0.019313</td>\n",
              "      <td>-0.002252</td>\n",
              "      <td>-0.006220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>-434.88647</td>\n",
              "      <td>41.972150</td>\n",
              "      <td>-29.416862</td>\n",
              "      <td>18.537344</td>\n",
              "      <td>-4.156565</td>\n",
              "      <td>5.257353</td>\n",
              "      <td>-11.410935</td>\n",
              "      <td>-8.983023</td>\n",
              "      <td>-11.285996</td>\n",
              "      <td>...</td>\n",
              "      <td>0.600825</td>\n",
              "      <td>0.648380</td>\n",
              "      <td>0.675295</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>-0.010576</td>\n",
              "      <td>-0.000228</td>\n",
              "      <td>0.011102</td>\n",
              "      <td>-0.074188</td>\n",
              "      <td>0.012116</td>\n",
              "      <td>-0.000779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>-454.89886</td>\n",
              "      <td>45.067265</td>\n",
              "      <td>-0.193278</td>\n",
              "      <td>15.111545</td>\n",
              "      <td>3.080835</td>\n",
              "      <td>4.204241</td>\n",
              "      <td>-10.440731</td>\n",
              "      <td>-6.615343</td>\n",
              "      <td>-16.249382</td>\n",
              "      <td>...</td>\n",
              "      <td>0.506934</td>\n",
              "      <td>0.575784</td>\n",
              "      <td>0.523305</td>\n",
              "      <td>0.423375</td>\n",
              "      <td>0.009395</td>\n",
              "      <td>-0.025547</td>\n",
              "      <td>-0.035554</td>\n",
              "      <td>-0.025885</td>\n",
              "      <td>0.011198</td>\n",
              "      <td>-0.000163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>-451.45264</td>\n",
              "      <td>71.335200</td>\n",
              "      <td>12.223010</td>\n",
              "      <td>32.649555</td>\n",
              "      <td>3.997801</td>\n",
              "      <td>15.200773</td>\n",
              "      <td>-2.812883</td>\n",
              "      <td>-1.384373</td>\n",
              "      <td>-6.467040</td>\n",
              "      <td>...</td>\n",
              "      <td>0.696758</td>\n",
              "      <td>0.719856</td>\n",
              "      <td>0.667424</td>\n",
              "      <td>0.570905</td>\n",
              "      <td>0.003044</td>\n",
              "      <td>-0.006101</td>\n",
              "      <td>-0.009038</td>\n",
              "      <td>-0.049699</td>\n",
              "      <td>0.027615</td>\n",
              "      <td>0.021319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>-478.92007</td>\n",
              "      <td>56.887730</td>\n",
              "      <td>-9.496516</td>\n",
              "      <td>12.799419</td>\n",
              "      <td>-0.196528</td>\n",
              "      <td>3.286758</td>\n",
              "      <td>-9.909594</td>\n",
              "      <td>-2.337262</td>\n",
              "      <td>-7.694031</td>\n",
              "      <td>...</td>\n",
              "      <td>0.649602</td>\n",
              "      <td>0.664169</td>\n",
              "      <td>0.700484</td>\n",
              "      <td>0.594266</td>\n",
              "      <td>-0.029971</td>\n",
              "      <td>0.019514</td>\n",
              "      <td>-0.009344</td>\n",
              "      <td>-0.040153</td>\n",
              "      <td>0.013369</td>\n",
              "      <td>-0.006417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>423</th>\n",
              "      <td>423</td>\n",
              "      <td>-478.05527</td>\n",
              "      <td>5.705422</td>\n",
              "      <td>-24.631056</td>\n",
              "      <td>27.622614</td>\n",
              "      <td>-19.235449</td>\n",
              "      <td>-7.040019</td>\n",
              "      <td>-13.293165</td>\n",
              "      <td>-9.629133</td>\n",
              "      <td>-14.067036</td>\n",
              "      <td>...</td>\n",
              "      <td>0.597085</td>\n",
              "      <td>0.590422</td>\n",
              "      <td>0.481957</td>\n",
              "      <td>0.481437</td>\n",
              "      <td>-0.011216</td>\n",
              "      <td>-0.031485</td>\n",
              "      <td>-0.029936</td>\n",
              "      <td>0.042160</td>\n",
              "      <td>-0.013815</td>\n",
              "      <td>0.009839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424</th>\n",
              "      <td>424</td>\n",
              "      <td>-427.36716</td>\n",
              "      <td>51.473750</td>\n",
              "      <td>4.835125</td>\n",
              "      <td>40.900140</td>\n",
              "      <td>6.937289</td>\n",
              "      <td>13.700687</td>\n",
              "      <td>-8.331753</td>\n",
              "      <td>-0.583414</td>\n",
              "      <td>-6.279562</td>\n",
              "      <td>...</td>\n",
              "      <td>0.671834</td>\n",
              "      <td>0.638155</td>\n",
              "      <td>0.574542</td>\n",
              "      <td>0.503516</td>\n",
              "      <td>0.019632</td>\n",
              "      <td>0.046315</td>\n",
              "      <td>0.074801</td>\n",
              "      <td>0.018078</td>\n",
              "      <td>0.022948</td>\n",
              "      <td>-0.016365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>425</th>\n",
              "      <td>425</td>\n",
              "      <td>-467.15588</td>\n",
              "      <td>52.217026</td>\n",
              "      <td>10.470471</td>\n",
              "      <td>47.414780</td>\n",
              "      <td>8.690019</td>\n",
              "      <td>15.601270</td>\n",
              "      <td>-2.032935</td>\n",
              "      <td>4.985774</td>\n",
              "      <td>-7.432734</td>\n",
              "      <td>...</td>\n",
              "      <td>0.531997</td>\n",
              "      <td>0.523647</td>\n",
              "      <td>0.509723</td>\n",
              "      <td>0.498566</td>\n",
              "      <td>0.051532</td>\n",
              "      <td>0.015525</td>\n",
              "      <td>-0.006052</td>\n",
              "      <td>0.018201</td>\n",
              "      <td>0.004361</td>\n",
              "      <td>-0.021649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>426</th>\n",
              "      <td>426</td>\n",
              "      <td>-526.19570</td>\n",
              "      <td>11.317784</td>\n",
              "      <td>-18.942173</td>\n",
              "      <td>29.540787</td>\n",
              "      <td>-28.057306</td>\n",
              "      <td>1.299599</td>\n",
              "      <td>-16.251814</td>\n",
              "      <td>-7.512440</td>\n",
              "      <td>-23.191568</td>\n",
              "      <td>...</td>\n",
              "      <td>0.497285</td>\n",
              "      <td>0.528861</td>\n",
              "      <td>0.501070</td>\n",
              "      <td>0.496900</td>\n",
              "      <td>0.007856</td>\n",
              "      <td>0.005917</td>\n",
              "      <td>-0.045393</td>\n",
              "      <td>-0.004246</td>\n",
              "      <td>-0.001628</td>\n",
              "      <td>0.015051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>427</th>\n",
              "      <td>427</td>\n",
              "      <td>-437.97220</td>\n",
              "      <td>5.289146</td>\n",
              "      <td>-22.667547</td>\n",
              "      <td>25.394840</td>\n",
              "      <td>-28.545520</td>\n",
              "      <td>0.428451</td>\n",
              "      <td>-11.650926</td>\n",
              "      <td>-8.022680</td>\n",
              "      <td>-18.337156</td>\n",
              "      <td>...</td>\n",
              "      <td>0.510255</td>\n",
              "      <td>0.495079</td>\n",
              "      <td>0.539387</td>\n",
              "      <td>0.527394</td>\n",
              "      <td>-0.001086</td>\n",
              "      <td>0.013948</td>\n",
              "      <td>0.013274</td>\n",
              "      <td>-0.056855</td>\n",
              "      <td>-0.001004</td>\n",
              "      <td>-0.001355</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>428 rows × 260 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3343b817-88ac-449e-ba95-a1a8bce3fe46')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3343b817-88ac-449e-ba95-a1a8bce3fe46 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3343b817-88ac-449e-ba95-a1a8bce3fe46');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a3dfd675-9e9b-4374-958a-2573d3a7db5e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a3dfd675-9e9b-4374-958a-2573d3a7db5e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a3dfd675-9e9b-4374-958a-2573d3a7db5e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "x_train"
            }
          },
          "metadata": {},
          "execution_count": 217
        }
      ],
      "source": [
        "x_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "id": "d2127f10",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2127f10",
        "outputId": "39d758b1-5b45-47c4-ca6f-cb330f0e11f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      0\n",
              "1      2\n",
              "2      0\n",
              "3      3\n",
              "4      0\n",
              "      ..\n",
              "423    0\n",
              "424    3\n",
              "425    3\n",
              "426    2\n",
              "427    2\n",
              "Name: 0.1, Length: 428, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 218
        }
      ],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "id": "7311d746",
      "metadata": {
        "id": "7311d746"
      },
      "outputs": [],
      "source": [
        "x_test=df2.iloc[:,0:(df2.shape[1]-1)]\n",
        "y_test=df2.iloc[:,-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "id": "ae86f98c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "ae86f98c",
        "outputId": "36e7583f-2bf7-4764-9c0c-cf6e264135b0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Unnamed: 0          0          1          2          3          4  \\\n",
              "0             0 -469.48477  88.400730  -7.127512  29.156132   5.335554   \n",
              "1             1 -447.12630  86.114920   4.772520  38.097256   8.324276   \n",
              "2             2 -433.33972  29.850212  -8.025759  23.749544   1.409119   \n",
              "3             3 -465.22060  37.216230 -27.930758  18.472618  -4.044474   \n",
              "4             4 -407.61963  39.778786  -9.883366   9.930613  -1.744454   \n",
              "..          ...        ...        ...        ...        ...        ...   \n",
              "102         102 -435.40150  17.910067 -32.667010  20.082544 -26.186550   \n",
              "103         103 -479.66450   7.133218 -38.469020   7.929045 -32.369520   \n",
              "104         104 -475.25198  48.588770  -2.106867  34.655080   3.924164   \n",
              "105         105 -505.94534  -1.201753 -28.506739  24.164959 -38.382088   \n",
              "106         106 -418.62490  57.015880   6.383415  47.618423  -8.051490   \n",
              "\n",
              "            5          6          7          8  ...       249       250  \\\n",
              "0    7.147227  -6.727572  -8.307674  -3.364513  ...  0.646426  0.635522   \n",
              "1    8.538317  -4.507682  -7.680664  -7.317249  ...  0.698287  0.615263   \n",
              "2    8.578079  -4.816757  -7.580163 -22.596510  ...  0.586567  0.574034   \n",
              "3    3.874021 -12.937754 -11.636196 -20.833626  ...  0.496986  0.525122   \n",
              "4    6.709172 -12.445559  -3.181297 -17.876530  ...  0.591737  0.621760   \n",
              "..        ...        ...        ...        ...  ...       ...       ...   \n",
              "102  4.677832 -12.905549  -9.618332 -18.023060  ...  0.598869  0.536450   \n",
              "103  4.264574 -14.345546  -6.981534 -17.207087  ...  0.571921  0.540806   \n",
              "104 -7.597296 -17.605679 -13.560740 -15.469123  ...  0.663843  0.650717   \n",
              "105  1.337496 -13.754961  -5.442271 -14.897131  ...  0.595059  0.499615   \n",
              "106  4.213936 -15.029120  -2.448467 -10.805821  ...  0.566316  0.569263   \n",
              "\n",
              "          251       252       253       254       255       256       257  \\\n",
              "0    0.544438  0.532856 -0.018488  0.011973 -0.011038  0.079758 -0.021044   \n",
              "1    0.566964  0.605103 -0.008389 -0.051995 -0.056544 -0.020014  0.013964   \n",
              "2    0.558508  0.499983 -0.009560  0.001801 -0.004441 -0.018945  0.011005   \n",
              "3    0.552413  0.580766  0.016770  0.010151  0.024988  0.011547 -0.001119   \n",
              "4    0.691176  0.645503  0.023878  0.001674  0.000677 -0.006433 -0.002290   \n",
              "..        ...       ...       ...       ...       ...       ...       ...   \n",
              "102  0.517381  0.519304 -0.007135 -0.007543  0.027781 -0.004844  0.001824   \n",
              "103  0.580027  0.603574 -0.018925 -0.009150  0.035872  0.004143 -0.000523   \n",
              "104  0.574044  0.481354 -0.014255  0.003728 -0.059519 -0.000201  0.013722   \n",
              "105  0.552792  0.578515 -0.030600 -0.005539  0.033227  0.001151 -0.006986   \n",
              "106  0.551809  0.576764  0.016046 -0.025610  0.025426 -0.094371  0.008999   \n",
              "\n",
              "          258  \n",
              "0   -0.019445  \n",
              "1    0.008349  \n",
              "2   -0.011105  \n",
              "3    0.005264  \n",
              "4   -0.001868  \n",
              "..        ...  \n",
              "102  0.008224  \n",
              "103 -0.005385  \n",
              "104 -0.000998  \n",
              "105 -0.002342  \n",
              "106 -0.004985  \n",
              "\n",
              "[107 rows x 260 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4c97d97b-48af-4592-98f5-0a10a76dbe7e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>...</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "      <th>257</th>\n",
              "      <th>258</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-469.48477</td>\n",
              "      <td>88.400730</td>\n",
              "      <td>-7.127512</td>\n",
              "      <td>29.156132</td>\n",
              "      <td>5.335554</td>\n",
              "      <td>7.147227</td>\n",
              "      <td>-6.727572</td>\n",
              "      <td>-8.307674</td>\n",
              "      <td>-3.364513</td>\n",
              "      <td>...</td>\n",
              "      <td>0.646426</td>\n",
              "      <td>0.635522</td>\n",
              "      <td>0.544438</td>\n",
              "      <td>0.532856</td>\n",
              "      <td>-0.018488</td>\n",
              "      <td>0.011973</td>\n",
              "      <td>-0.011038</td>\n",
              "      <td>0.079758</td>\n",
              "      <td>-0.021044</td>\n",
              "      <td>-0.019445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>-447.12630</td>\n",
              "      <td>86.114920</td>\n",
              "      <td>4.772520</td>\n",
              "      <td>38.097256</td>\n",
              "      <td>8.324276</td>\n",
              "      <td>8.538317</td>\n",
              "      <td>-4.507682</td>\n",
              "      <td>-7.680664</td>\n",
              "      <td>-7.317249</td>\n",
              "      <td>...</td>\n",
              "      <td>0.698287</td>\n",
              "      <td>0.615263</td>\n",
              "      <td>0.566964</td>\n",
              "      <td>0.605103</td>\n",
              "      <td>-0.008389</td>\n",
              "      <td>-0.051995</td>\n",
              "      <td>-0.056544</td>\n",
              "      <td>-0.020014</td>\n",
              "      <td>0.013964</td>\n",
              "      <td>0.008349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>-433.33972</td>\n",
              "      <td>29.850212</td>\n",
              "      <td>-8.025759</td>\n",
              "      <td>23.749544</td>\n",
              "      <td>1.409119</td>\n",
              "      <td>8.578079</td>\n",
              "      <td>-4.816757</td>\n",
              "      <td>-7.580163</td>\n",
              "      <td>-22.596510</td>\n",
              "      <td>...</td>\n",
              "      <td>0.586567</td>\n",
              "      <td>0.574034</td>\n",
              "      <td>0.558508</td>\n",
              "      <td>0.499983</td>\n",
              "      <td>-0.009560</td>\n",
              "      <td>0.001801</td>\n",
              "      <td>-0.004441</td>\n",
              "      <td>-0.018945</td>\n",
              "      <td>0.011005</td>\n",
              "      <td>-0.011105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>-465.22060</td>\n",
              "      <td>37.216230</td>\n",
              "      <td>-27.930758</td>\n",
              "      <td>18.472618</td>\n",
              "      <td>-4.044474</td>\n",
              "      <td>3.874021</td>\n",
              "      <td>-12.937754</td>\n",
              "      <td>-11.636196</td>\n",
              "      <td>-20.833626</td>\n",
              "      <td>...</td>\n",
              "      <td>0.496986</td>\n",
              "      <td>0.525122</td>\n",
              "      <td>0.552413</td>\n",
              "      <td>0.580766</td>\n",
              "      <td>0.016770</td>\n",
              "      <td>0.010151</td>\n",
              "      <td>0.024988</td>\n",
              "      <td>0.011547</td>\n",
              "      <td>-0.001119</td>\n",
              "      <td>0.005264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>-407.61963</td>\n",
              "      <td>39.778786</td>\n",
              "      <td>-9.883366</td>\n",
              "      <td>9.930613</td>\n",
              "      <td>-1.744454</td>\n",
              "      <td>6.709172</td>\n",
              "      <td>-12.445559</td>\n",
              "      <td>-3.181297</td>\n",
              "      <td>-17.876530</td>\n",
              "      <td>...</td>\n",
              "      <td>0.591737</td>\n",
              "      <td>0.621760</td>\n",
              "      <td>0.691176</td>\n",
              "      <td>0.645503</td>\n",
              "      <td>0.023878</td>\n",
              "      <td>0.001674</td>\n",
              "      <td>0.000677</td>\n",
              "      <td>-0.006433</td>\n",
              "      <td>-0.002290</td>\n",
              "      <td>-0.001868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>102</td>\n",
              "      <td>-435.40150</td>\n",
              "      <td>17.910067</td>\n",
              "      <td>-32.667010</td>\n",
              "      <td>20.082544</td>\n",
              "      <td>-26.186550</td>\n",
              "      <td>4.677832</td>\n",
              "      <td>-12.905549</td>\n",
              "      <td>-9.618332</td>\n",
              "      <td>-18.023060</td>\n",
              "      <td>...</td>\n",
              "      <td>0.598869</td>\n",
              "      <td>0.536450</td>\n",
              "      <td>0.517381</td>\n",
              "      <td>0.519304</td>\n",
              "      <td>-0.007135</td>\n",
              "      <td>-0.007543</td>\n",
              "      <td>0.027781</td>\n",
              "      <td>-0.004844</td>\n",
              "      <td>0.001824</td>\n",
              "      <td>0.008224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>103</td>\n",
              "      <td>-479.66450</td>\n",
              "      <td>7.133218</td>\n",
              "      <td>-38.469020</td>\n",
              "      <td>7.929045</td>\n",
              "      <td>-32.369520</td>\n",
              "      <td>4.264574</td>\n",
              "      <td>-14.345546</td>\n",
              "      <td>-6.981534</td>\n",
              "      <td>-17.207087</td>\n",
              "      <td>...</td>\n",
              "      <td>0.571921</td>\n",
              "      <td>0.540806</td>\n",
              "      <td>0.580027</td>\n",
              "      <td>0.603574</td>\n",
              "      <td>-0.018925</td>\n",
              "      <td>-0.009150</td>\n",
              "      <td>0.035872</td>\n",
              "      <td>0.004143</td>\n",
              "      <td>-0.000523</td>\n",
              "      <td>-0.005385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>104</td>\n",
              "      <td>-475.25198</td>\n",
              "      <td>48.588770</td>\n",
              "      <td>-2.106867</td>\n",
              "      <td>34.655080</td>\n",
              "      <td>3.924164</td>\n",
              "      <td>-7.597296</td>\n",
              "      <td>-17.605679</td>\n",
              "      <td>-13.560740</td>\n",
              "      <td>-15.469123</td>\n",
              "      <td>...</td>\n",
              "      <td>0.663843</td>\n",
              "      <td>0.650717</td>\n",
              "      <td>0.574044</td>\n",
              "      <td>0.481354</td>\n",
              "      <td>-0.014255</td>\n",
              "      <td>0.003728</td>\n",
              "      <td>-0.059519</td>\n",
              "      <td>-0.000201</td>\n",
              "      <td>0.013722</td>\n",
              "      <td>-0.000998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>105</td>\n",
              "      <td>-505.94534</td>\n",
              "      <td>-1.201753</td>\n",
              "      <td>-28.506739</td>\n",
              "      <td>24.164959</td>\n",
              "      <td>-38.382088</td>\n",
              "      <td>1.337496</td>\n",
              "      <td>-13.754961</td>\n",
              "      <td>-5.442271</td>\n",
              "      <td>-14.897131</td>\n",
              "      <td>...</td>\n",
              "      <td>0.595059</td>\n",
              "      <td>0.499615</td>\n",
              "      <td>0.552792</td>\n",
              "      <td>0.578515</td>\n",
              "      <td>-0.030600</td>\n",
              "      <td>-0.005539</td>\n",
              "      <td>0.033227</td>\n",
              "      <td>0.001151</td>\n",
              "      <td>-0.006986</td>\n",
              "      <td>-0.002342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>106</td>\n",
              "      <td>-418.62490</td>\n",
              "      <td>57.015880</td>\n",
              "      <td>6.383415</td>\n",
              "      <td>47.618423</td>\n",
              "      <td>-8.051490</td>\n",
              "      <td>4.213936</td>\n",
              "      <td>-15.029120</td>\n",
              "      <td>-2.448467</td>\n",
              "      <td>-10.805821</td>\n",
              "      <td>...</td>\n",
              "      <td>0.566316</td>\n",
              "      <td>0.569263</td>\n",
              "      <td>0.551809</td>\n",
              "      <td>0.576764</td>\n",
              "      <td>0.016046</td>\n",
              "      <td>-0.025610</td>\n",
              "      <td>0.025426</td>\n",
              "      <td>-0.094371</td>\n",
              "      <td>0.008999</td>\n",
              "      <td>-0.004985</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>107 rows × 260 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4c97d97b-48af-4592-98f5-0a10a76dbe7e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4c97d97b-48af-4592-98f5-0a10a76dbe7e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4c97d97b-48af-4592-98f5-0a10a76dbe7e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-26057fc5-9e0a-484a-90cf-dc4df248e9d3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-26057fc5-9e0a-484a-90cf-dc4df248e9d3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-26057fc5-9e0a-484a-90cf-dc4df248e9d3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "x_test"
            }
          },
          "metadata": {},
          "execution_count": 220
        }
      ],
      "source": [
        "x_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "id": "62579094",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62579094",
        "outputId": "aafb0c52-e4bb-48cc-f0f1-bc9898af4514"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      1\n",
              "1      1\n",
              "2      2\n",
              "3      2\n",
              "4      4\n",
              "      ..\n",
              "102    0\n",
              "103    2\n",
              "104    6\n",
              "105    2\n",
              "106    5\n",
              "Name: 0.1, Length: 107, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 221
        }
      ],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "id": "3beb4ef2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "3beb4ef2",
        "outputId": "1b0210b1-b6c6-4df5-8863-d2dbc83fe329"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: ylabel='count'>"
            ]
          },
          "metadata": {},
          "execution_count": 222
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "sns.countplot(df1['0'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "id": "d58a5824",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "d58a5824",
        "outputId": "4810733e-145a-4c3c-ac2c-7098ea5efc84"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: ylabel='count'>"
            ]
          },
          "metadata": {},
          "execution_count": 223
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "sns.countplot(df2['0'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "id": "f4bf0f42",
      "metadata": {
        "id": "f4bf0f42"
      },
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "id": "5bd3a370",
      "metadata": {
        "id": "5bd3a370"
      },
      "outputs": [],
      "source": [
        "# x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "id": "7b9f98bc",
      "metadata": {
        "id": "7b9f98bc"
      },
      "outputs": [],
      "source": [
        "# x_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "id": "c8d669e4",
      "metadata": {
        "id": "c8d669e4"
      },
      "outputs": [],
      "source": [
        "# y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "id": "3e5f00f5",
      "metadata": {
        "id": "3e5f00f5"
      },
      "outputs": [],
      "source": [
        "# x_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "id": "6d201bc1",
      "metadata": {
        "id": "6d201bc1"
      },
      "outputs": [],
      "source": [
        "# y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 230,
      "id": "000082a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "000082a7",
        "outputId": "c5bbc74b-aece-4cf1-b814-5a0aafd3350a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 4, 3, 0, 6, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 230
        }
      ],
      "source": [
        "#Check unique values for y_test\n",
        "y_test.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "id": "0c50dd63",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c50dd63",
        "outputId": "a4bc584d-cc40-4c64-f15e-212eaff3cfb2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 2, 3, 5, 1, 4, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 231
        }
      ],
      "source": [
        "#Check unique values for y_train\n",
        "y_train.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "id": "2f69e286",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f69e286",
        "outputId": "5510ce6c-9d71-45e9-a926-c04da672952b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 1.0726817042606516,\n",
              " 1: 0.9705215419501134,\n",
              " 2: 0.5994397759103641,\n",
              " 3: 1.2478134110787171,\n",
              " 4: 1.0918367346938775,\n",
              " 5: 0.9553571428571429,\n",
              " 6: 1.6525096525096525}"
            ]
          },
          "metadata": {},
          "execution_count": 232
        }
      ],
      "source": [
        "from sklearn.utils import compute_class_weight\n",
        "class_weights = compute_class_weight(\n",
        "                                        class_weight = \"balanced\",\n",
        "                                        classes = np.unique(y_train),\n",
        "                                        y = y_train\n",
        "                                    )\n",
        "class_weights = dict(zip(np.unique(y_train), class_weights))\n",
        "class_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "id": "87b553a9",
      "metadata": {
        "id": "87b553a9"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "#Normalize the data\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(x_train)\n",
        "X_train_scalled = scaler.transform(x_train)\n",
        "X_test_scalled = scaler.transform(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "id": "7b62969d",
      "metadata": {
        "id": "7b62969d"
      },
      "outputs": [],
      "source": [
        "#Import packages for CNN\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, Conv1D\n",
        "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, BatchNormalization, Flatten, MaxPooling1D\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.regularizers import l2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "id": "c598744f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c598744f",
        "outputId": "51e3704d-5280-4bda-83ca-e84e28cd692d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[354, 729, 415, 878]\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "res = []\n",
        "for j in range(4):\n",
        "    res.append(random.randint(300, 1000))\n",
        "# res.sort(reverse=True)\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(260, 1)))\n",
        "model.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='sigmoid'))\n",
        "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# model.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='sigmoid'))\n",
        "# model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Dropout(0.2))\n",
        "\n",
        "# model.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='sigmoid'))\n",
        "# model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Dropout(0.3))\n",
        "##\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "model.add(LSTM(256))\n",
        "\n",
        "\n",
        "model.add(Dense(256, activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(256, activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "##\n",
        "# model.add(Dense(64, activation='sigmoid'))\n",
        "# # model.add(BatchNormalization())\n",
        "# model.add(Dropout(0.3))\n",
        "\n",
        "# model.add(Dense(32, activation='sigmoid'))\n",
        "# # model.add(BatchNormalization())\n",
        "# model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(7, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Q6P4AQ4Mrdt",
        "outputId": "ceda872c-619c-4cc0-eb6e-9504c0052456"
      },
      "id": "_Q6P4AQ4Mrdt",
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_16 (Conv1D)          (None, 260, 256)          1536      \n",
            "                                                                 \n",
            " max_pooling1d_16 (MaxPooli  (None, 130, 256)          0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_16 (Ba  (None, 130, 256)          1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv1d_17 (Conv1D)          (None, 130, 256)          327936    \n",
            "                                                                 \n",
            " max_pooling1d_17 (MaxPooli  (None, 65, 256)           0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_17 (Ba  (None, 65, 256)           1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_16 (Dropout)        (None, 65, 256)           0         \n",
            "                                                                 \n",
            " conv1d_18 (Conv1D)          (None, 65, 256)           327936    \n",
            "                                                                 \n",
            " max_pooling1d_18 (MaxPooli  (None, 33, 256)           0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_18 (Ba  (None, 33, 256)           1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_17 (Dropout)        (None, 33, 256)           0         \n",
            "                                                                 \n",
            " conv1d_19 (Conv1D)          (None, 33, 128)           163968    \n",
            "                                                                 \n",
            " max_pooling1d_19 (MaxPooli  (None, 17, 128)           0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_19 (Ba  (None, 17, 128)           512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_18 (Dropout)        (None, 17, 128)           0         \n",
            "                                                                 \n",
            " lstm_8 (LSTM)               (None, 17, 256)           394240    \n",
            "                                                                 \n",
            " lstm_9 (LSTM)               (None, 256)               525312    \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dropout_19 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 7)                 1799      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1877895 (7.16 MB)\n",
            "Trainable params: 1876103 (7.16 MB)\n",
            "Non-trainable params: 1792 (7.00 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tempfile\n",
        "initial_weights = os.path.join(tempfile.mkdtemp(), 'initial_weights')\n",
        "model.save_weights(initial_weights)"
      ],
      "metadata": {
        "id": "JCEBlr_YMzuM"
      },
      "id": "JCEBlr_YMzuM",
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(initial_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8l3JMDetM3-D",
        "outputId": "86d7473d-7d81-47ee-f7b7-c673706404e7"
      },
      "id": "8l3JMDetM3-D",
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7a5244b45870>"
            ]
          },
          "metadata": {},
          "execution_count": 238
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimiser = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=optimiser,\n",
        "              loss='sparse_categorical_crossentropy',                             #CategoricalCrossentropy\n",
        "              metrics=['SparseCategoricalAccuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WglAV3TMM7KM",
        "outputId": "bf7cd0c6-6096-4962-b954-5042f825cb6c"
      },
      "id": "WglAV3TMM7KM",
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_16 (Conv1D)          (None, 260, 256)          1536      \n",
            "                                                                 \n",
            " max_pooling1d_16 (MaxPooli  (None, 130, 256)          0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_16 (Ba  (None, 130, 256)          1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv1d_17 (Conv1D)          (None, 130, 256)          327936    \n",
            "                                                                 \n",
            " max_pooling1d_17 (MaxPooli  (None, 65, 256)           0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_17 (Ba  (None, 65, 256)           1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_16 (Dropout)        (None, 65, 256)           0         \n",
            "                                                                 \n",
            " conv1d_18 (Conv1D)          (None, 65, 256)           327936    \n",
            "                                                                 \n",
            " max_pooling1d_18 (MaxPooli  (None, 33, 256)           0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_18 (Ba  (None, 33, 256)           1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_17 (Dropout)        (None, 33, 256)           0         \n",
            "                                                                 \n",
            " conv1d_19 (Conv1D)          (None, 33, 128)           163968    \n",
            "                                                                 \n",
            " max_pooling1d_19 (MaxPooli  (None, 17, 128)           0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_19 (Ba  (None, 17, 128)           512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_18 (Dropout)        (None, 17, 128)           0         \n",
            "                                                                 \n",
            " lstm_8 (LSTM)               (None, 17, 256)           394240    \n",
            "                                                                 \n",
            " lstm_9 (LSTM)               (None, 256)               525312    \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dropout_19 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 7)                 1799      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1877895 (7.16 MB)\n",
            "Trainable params: 1876103 (7.16 MB)\n",
            "Non-trainable params: 1792 (7.00 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path='cnn_lstm_emodb3.ckpt'\n",
        "checkpoint_dir=os.path.dirname(checkpoint_path)\n",
        "callback1=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, monitor='val_sparse_categorical_accuracy', verbose=1,\n",
        "   save_best_only=True,save_weights_only=True,)\n",
        "callback2=tf.keras.callbacks.EarlyStopping(monitor='val_sparse_categorical_accuracy',min_delta=0, patience=50, verbose=0, mode='auto',baseline=None,restore_best_weights=True)\n",
        "cp_callback=[callback1,callback2]"
      ],
      "metadata": {
        "id": "Kg0_S4kRM8DV"
      },
      "id": "Kg0_S4kRM8DV",
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train_scalled, y_train, validation_data=(X_test_scalled, y_test), batch_size=64, epochs=900, verbose=1,class_weight=class_weights,callbacks=cp_callback)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giDUxkTnNGXy",
        "outputId": "f114b0dd-942e-49f3-c0cf-a28d7553f38d"
      },
      "id": "giDUxkTnNGXy",
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.9217 - sparse_categorical_accuracy: 0.2056\n",
            "Epoch 1: val_sparse_categorical_accuracy improved from -inf to 0.15888, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 7s 206ms/step - loss: 1.9217 - sparse_categorical_accuracy: 0.2056 - val_loss: 1.9547 - val_sparse_categorical_accuracy: 0.1589\n",
            "Epoch 2/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.8699 - sparse_categorical_accuracy: 0.2687\n",
            "Epoch 2: val_sparse_categorical_accuracy did not improve from 0.15888\n",
            "7/7 [==============================] - 0s 35ms/step - loss: 1.8699 - sparse_categorical_accuracy: 0.2687 - val_loss: 1.9527 - val_sparse_categorical_accuracy: 0.1589\n",
            "Epoch 3/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.8291 - sparse_categorical_accuracy: 0.3131\n",
            "Epoch 3: val_sparse_categorical_accuracy did not improve from 0.15888\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 1.8291 - sparse_categorical_accuracy: 0.3131 - val_loss: 1.9506 - val_sparse_categorical_accuracy: 0.1589\n",
            "Epoch 4/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.7966 - sparse_categorical_accuracy: 0.3131\n",
            "Epoch 4: val_sparse_categorical_accuracy did not improve from 0.15888\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 1.7966 - sparse_categorical_accuracy: 0.3131 - val_loss: 1.9497 - val_sparse_categorical_accuracy: 0.1589\n",
            "Epoch 5/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.7618 - sparse_categorical_accuracy: 0.3575\n",
            "Epoch 5: val_sparse_categorical_accuracy did not improve from 0.15888\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 1.7618 - sparse_categorical_accuracy: 0.3575 - val_loss: 1.9507 - val_sparse_categorical_accuracy: 0.1589\n",
            "Epoch 6/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.7102 - sparse_categorical_accuracy: 0.3879\n",
            "Epoch 6: val_sparse_categorical_accuracy improved from 0.15888 to 0.19626, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 1.7102 - sparse_categorical_accuracy: 0.3879 - val_loss: 1.9570 - val_sparse_categorical_accuracy: 0.1963\n",
            "Epoch 7/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.6597 - sparse_categorical_accuracy: 0.4206\n",
            "Epoch 7: val_sparse_categorical_accuracy did not improve from 0.19626\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 1.6597 - sparse_categorical_accuracy: 0.4206 - val_loss: 1.9776 - val_sparse_categorical_accuracy: 0.1682\n",
            "Epoch 8/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.5898 - sparse_categorical_accuracy: 0.4393\n",
            "Epoch 8: val_sparse_categorical_accuracy improved from 0.19626 to 0.20561, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 1.5898 - sparse_categorical_accuracy: 0.4393 - val_loss: 2.0262 - val_sparse_categorical_accuracy: 0.2056\n",
            "Epoch 9/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 1.4919 - sparse_categorical_accuracy: 0.4714\n",
            "Epoch 9: val_sparse_categorical_accuracy did not improve from 0.20561\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 1.5012 - sparse_categorical_accuracy: 0.4603 - val_loss: 2.1292 - val_sparse_categorical_accuracy: 0.1682\n",
            "Epoch 10/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.3855 - sparse_categorical_accuracy: 0.4836\n",
            "Epoch 10: val_sparse_categorical_accuracy did not improve from 0.20561\n",
            "7/7 [==============================] - 0s 30ms/step - loss: 1.3855 - sparse_categorical_accuracy: 0.4836 - val_loss: 2.2859 - val_sparse_categorical_accuracy: 0.1869\n",
            "Epoch 11/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.3055 - sparse_categorical_accuracy: 0.4836\n",
            "Epoch 11: val_sparse_categorical_accuracy did not improve from 0.20561\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 1.3055 - sparse_categorical_accuracy: 0.4836 - val_loss: 2.4422 - val_sparse_categorical_accuracy: 0.2056\n",
            "Epoch 12/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.1970 - sparse_categorical_accuracy: 0.5374\n",
            "Epoch 12: val_sparse_categorical_accuracy did not improve from 0.20561\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 1.1970 - sparse_categorical_accuracy: 0.5374 - val_loss: 2.6873 - val_sparse_categorical_accuracy: 0.1589\n",
            "Epoch 13/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.0967 - sparse_categorical_accuracy: 0.5888\n",
            "Epoch 13: val_sparse_categorical_accuracy did not improve from 0.20561\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 1.0967 - sparse_categorical_accuracy: 0.5888 - val_loss: 2.7051 - val_sparse_categorical_accuracy: 0.1589\n",
            "Epoch 14/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.0484 - sparse_categorical_accuracy: 0.5935\n",
            "Epoch 14: val_sparse_categorical_accuracy did not improve from 0.20561\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 1.0484 - sparse_categorical_accuracy: 0.5935 - val_loss: 2.7246 - val_sparse_categorical_accuracy: 0.1589\n",
            "Epoch 15/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.9480 - sparse_categorical_accuracy: 0.6262\n",
            "Epoch 15: val_sparse_categorical_accuracy did not improve from 0.20561\n",
            "7/7 [==============================] - 0s 36ms/step - loss: 0.9480 - sparse_categorical_accuracy: 0.6262 - val_loss: 3.0733 - val_sparse_categorical_accuracy: 0.2056\n",
            "Epoch 16/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.8315 - sparse_categorical_accuracy: 0.6849\n",
            "Epoch 16: val_sparse_categorical_accuracy did not improve from 0.20561\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 0.8422 - sparse_categorical_accuracy: 0.6752 - val_loss: 2.8231 - val_sparse_categorical_accuracy: 0.1589\n",
            "Epoch 17/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.7804 - sparse_categorical_accuracy: 0.6986\n",
            "Epoch 17: val_sparse_categorical_accuracy did not improve from 0.20561\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.7804 - sparse_categorical_accuracy: 0.6986 - val_loss: 2.8897 - val_sparse_categorical_accuracy: 0.1869\n",
            "Epoch 18/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.6994 - sparse_categorical_accuracy: 0.7057\n",
            "Epoch 18: val_sparse_categorical_accuracy did not improve from 0.20561\n",
            "7/7 [==============================] - 0s 35ms/step - loss: 0.7043 - sparse_categorical_accuracy: 0.7103 - val_loss: 2.5817 - val_sparse_categorical_accuracy: 0.1589\n",
            "Epoch 19/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.6297 - sparse_categorical_accuracy: 0.7188\n",
            "Epoch 19: val_sparse_categorical_accuracy did not improve from 0.20561\n",
            "7/7 [==============================] - 0s 30ms/step - loss: 0.6262 - sparse_categorical_accuracy: 0.7196 - val_loss: 2.1985 - val_sparse_categorical_accuracy: 0.1589\n",
            "Epoch 20/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.5660 - sparse_categorical_accuracy: 0.7664\n",
            "Epoch 20: val_sparse_categorical_accuracy improved from 0.20561 to 0.21495, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 52ms/step - loss: 0.5660 - sparse_categorical_accuracy: 0.7664 - val_loss: 1.9452 - val_sparse_categorical_accuracy: 0.2150\n",
            "Epoch 21/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.5648 - sparse_categorical_accuracy: 0.7523\n",
            "Epoch 21: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.5648 - sparse_categorical_accuracy: 0.7523 - val_loss: 2.0091 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 22/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.4982 - sparse_categorical_accuracy: 0.7780\n",
            "Epoch 22: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.4982 - sparse_categorical_accuracy: 0.7780 - val_loss: 2.1720 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 23/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.4668 - sparse_categorical_accuracy: 0.8061\n",
            "Epoch 23: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.4668 - sparse_categorical_accuracy: 0.8061 - val_loss: 2.3342 - val_sparse_categorical_accuracy: 0.1402\n",
            "Epoch 24/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.4489 - sparse_categorical_accuracy: 0.8271\n",
            "Epoch 24: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.4489 - sparse_categorical_accuracy: 0.8271 - val_loss: 2.6526 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 25/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.4188 - sparse_categorical_accuracy: 0.8294\n",
            "Epoch 25: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.4188 - sparse_categorical_accuracy: 0.8294 - val_loss: 2.9074 - val_sparse_categorical_accuracy: 0.1402\n",
            "Epoch 26/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.4013 - sparse_categorical_accuracy: 0.8435\n",
            "Epoch 26: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.4013 - sparse_categorical_accuracy: 0.8435 - val_loss: 3.2755 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 27/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.3585 - sparse_categorical_accuracy: 0.8505\n",
            "Epoch 27: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.3585 - sparse_categorical_accuracy: 0.8505 - val_loss: 3.3588 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 28/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.3151 - sparse_categorical_accuracy: 0.8832\n",
            "Epoch 28: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.3151 - sparse_categorical_accuracy: 0.8832 - val_loss: 4.0881 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 29/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.3383 - sparse_categorical_accuracy: 0.8505\n",
            "Epoch 29: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.3383 - sparse_categorical_accuracy: 0.8505 - val_loss: 4.5260 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 30/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.2990 - sparse_categorical_accuracy: 0.8949\n",
            "Epoch 30: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.2990 - sparse_categorical_accuracy: 0.8949 - val_loss: 5.5611 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 31/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.2470 - sparse_categorical_accuracy: 0.9159\n",
            "Epoch 31: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.2470 - sparse_categorical_accuracy: 0.9159 - val_loss: 5.8054 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 32/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.2384 - sparse_categorical_accuracy: 0.9065\n",
            "Epoch 32: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.2384 - sparse_categorical_accuracy: 0.9065 - val_loss: 6.0839 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 33/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.2275 - sparse_categorical_accuracy: 0.9299\n",
            "Epoch 33: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.2275 - sparse_categorical_accuracy: 0.9299 - val_loss: 6.4146 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 34/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.2775 - sparse_categorical_accuracy: 0.8808\n",
            "Epoch 34: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.2775 - sparse_categorical_accuracy: 0.8808 - val_loss: 6.5629 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 35/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.2082 - sparse_categorical_accuracy: 0.9322\n",
            "Epoch 35: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.2082 - sparse_categorical_accuracy: 0.9322 - val_loss: 6.7506 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 36/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1906 - sparse_categorical_accuracy: 0.9299\n",
            "Epoch 36: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.1906 - sparse_categorical_accuracy: 0.9299 - val_loss: 6.4308 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 37/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1655 - sparse_categorical_accuracy: 0.9463\n",
            "Epoch 37: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.1655 - sparse_categorical_accuracy: 0.9463 - val_loss: 6.7076 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 38/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1710 - sparse_categorical_accuracy: 0.9393\n",
            "Epoch 38: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.1710 - sparse_categorical_accuracy: 0.9393 - val_loss: 6.1116 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 39/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1539 - sparse_categorical_accuracy: 0.9463\n",
            "Epoch 39: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.1539 - sparse_categorical_accuracy: 0.9463 - val_loss: 7.1265 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 40/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1234 - sparse_categorical_accuracy: 0.9603\n",
            "Epoch 40: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.1234 - sparse_categorical_accuracy: 0.9603 - val_loss: 7.2986 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 41/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1448 - sparse_categorical_accuracy: 0.9556\n",
            "Epoch 41: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.1448 - sparse_categorical_accuracy: 0.9556 - val_loss: 6.6860 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 42/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1599 - sparse_categorical_accuracy: 0.9416\n",
            "Epoch 42: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.1599 - sparse_categorical_accuracy: 0.9416 - val_loss: 6.3450 - val_sparse_categorical_accuracy: 0.1402\n",
            "Epoch 43/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1553 - sparse_categorical_accuracy: 0.9322\n",
            "Epoch 43: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.1553 - sparse_categorical_accuracy: 0.9322 - val_loss: 7.3520 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 44/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1166 - sparse_categorical_accuracy: 0.9556\n",
            "Epoch 44: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.1166 - sparse_categorical_accuracy: 0.9556 - val_loss: 6.7809 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 45/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1306 - sparse_categorical_accuracy: 0.9486\n",
            "Epoch 45: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.1306 - sparse_categorical_accuracy: 0.9486 - val_loss: 7.4368 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 46/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1087 - sparse_categorical_accuracy: 0.9579\n",
            "Epoch 46: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.1087 - sparse_categorical_accuracy: 0.9579 - val_loss: 7.5974 - val_sparse_categorical_accuracy: 0.1402\n",
            "Epoch 47/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0837 - sparse_categorical_accuracy: 0.9743\n",
            "Epoch 47: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0837 - sparse_categorical_accuracy: 0.9743 - val_loss: 7.4563 - val_sparse_categorical_accuracy: 0.1402\n",
            "Epoch 48/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0793 - sparse_categorical_accuracy: 0.9673\n",
            "Epoch 48: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0793 - sparse_categorical_accuracy: 0.9673 - val_loss: 7.8164 - val_sparse_categorical_accuracy: 0.1402\n",
            "Epoch 49/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0754 - sparse_categorical_accuracy: 0.9766\n",
            "Epoch 49: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0754 - sparse_categorical_accuracy: 0.9766 - val_loss: 7.5574 - val_sparse_categorical_accuracy: 0.1402\n",
            "Epoch 50/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0890 - sparse_categorical_accuracy: 0.9673\n",
            "Epoch 50: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0890 - sparse_categorical_accuracy: 0.9673 - val_loss: 7.9125 - val_sparse_categorical_accuracy: 0.1402\n",
            "Epoch 51/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0731 - sparse_categorical_accuracy: 0.9743\n",
            "Epoch 51: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0731 - sparse_categorical_accuracy: 0.9743 - val_loss: 6.8224 - val_sparse_categorical_accuracy: 0.1402\n",
            "Epoch 52/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0813 - sparse_categorical_accuracy: 0.9696\n",
            "Epoch 52: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0813 - sparse_categorical_accuracy: 0.9696 - val_loss: 6.7447 - val_sparse_categorical_accuracy: 0.1402\n",
            "Epoch 53/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0704 - sparse_categorical_accuracy: 0.9696\n",
            "Epoch 53: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0704 - sparse_categorical_accuracy: 0.9696 - val_loss: 7.7302 - val_sparse_categorical_accuracy: 0.1495\n",
            "Epoch 54/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0538 - sparse_categorical_accuracy: 0.9883\n",
            "Epoch 54: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0538 - sparse_categorical_accuracy: 0.9883 - val_loss: 6.1698 - val_sparse_categorical_accuracy: 0.1589\n",
            "Epoch 55/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0636 - sparse_categorical_accuracy: 0.9766\n",
            "Epoch 55: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0636 - sparse_categorical_accuracy: 0.9766 - val_loss: 6.2401 - val_sparse_categorical_accuracy: 0.1589\n",
            "Epoch 56/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0565 - sparse_categorical_accuracy: 0.9813\n",
            "Epoch 56: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0565 - sparse_categorical_accuracy: 0.9813 - val_loss: 7.0454 - val_sparse_categorical_accuracy: 0.1589\n",
            "Epoch 57/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0362 - sparse_categorical_accuracy: 0.9883\n",
            "Epoch 57: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0362 - sparse_categorical_accuracy: 0.9883 - val_loss: 7.2711 - val_sparse_categorical_accuracy: 0.1589\n",
            "Epoch 58/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0372 - sparse_categorical_accuracy: 0.9883\n",
            "Epoch 58: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0372 - sparse_categorical_accuracy: 0.9883 - val_loss: 7.6422 - val_sparse_categorical_accuracy: 0.1589\n",
            "Epoch 59/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0352 - sparse_categorical_accuracy: 0.9883\n",
            "Epoch 59: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0352 - sparse_categorical_accuracy: 0.9883 - val_loss: 7.3558 - val_sparse_categorical_accuracy: 0.1589\n",
            "Epoch 60/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0260 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 60: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0260 - sparse_categorical_accuracy: 0.9907 - val_loss: 7.0015 - val_sparse_categorical_accuracy: 0.1682\n",
            "Epoch 61/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0212 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 61: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0212 - sparse_categorical_accuracy: 0.9930 - val_loss: 7.4140 - val_sparse_categorical_accuracy: 0.1589\n",
            "Epoch 62/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0211 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 62: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0211 - sparse_categorical_accuracy: 0.9930 - val_loss: 7.5669 - val_sparse_categorical_accuracy: 0.1589\n",
            "Epoch 63/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0504 - sparse_categorical_accuracy: 0.9790\n",
            "Epoch 63: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0504 - sparse_categorical_accuracy: 0.9790 - val_loss: 7.5085 - val_sparse_categorical_accuracy: 0.1589\n",
            "Epoch 64/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0432 - sparse_categorical_accuracy: 0.9836\n",
            "Epoch 64: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0432 - sparse_categorical_accuracy: 0.9836 - val_loss: 6.8181 - val_sparse_categorical_accuracy: 0.1869\n",
            "Epoch 65/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0403 - sparse_categorical_accuracy: 0.9883\n",
            "Epoch 65: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0403 - sparse_categorical_accuracy: 0.9883 - val_loss: 6.1854 - val_sparse_categorical_accuracy: 0.1776\n",
            "Epoch 66/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0456 - sparse_categorical_accuracy: 0.9860\n",
            "Epoch 66: val_sparse_categorical_accuracy did not improve from 0.21495\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0456 - sparse_categorical_accuracy: 0.9860 - val_loss: 5.9861 - val_sparse_categorical_accuracy: 0.2056\n",
            "Epoch 67/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0217 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 67: val_sparse_categorical_accuracy improved from 0.21495 to 0.22430, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 50ms/step - loss: 0.0217 - sparse_categorical_accuracy: 0.9953 - val_loss: 6.0609 - val_sparse_categorical_accuracy: 0.2243\n",
            "Epoch 68/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0252 - sparse_categorical_accuracy: 0.9883\n",
            "Epoch 68: val_sparse_categorical_accuracy did not improve from 0.22430\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0252 - sparse_categorical_accuracy: 0.9883 - val_loss: 5.9366 - val_sparse_categorical_accuracy: 0.2056\n",
            "Epoch 69/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0244 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 69: val_sparse_categorical_accuracy did not improve from 0.22430\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0244 - sparse_categorical_accuracy: 0.9953 - val_loss: 5.9125 - val_sparse_categorical_accuracy: 0.1869\n",
            "Epoch 70/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0336 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 70: val_sparse_categorical_accuracy did not improve from 0.22430\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0336 - sparse_categorical_accuracy: 0.9953 - val_loss: 6.2580 - val_sparse_categorical_accuracy: 0.1963\n",
            "Epoch 71/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0438 - sparse_categorical_accuracy: 0.9792\n",
            "Epoch 71: val_sparse_categorical_accuracy did not improve from 0.22430\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.0405 - sparse_categorical_accuracy: 0.9813 - val_loss: 5.9629 - val_sparse_categorical_accuracy: 0.2056\n",
            "Epoch 72/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0509 - sparse_categorical_accuracy: 0.9836\n",
            "Epoch 72: val_sparse_categorical_accuracy improved from 0.22430 to 0.31776, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0509 - sparse_categorical_accuracy: 0.9836 - val_loss: 4.5818 - val_sparse_categorical_accuracy: 0.3178\n",
            "Epoch 73/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0563 - sparse_categorical_accuracy: 0.9813\n",
            "Epoch 73: val_sparse_categorical_accuracy improved from 0.31776 to 0.32710, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 61ms/step - loss: 0.0563 - sparse_categorical_accuracy: 0.9813 - val_loss: 4.0197 - val_sparse_categorical_accuracy: 0.3271\n",
            "Epoch 74/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0177 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 74: val_sparse_categorical_accuracy improved from 0.32710 to 0.34579, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0177 - sparse_categorical_accuracy: 0.9953 - val_loss: 3.9816 - val_sparse_categorical_accuracy: 0.3458\n",
            "Epoch 75/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0380 - sparse_categorical_accuracy: 0.9792\n",
            "Epoch 75: val_sparse_categorical_accuracy improved from 0.34579 to 0.40187, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0353 - sparse_categorical_accuracy: 0.9813 - val_loss: 3.8931 - val_sparse_categorical_accuracy: 0.4019\n",
            "Epoch 76/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0687 - sparse_categorical_accuracy: 0.9766\n",
            "Epoch 76: val_sparse_categorical_accuracy improved from 0.40187 to 0.41121, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.0657 - sparse_categorical_accuracy: 0.9790 - val_loss: 4.1546 - val_sparse_categorical_accuracy: 0.4112\n",
            "Epoch 77/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0901 - sparse_categorical_accuracy: 0.9766\n",
            "Epoch 77: val_sparse_categorical_accuracy improved from 0.41121 to 0.43925, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.1123 - sparse_categorical_accuracy: 0.9696 - val_loss: 3.6423 - val_sparse_categorical_accuracy: 0.4393\n",
            "Epoch 78/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0524 - sparse_categorical_accuracy: 0.9844\n",
            "Epoch 78: val_sparse_categorical_accuracy did not improve from 0.43925\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.0496 - sparse_categorical_accuracy: 0.9860 - val_loss: 3.3083 - val_sparse_categorical_accuracy: 0.4112\n",
            "Epoch 79/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0380 - sparse_categorical_accuracy: 0.9896\n",
            "Epoch 79: val_sparse_categorical_accuracy improved from 0.43925 to 0.44860, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 55ms/step - loss: 0.0355 - sparse_categorical_accuracy: 0.9907 - val_loss: 3.1308 - val_sparse_categorical_accuracy: 0.4486\n",
            "Epoch 80/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0303 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 80: val_sparse_categorical_accuracy improved from 0.44860 to 0.46729, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 51ms/step - loss: 0.0303 - sparse_categorical_accuracy: 0.9907 - val_loss: 2.8646 - val_sparse_categorical_accuracy: 0.4673\n",
            "Epoch 81/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0226 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 81: val_sparse_categorical_accuracy improved from 0.46729 to 0.53271, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 52ms/step - loss: 0.0226 - sparse_categorical_accuracy: 0.9953 - val_loss: 2.9347 - val_sparse_categorical_accuracy: 0.5327\n",
            "Epoch 82/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0468 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 82: val_sparse_categorical_accuracy did not improve from 0.53271\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0468 - sparse_categorical_accuracy: 0.9907 - val_loss: 2.5211 - val_sparse_categorical_accuracy: 0.5234\n",
            "Epoch 83/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0312 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 83: val_sparse_categorical_accuracy improved from 0.53271 to 0.57009, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.0312 - sparse_categorical_accuracy: 0.9907 - val_loss: 2.4214 - val_sparse_categorical_accuracy: 0.5701\n",
            "Epoch 84/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0307 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 84: val_sparse_categorical_accuracy did not improve from 0.57009\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0307 - sparse_categorical_accuracy: 0.9907 - val_loss: 2.2418 - val_sparse_categorical_accuracy: 0.5421\n",
            "Epoch 85/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0257 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 85: val_sparse_categorical_accuracy improved from 0.57009 to 0.59813, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 49ms/step - loss: 0.0257 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.8282 - val_sparse_categorical_accuracy: 0.5981\n",
            "Epoch 86/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0182 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 86: val_sparse_categorical_accuracy improved from 0.59813 to 0.61682, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 50ms/step - loss: 0.0182 - sparse_categorical_accuracy: 0.9953 - val_loss: 2.0332 - val_sparse_categorical_accuracy: 0.6168\n",
            "Epoch 87/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0176 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 87: val_sparse_categorical_accuracy did not improve from 0.61682\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0176 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.9397 - val_sparse_categorical_accuracy: 0.5981\n",
            "Epoch 88/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0125 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 88: val_sparse_categorical_accuracy did not improve from 0.61682\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0125 - sparse_categorical_accuracy: 0.9953 - val_loss: 2.0500 - val_sparse_categorical_accuracy: 0.5794\n",
            "Epoch 89/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0209 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 89: val_sparse_categorical_accuracy did not improve from 0.61682\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0209 - sparse_categorical_accuracy: 0.9907 - val_loss: 2.1808 - val_sparse_categorical_accuracy: 0.5888\n",
            "Epoch 90/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0196 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 90: val_sparse_categorical_accuracy improved from 0.61682 to 0.62617, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 51ms/step - loss: 0.0196 - sparse_categorical_accuracy: 0.9930 - val_loss: 2.0270 - val_sparse_categorical_accuracy: 0.6262\n",
            "Epoch 91/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0248 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 91: val_sparse_categorical_accuracy did not improve from 0.62617\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0248 - sparse_categorical_accuracy: 0.9907 - val_loss: 2.0908 - val_sparse_categorical_accuracy: 0.5981\n",
            "Epoch 92/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0238 - sparse_categorical_accuracy: 0.9883\n",
            "Epoch 92: val_sparse_categorical_accuracy improved from 0.62617 to 0.65421, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 49ms/step - loss: 0.0238 - sparse_categorical_accuracy: 0.9883 - val_loss: 1.9803 - val_sparse_categorical_accuracy: 0.6542\n",
            "Epoch 93/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0278 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 93: val_sparse_categorical_accuracy improved from 0.65421 to 0.66355, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 53ms/step - loss: 0.0278 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.6650 - val_sparse_categorical_accuracy: 0.6636\n",
            "Epoch 94/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0098 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 94: val_sparse_categorical_accuracy improved from 0.66355 to 0.69159, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 53ms/step - loss: 0.0098 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.5358 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 95/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0460 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 95: val_sparse_categorical_accuracy did not improve from 0.69159\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0460 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.4487 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 96/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0100 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 96: val_sparse_categorical_accuracy did not improve from 0.69159\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0100 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.8717 - val_sparse_categorical_accuracy: 0.6168\n",
            "Epoch 97/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0345 - sparse_categorical_accuracy: 0.9883\n",
            "Epoch 97: val_sparse_categorical_accuracy improved from 0.69159 to 0.70093, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 54ms/step - loss: 0.0345 - sparse_categorical_accuracy: 0.9883 - val_loss: 1.8681 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 98/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0369 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 98: val_sparse_categorical_accuracy improved from 0.70093 to 0.71028, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 51ms/step - loss: 0.0369 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.6740 - val_sparse_categorical_accuracy: 0.7103\n",
            "Epoch 99/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0241 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 99: val_sparse_categorical_accuracy improved from 0.71028 to 0.71963, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 49ms/step - loss: 0.0241 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.5618 - val_sparse_categorical_accuracy: 0.7196\n",
            "Epoch 100/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0209 - sparse_categorical_accuracy: 0.9883\n",
            "Epoch 100: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0209 - sparse_categorical_accuracy: 0.9883 - val_loss: 1.3207 - val_sparse_categorical_accuracy: 0.7196\n",
            "Epoch 101/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0468 - sparse_categorical_accuracy: 0.9813\n",
            "Epoch 101: val_sparse_categorical_accuracy improved from 0.71963 to 0.73832, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 52ms/step - loss: 0.0468 - sparse_categorical_accuracy: 0.9813 - val_loss: 1.3008 - val_sparse_categorical_accuracy: 0.7383\n",
            "Epoch 102/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0359 - sparse_categorical_accuracy: 0.9836\n",
            "Epoch 102: val_sparse_categorical_accuracy improved from 0.73832 to 0.78505, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 49ms/step - loss: 0.0359 - sparse_categorical_accuracy: 0.9836 - val_loss: 1.2032 - val_sparse_categorical_accuracy: 0.7850\n",
            "Epoch 103/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0512 - sparse_categorical_accuracy: 0.9836\n",
            "Epoch 103: val_sparse_categorical_accuracy did not improve from 0.78505\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0512 - sparse_categorical_accuracy: 0.9836 - val_loss: 1.1490 - val_sparse_categorical_accuracy: 0.7664\n",
            "Epoch 104/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0471 - sparse_categorical_accuracy: 0.9860\n",
            "Epoch 104: val_sparse_categorical_accuracy did not improve from 0.78505\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0471 - sparse_categorical_accuracy: 0.9860 - val_loss: 1.0828 - val_sparse_categorical_accuracy: 0.7757\n",
            "Epoch 105/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0234 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 105: val_sparse_categorical_accuracy did not improve from 0.78505\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0234 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.2434 - val_sparse_categorical_accuracy: 0.7664\n",
            "Epoch 106/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0423 - sparse_categorical_accuracy: 0.9836\n",
            "Epoch 106: val_sparse_categorical_accuracy did not improve from 0.78505\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0423 - sparse_categorical_accuracy: 0.9836 - val_loss: 1.2910 - val_sparse_categorical_accuracy: 0.7196\n",
            "Epoch 107/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0273 - sparse_categorical_accuracy: 0.9883\n",
            "Epoch 107: val_sparse_categorical_accuracy did not improve from 0.78505\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0273 - sparse_categorical_accuracy: 0.9883 - val_loss: 1.2160 - val_sparse_categorical_accuracy: 0.7196\n",
            "Epoch 108/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0087 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 108: val_sparse_categorical_accuracy did not improve from 0.78505\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0087 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.2812 - val_sparse_categorical_accuracy: 0.7383\n",
            "Epoch 109/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0197 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 109: val_sparse_categorical_accuracy did not improve from 0.78505\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0197 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.1147 - val_sparse_categorical_accuracy: 0.7196\n",
            "Epoch 110/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0191 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 110: val_sparse_categorical_accuracy did not improve from 0.78505\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0191 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.0200 - val_sparse_categorical_accuracy: 0.7850\n",
            "Epoch 111/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0124 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 111: val_sparse_categorical_accuracy did not improve from 0.78505\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0124 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.1449 - val_sparse_categorical_accuracy: 0.7664\n",
            "Epoch 112/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0384 - sparse_categorical_accuracy: 0.9836\n",
            "Epoch 112: val_sparse_categorical_accuracy did not improve from 0.78505\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0384 - sparse_categorical_accuracy: 0.9836 - val_loss: 1.1639 - val_sparse_categorical_accuracy: 0.7570\n",
            "Epoch 113/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0471 - sparse_categorical_accuracy: 0.9860\n",
            "Epoch 113: val_sparse_categorical_accuracy did not improve from 0.78505\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0471 - sparse_categorical_accuracy: 0.9860 - val_loss: 1.2695 - val_sparse_categorical_accuracy: 0.7850\n",
            "Epoch 114/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0533 - sparse_categorical_accuracy: 0.9836\n",
            "Epoch 114: val_sparse_categorical_accuracy did not improve from 0.78505\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0533 - sparse_categorical_accuracy: 0.9836 - val_loss: 1.2777 - val_sparse_categorical_accuracy: 0.7570\n",
            "Epoch 115/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0504 - sparse_categorical_accuracy: 0.9860\n",
            "Epoch 115: val_sparse_categorical_accuracy did not improve from 0.78505\n",
            "7/7 [==============================] - 0s 30ms/step - loss: 0.0504 - sparse_categorical_accuracy: 0.9860 - val_loss: 1.0458 - val_sparse_categorical_accuracy: 0.7850\n",
            "Epoch 116/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0478 - sparse_categorical_accuracy: 0.9860\n",
            "Epoch 116: val_sparse_categorical_accuracy did not improve from 0.78505\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0478 - sparse_categorical_accuracy: 0.9860 - val_loss: 1.0342 - val_sparse_categorical_accuracy: 0.7383\n",
            "Epoch 117/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0920 - sparse_categorical_accuracy: 0.9626\n",
            "Epoch 117: val_sparse_categorical_accuracy did not improve from 0.78505\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0920 - sparse_categorical_accuracy: 0.9626 - val_loss: 1.3478 - val_sparse_categorical_accuracy: 0.7290\n",
            "Epoch 118/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0380 - sparse_categorical_accuracy: 0.9883\n",
            "Epoch 118: val_sparse_categorical_accuracy did not improve from 0.78505\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0380 - sparse_categorical_accuracy: 0.9883 - val_loss: 1.6147 - val_sparse_categorical_accuracy: 0.6542\n",
            "Epoch 119/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0610 - sparse_categorical_accuracy: 0.9813\n",
            "Epoch 119: val_sparse_categorical_accuracy did not improve from 0.78505\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.0610 - sparse_categorical_accuracy: 0.9813 - val_loss: 1.0779 - val_sparse_categorical_accuracy: 0.7383\n",
            "Epoch 120/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0125 - sparse_categorical_accuracy: 0.9974\n",
            "Epoch 120: val_sparse_categorical_accuracy did not improve from 0.78505\n",
            "7/7 [==============================] - 0s 34ms/step - loss: 0.0118 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.1581 - val_sparse_categorical_accuracy: 0.7477\n",
            "Epoch 121/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0327 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 121: val_sparse_categorical_accuracy did not improve from 0.78505\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 0.0327 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.0331 - val_sparse_categorical_accuracy: 0.7850\n",
            "Epoch 122/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0457 - sparse_categorical_accuracy: 0.9883\n",
            "Epoch 122: val_sparse_categorical_accuracy did not improve from 0.78505\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 0.0457 - sparse_categorical_accuracy: 0.9883 - val_loss: 1.2816 - val_sparse_categorical_accuracy: 0.7196\n",
            "Epoch 123/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0177 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 123: val_sparse_categorical_accuracy did not improve from 0.78505\n",
            "7/7 [==============================] - 0s 30ms/step - loss: 0.0177 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.1680 - val_sparse_categorical_accuracy: 0.7757\n",
            "Epoch 124/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0184 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 124: val_sparse_categorical_accuracy improved from 0.78505 to 0.80374, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0184 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.0852 - val_sparse_categorical_accuracy: 0.8037\n",
            "Epoch 125/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0146 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 125: val_sparse_categorical_accuracy did not improve from 0.80374\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 0.0146 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.1191 - val_sparse_categorical_accuracy: 0.7850\n",
            "Epoch 126/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0307 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 126: val_sparse_categorical_accuracy did not improve from 0.80374\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 0.0307 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.0928 - val_sparse_categorical_accuracy: 0.7664\n",
            "Epoch 127/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0151 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 127: val_sparse_categorical_accuracy did not improve from 0.80374\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 0.0151 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.0969 - val_sparse_categorical_accuracy: 0.7757\n",
            "Epoch 128/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0210 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 128: val_sparse_categorical_accuracy did not improve from 0.80374\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 0.0210 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.0814 - val_sparse_categorical_accuracy: 0.7944\n",
            "Epoch 129/900\n",
            "5/7 [====================>.........] - ETA: 0s - loss: 0.0078 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 129: val_sparse_categorical_accuracy did not improve from 0.80374\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.0069 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.1340 - val_sparse_categorical_accuracy: 0.7944\n",
            "Epoch 130/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0190 - sparse_categorical_accuracy: 0.9974\n",
            "Epoch 130: val_sparse_categorical_accuracy did not improve from 0.80374\n",
            "7/7 [==============================] - 0s 35ms/step - loss: 0.0171 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.1406 - val_sparse_categorical_accuracy: 0.7850\n",
            "Epoch 131/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0112 - sparse_categorical_accuracy: 0.9948\n",
            "Epoch 131: val_sparse_categorical_accuracy did not improve from 0.80374\n",
            "7/7 [==============================] - 0s 35ms/step - loss: 0.0112 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.1664 - val_sparse_categorical_accuracy: 0.7664\n",
            "Epoch 132/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0041 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 132: val_sparse_categorical_accuracy did not improve from 0.80374\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.0041 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.1733 - val_sparse_categorical_accuracy: 0.7570\n",
            "Epoch 133/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0094 - sparse_categorical_accuracy: 0.9948\n",
            "Epoch 133: val_sparse_categorical_accuracy did not improve from 0.80374\n",
            "7/7 [==============================] - 0s 36ms/step - loss: 0.0086 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.1569 - val_sparse_categorical_accuracy: 0.7664\n",
            "Epoch 134/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0087 - sparse_categorical_accuracy: 0.9974\n",
            "Epoch 134: val_sparse_categorical_accuracy did not improve from 0.80374\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0088 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.1395 - val_sparse_categorical_accuracy: 0.7850\n",
            "Epoch 135/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0049 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 135: val_sparse_categorical_accuracy did not improve from 0.80374\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0049 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.1411 - val_sparse_categorical_accuracy: 0.7944\n",
            "Epoch 136/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0143 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 136: val_sparse_categorical_accuracy did not improve from 0.80374\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0143 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.1568 - val_sparse_categorical_accuracy: 0.7944\n",
            "Epoch 137/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0117 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 137: val_sparse_categorical_accuracy did not improve from 0.80374\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0117 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.2392 - val_sparse_categorical_accuracy: 0.7664\n",
            "Epoch 138/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0043 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 138: val_sparse_categorical_accuracy did not improve from 0.80374\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0043 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.1858 - val_sparse_categorical_accuracy: 0.7757\n",
            "Epoch 139/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0100 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 139: val_sparse_categorical_accuracy did not improve from 0.80374\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0100 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.2073 - val_sparse_categorical_accuracy: 0.7850\n",
            "Epoch 140/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0042 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 140: val_sparse_categorical_accuracy did not improve from 0.80374\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0042 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.3483 - val_sparse_categorical_accuracy: 0.7290\n",
            "Epoch 141/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0203 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 141: val_sparse_categorical_accuracy did not improve from 0.80374\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0203 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.1877 - val_sparse_categorical_accuracy: 0.7757\n",
            "Epoch 142/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0041 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 142: val_sparse_categorical_accuracy did not improve from 0.80374\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0041 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.1676 - val_sparse_categorical_accuracy: 0.7850\n",
            "Epoch 143/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0061 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 143: val_sparse_categorical_accuracy did not improve from 0.80374\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0061 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.1880 - val_sparse_categorical_accuracy: 0.7944\n",
            "Epoch 144/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0049 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 144: val_sparse_categorical_accuracy did not improve from 0.80374\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0049 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.1997 - val_sparse_categorical_accuracy: 0.7850\n",
            "Epoch 145/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0056 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 145: val_sparse_categorical_accuracy did not improve from 0.80374\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0056 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.3239 - val_sparse_categorical_accuracy: 0.7850\n",
            "Epoch 146/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0130 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 146: val_sparse_categorical_accuracy did not improve from 0.80374\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0130 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.2776 - val_sparse_categorical_accuracy: 0.7664\n",
            "Epoch 147/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0150 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 147: val_sparse_categorical_accuracy did not improve from 0.80374\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0150 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.3192 - val_sparse_categorical_accuracy: 0.7383\n",
            "Epoch 148/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0091 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 148: val_sparse_categorical_accuracy did not improve from 0.80374\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0091 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.2567 - val_sparse_categorical_accuracy: 0.7944\n",
            "Epoch 149/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0051 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 149: val_sparse_categorical_accuracy did not improve from 0.80374\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0051 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.1622 - val_sparse_categorical_accuracy: 0.7757\n",
            "Epoch 150/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0031 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 150: val_sparse_categorical_accuracy did not improve from 0.80374\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0031 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.1326 - val_sparse_categorical_accuracy: 0.8037\n",
            "Epoch 151/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0055 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 151: val_sparse_categorical_accuracy improved from 0.80374 to 0.81308, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 51ms/step - loss: 0.0055 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.1144 - val_sparse_categorical_accuracy: 0.8131\n",
            "Epoch 152/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0027 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 152: val_sparse_categorical_accuracy did not improve from 0.81308\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0027 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.1203 - val_sparse_categorical_accuracy: 0.8131\n",
            "Epoch 153/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0080 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 153: val_sparse_categorical_accuracy did not improve from 0.81308\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0080 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.1492 - val_sparse_categorical_accuracy: 0.7944\n",
            "Epoch 154/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0039 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 154: val_sparse_categorical_accuracy did not improve from 0.81308\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0039 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.2403 - val_sparse_categorical_accuracy: 0.7757\n",
            "Epoch 155/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0010 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 155: val_sparse_categorical_accuracy did not improve from 0.81308\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0010 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.1618 - val_sparse_categorical_accuracy: 0.7944\n",
            "Epoch 156/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 7.7428e-04 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 156: val_sparse_categorical_accuracy did not improve from 0.81308\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 7.7428e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.1400 - val_sparse_categorical_accuracy: 0.7850\n",
            "Epoch 157/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0032 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 157: val_sparse_categorical_accuracy did not improve from 0.81308\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0032 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.0879 - val_sparse_categorical_accuracy: 0.7944\n",
            "Epoch 158/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0142 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 158: val_sparse_categorical_accuracy did not improve from 0.81308\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0142 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.1561 - val_sparse_categorical_accuracy: 0.7850\n",
            "Epoch 159/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0020 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 159: val_sparse_categorical_accuracy did not improve from 0.81308\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0020 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.3394 - val_sparse_categorical_accuracy: 0.7477\n",
            "Epoch 160/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0073 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 160: val_sparse_categorical_accuracy did not improve from 0.81308\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0073 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.3898 - val_sparse_categorical_accuracy: 0.7570\n",
            "Epoch 161/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0144 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 161: val_sparse_categorical_accuracy did not improve from 0.81308\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0144 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.3284 - val_sparse_categorical_accuracy: 0.7477\n",
            "Epoch 162/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0269 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 162: val_sparse_categorical_accuracy did not improve from 0.81308\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0269 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.2193 - val_sparse_categorical_accuracy: 0.7944\n",
            "Epoch 163/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0066 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 163: val_sparse_categorical_accuracy did not improve from 0.81308\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0066 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.5759 - val_sparse_categorical_accuracy: 0.7196\n",
            "Epoch 164/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0361 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 164: val_sparse_categorical_accuracy did not improve from 0.81308\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0361 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.2762 - val_sparse_categorical_accuracy: 0.7477\n",
            "Epoch 165/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0083 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 165: val_sparse_categorical_accuracy did not improve from 0.81308\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0083 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.2227 - val_sparse_categorical_accuracy: 0.7850\n",
            "Epoch 166/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0037 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 166: val_sparse_categorical_accuracy did not improve from 0.81308\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0037 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.3246 - val_sparse_categorical_accuracy: 0.7477\n",
            "Epoch 167/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0066 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 167: val_sparse_categorical_accuracy did not improve from 0.81308\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0066 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.2500 - val_sparse_categorical_accuracy: 0.7664\n",
            "Epoch 168/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0166 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 168: val_sparse_categorical_accuracy did not improve from 0.81308\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0166 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.1354 - val_sparse_categorical_accuracy: 0.7944\n",
            "Epoch 169/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0048 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 169: val_sparse_categorical_accuracy did not improve from 0.81308\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0048 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.2065 - val_sparse_categorical_accuracy: 0.7757\n",
            "Epoch 170/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0030 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 170: val_sparse_categorical_accuracy did not improve from 0.81308\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0030 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.2049 - val_sparse_categorical_accuracy: 0.7944\n",
            "Epoch 171/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0015 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 171: val_sparse_categorical_accuracy did not improve from 0.81308\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0015 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.2216 - val_sparse_categorical_accuracy: 0.7850\n",
            "Epoch 172/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0024 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 172: val_sparse_categorical_accuracy did not improve from 0.81308\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0024 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.2061 - val_sparse_categorical_accuracy: 0.7757\n",
            "Epoch 173/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0020 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 173: val_sparse_categorical_accuracy did not improve from 0.81308\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0020 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.1858 - val_sparse_categorical_accuracy: 0.7850\n",
            "Epoch 174/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0093 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 174: val_sparse_categorical_accuracy did not improve from 0.81308\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0093 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.2832 - val_sparse_categorical_accuracy: 0.7850\n",
            "Epoch 175/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0066 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 175: val_sparse_categorical_accuracy did not improve from 0.81308\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0066 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.3444 - val_sparse_categorical_accuracy: 0.7570\n",
            "Epoch 176/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0058 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 176: val_sparse_categorical_accuracy did not improve from 0.81308\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0058 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.3274 - val_sparse_categorical_accuracy: 0.7570\n",
            "Epoch 177/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0049 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 177: val_sparse_categorical_accuracy did not improve from 0.81308\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0049 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.1782 - val_sparse_categorical_accuracy: 0.8131\n",
            "Epoch 178/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0024 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 178: val_sparse_categorical_accuracy did not improve from 0.81308\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0024 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.2210 - val_sparse_categorical_accuracy: 0.7757\n",
            "Epoch 179/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0162 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 179: val_sparse_categorical_accuracy did not improve from 0.81308\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0162 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.2107 - val_sparse_categorical_accuracy: 0.7664\n",
            "Epoch 180/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0070 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 180: val_sparse_categorical_accuracy did not improve from 0.81308\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0070 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.1825 - val_sparse_categorical_accuracy: 0.7850\n",
            "Epoch 181/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0056 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 181: val_sparse_categorical_accuracy did not improve from 0.81308\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0056 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.1829 - val_sparse_categorical_accuracy: 0.7944\n",
            "Epoch 182/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0141 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 182: val_sparse_categorical_accuracy did not improve from 0.81308\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0141 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.0881 - val_sparse_categorical_accuracy: 0.8037\n",
            "Epoch 183/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0117 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 183: val_sparse_categorical_accuracy did not improve from 0.81308\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0117 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.5249 - val_sparse_categorical_accuracy: 0.7290\n",
            "Epoch 184/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0668 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 184: val_sparse_categorical_accuracy did not improve from 0.81308\n",
            "7/7 [==============================] - 0s 30ms/step - loss: 0.0668 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.4163 - val_sparse_categorical_accuracy: 0.7477\n",
            "Epoch 185/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0253 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 185: val_sparse_categorical_accuracy did not improve from 0.81308\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 0.0253 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.4579 - val_sparse_categorical_accuracy: 0.7196\n",
            "Epoch 186/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0350 - sparse_categorical_accuracy: 0.9883\n",
            "Epoch 186: val_sparse_categorical_accuracy did not improve from 0.81308\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 0.0350 - sparse_categorical_accuracy: 0.9883 - val_loss: 1.1490 - val_sparse_categorical_accuracy: 0.7664\n",
            "Epoch 187/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0330 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 187: val_sparse_categorical_accuracy improved from 0.81308 to 0.82243, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0330 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.1267 - val_sparse_categorical_accuracy: 0.8224\n",
            "Epoch 188/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0212 - sparse_categorical_accuracy: 0.9922\n",
            "Epoch 188: val_sparse_categorical_accuracy did not improve from 0.82243\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 0.0203 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.2117 - val_sparse_categorical_accuracy: 0.7944\n",
            "Epoch 189/900\n",
            "5/7 [====================>.........] - ETA: 0s - loss: 0.0070 - sparse_categorical_accuracy: 0.9969\n",
            "Epoch 189: val_sparse_categorical_accuracy did not improve from 0.82243\n",
            "7/7 [==============================] - 0s 34ms/step - loss: 0.0057 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.3147 - val_sparse_categorical_accuracy: 0.7570\n",
            "Epoch 190/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0256 - sparse_categorical_accuracy: 0.9896\n",
            "Epoch 190: val_sparse_categorical_accuracy did not improve from 0.82243\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.0233 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.2871 - val_sparse_categorical_accuracy: 0.7850\n",
            "Epoch 191/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0161 - sparse_categorical_accuracy: 0.9974\n",
            "Epoch 191: val_sparse_categorical_accuracy did not improve from 0.82243\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.0145 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.2621 - val_sparse_categorical_accuracy: 0.7477\n",
            "Epoch 192/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0154 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 192: val_sparse_categorical_accuracy did not improve from 0.82243\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.0154 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.2063 - val_sparse_categorical_accuracy: 0.7570\n",
            "Epoch 193/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0331 - sparse_categorical_accuracy: 0.9883\n",
            "Epoch 193: val_sparse_categorical_accuracy did not improve from 0.82243\n",
            "7/7 [==============================] - 0s 36ms/step - loss: 0.0331 - sparse_categorical_accuracy: 0.9883 - val_loss: 1.4092 - val_sparse_categorical_accuracy: 0.8037\n",
            "Epoch 194/900\n",
            "5/7 [====================>.........] - ETA: 0s - loss: 0.0220 - sparse_categorical_accuracy: 0.9906\n",
            "Epoch 194: val_sparse_categorical_accuracy did not improve from 0.82243\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 0.0244 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.2232 - val_sparse_categorical_accuracy: 0.7944\n",
            "Epoch 195/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0196 - sparse_categorical_accuracy: 0.9896\n",
            "Epoch 195: val_sparse_categorical_accuracy did not improve from 0.82243\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.0178 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.3103 - val_sparse_categorical_accuracy: 0.7290\n",
            "Epoch 196/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0329 - sparse_categorical_accuracy: 0.9870\n",
            "Epoch 196: val_sparse_categorical_accuracy did not improve from 0.82243\n",
            "7/7 [==============================] - 0s 35ms/step - loss: 0.0300 - sparse_categorical_accuracy: 0.9883 - val_loss: 1.4338 - val_sparse_categorical_accuracy: 0.7290\n",
            "Epoch 197/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0192 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 197: val_sparse_categorical_accuracy did not improve from 0.82243\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.0192 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.2076 - val_sparse_categorical_accuracy: 0.8037\n",
            "Epoch 198/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0254 - sparse_categorical_accuracy: 0.9948\n",
            "Epoch 198: val_sparse_categorical_accuracy did not improve from 0.82243\n",
            "7/7 [==============================] - 0s 34ms/step - loss: 0.0230 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.1793 - val_sparse_categorical_accuracy: 0.7757\n",
            "Epoch 199/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0041 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 199: val_sparse_categorical_accuracy did not improve from 0.82243\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.0041 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.1824 - val_sparse_categorical_accuracy: 0.7850\n",
            "Epoch 200/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0142 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 200: val_sparse_categorical_accuracy did not improve from 0.82243\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0142 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.1736 - val_sparse_categorical_accuracy: 0.7664\n",
            "Epoch 201/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0245 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 201: val_sparse_categorical_accuracy did not improve from 0.82243\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0245 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.2331 - val_sparse_categorical_accuracy: 0.7664\n",
            "Epoch 202/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0182 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 202: val_sparse_categorical_accuracy did not improve from 0.82243\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0182 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.1273 - val_sparse_categorical_accuracy: 0.7944\n",
            "Epoch 203/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0041 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 203: val_sparse_categorical_accuracy did not improve from 0.82243\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0041 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.0930 - val_sparse_categorical_accuracy: 0.8037\n",
            "Epoch 204/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0041 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 204: val_sparse_categorical_accuracy did not improve from 0.82243\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0041 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.0711 - val_sparse_categorical_accuracy: 0.8224\n",
            "Epoch 205/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0049 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 205: val_sparse_categorical_accuracy did not improve from 0.82243\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0049 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.0726 - val_sparse_categorical_accuracy: 0.8224\n",
            "Epoch 206/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0022 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 206: val_sparse_categorical_accuracy did not improve from 0.82243\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0022 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.1199 - val_sparse_categorical_accuracy: 0.8224\n",
            "Epoch 207/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0160 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 207: val_sparse_categorical_accuracy did not improve from 0.82243\n",
            "7/7 [==============================] - 0s 30ms/step - loss: 0.0160 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.1362 - val_sparse_categorical_accuracy: 0.8037\n",
            "Epoch 208/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0147 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 208: val_sparse_categorical_accuracy did not improve from 0.82243\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0147 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.0986 - val_sparse_categorical_accuracy: 0.8037\n",
            "Epoch 209/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0140 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 209: val_sparse_categorical_accuracy did not improve from 0.82243\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0140 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.1518 - val_sparse_categorical_accuracy: 0.8037\n",
            "Epoch 210/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0078 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 210: val_sparse_categorical_accuracy did not improve from 0.82243\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0078 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.1697 - val_sparse_categorical_accuracy: 0.7664\n",
            "Epoch 211/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0068 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 211: val_sparse_categorical_accuracy did not improve from 0.82243\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0068 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.1644 - val_sparse_categorical_accuracy: 0.7944\n",
            "Epoch 212/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0046 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 212: val_sparse_categorical_accuracy did not improve from 0.82243\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0046 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.0767 - val_sparse_categorical_accuracy: 0.7664\n",
            "Epoch 213/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0114 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 213: val_sparse_categorical_accuracy did not improve from 0.82243\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0114 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.0490 - val_sparse_categorical_accuracy: 0.7757\n",
            "Epoch 214/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0069 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 214: val_sparse_categorical_accuracy did not improve from 0.82243\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0069 - sparse_categorical_accuracy: 0.9977 - val_loss: 0.9571 - val_sparse_categorical_accuracy: 0.8037\n",
            "Epoch 215/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0019 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 215: val_sparse_categorical_accuracy did not improve from 0.82243\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0019 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.9539 - val_sparse_categorical_accuracy: 0.8037\n",
            "Epoch 216/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0069 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 216: val_sparse_categorical_accuracy did not improve from 0.82243\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0069 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.0627 - val_sparse_categorical_accuracy: 0.7850\n",
            "Epoch 217/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0027 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 217: val_sparse_categorical_accuracy did not improve from 0.82243\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0027 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.2724 - val_sparse_categorical_accuracy: 0.7477\n",
            "Epoch 218/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0081 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 218: val_sparse_categorical_accuracy did not improve from 0.82243\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0081 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.3059 - val_sparse_categorical_accuracy: 0.7757\n",
            "Epoch 219/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0118 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 219: val_sparse_categorical_accuracy did not improve from 0.82243\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0118 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.3032 - val_sparse_categorical_accuracy: 0.7570\n",
            "Epoch 220/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0155 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 220: val_sparse_categorical_accuracy did not improve from 0.82243\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0155 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.1525 - val_sparse_categorical_accuracy: 0.7944\n",
            "Epoch 221/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0127 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 221: val_sparse_categorical_accuracy did not improve from 0.82243\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0127 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.0698 - val_sparse_categorical_accuracy: 0.8037\n",
            "Epoch 222/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0069 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 222: val_sparse_categorical_accuracy did not improve from 0.82243\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0069 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.1150 - val_sparse_categorical_accuracy: 0.7944\n",
            "Epoch 223/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0141 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 223: val_sparse_categorical_accuracy did not improve from 0.82243\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0141 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.1695 - val_sparse_categorical_accuracy: 0.8131\n",
            "Epoch 224/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0156 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 224: val_sparse_categorical_accuracy did not improve from 0.82243\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0156 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.4261 - val_sparse_categorical_accuracy: 0.7477\n",
            "Epoch 225/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0097 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 225: val_sparse_categorical_accuracy did not improve from 0.82243\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0097 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.1601 - val_sparse_categorical_accuracy: 0.8037\n",
            "Epoch 226/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0111 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 226: val_sparse_categorical_accuracy improved from 0.82243 to 0.83178, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 51ms/step - loss: 0.0111 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.0685 - val_sparse_categorical_accuracy: 0.8318\n",
            "Epoch 227/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0025 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 227: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0025 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.1578 - val_sparse_categorical_accuracy: 0.7944\n",
            "Epoch 228/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0016 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 228: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0015 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.2839 - val_sparse_categorical_accuracy: 0.7664\n",
            "Epoch 229/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0088 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 229: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0088 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.0934 - val_sparse_categorical_accuracy: 0.8037\n",
            "Epoch 230/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0120 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 230: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0120 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.0813 - val_sparse_categorical_accuracy: 0.8037\n",
            "Epoch 231/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0191 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 231: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0191 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.0562 - val_sparse_categorical_accuracy: 0.8131\n",
            "Epoch 232/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0025 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 232: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0025 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.0696 - val_sparse_categorical_accuracy: 0.7944\n",
            "Epoch 233/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0028 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 233: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0028 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.1477 - val_sparse_categorical_accuracy: 0.7944\n",
            "Epoch 234/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0080 - sparse_categorical_accuracy: 0.9953    \n",
            "Epoch 234: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0080 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.0967 - val_sparse_categorical_accuracy: 0.7850\n",
            "Epoch 235/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0061 - sparse_categorical_accuracy: 0.9977    \n",
            "Epoch 235: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0061 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.1051 - val_sparse_categorical_accuracy: 0.7944\n",
            "Epoch 236/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0017 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 236: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0017 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.1593 - val_sparse_categorical_accuracy: 0.7944\n",
            "Epoch 237/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0048 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 237: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0048 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.1198 - val_sparse_categorical_accuracy: 0.7850\n",
            "Epoch 238/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0014 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 238: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0014 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.1496 - val_sparse_categorical_accuracy: 0.7850\n",
            "Epoch 239/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0054 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 239: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0054 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.1948 - val_sparse_categorical_accuracy: 0.7664\n",
            "Epoch 240/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0182 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 240: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0182 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.2150 - val_sparse_categorical_accuracy: 0.7383\n",
            "Epoch 241/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0059 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 241: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0059 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.1744 - val_sparse_categorical_accuracy: 0.7944\n",
            "Epoch 242/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0046 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 242: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0046 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.1594 - val_sparse_categorical_accuracy: 0.7664\n",
            "Epoch 243/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0013 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 243: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0013 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.1330 - val_sparse_categorical_accuracy: 0.7757\n",
            "Epoch 244/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0029 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 244: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0029 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.1456 - val_sparse_categorical_accuracy: 0.7850\n",
            "Epoch 245/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0017 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 245: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0017 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.3176 - val_sparse_categorical_accuracy: 0.7757\n",
            "Epoch 246/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0045 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 246: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0045 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.1998 - val_sparse_categorical_accuracy: 0.7757\n",
            "Epoch 247/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0038 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 247: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0038 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.1838 - val_sparse_categorical_accuracy: 0.7944\n",
            "Epoch 248/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0020 - sparse_categorical_accuracy: 1.0000    \n",
            "Epoch 248: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0020 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.2096 - val_sparse_categorical_accuracy: 0.8037\n",
            "Epoch 249/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0015 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 249: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 30ms/step - loss: 0.0015 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.1971 - val_sparse_categorical_accuracy: 0.8224\n",
            "Epoch 250/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0015 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 250: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.0040 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.1647 - val_sparse_categorical_accuracy: 0.7944\n",
            "Epoch 251/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0061 - sparse_categorical_accuracy: 0.9974\n",
            "Epoch 251: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.0057 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.3259 - val_sparse_categorical_accuracy: 0.7664\n",
            "Epoch 252/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0146 - sparse_categorical_accuracy: 0.9896\n",
            "Epoch 252: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 0.0132 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.4136 - val_sparse_categorical_accuracy: 0.7850\n",
            "Epoch 253/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0065 - sparse_categorical_accuracy: 1.0000    \n",
            "Epoch 253: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 35ms/step - loss: 0.0065 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.6254 - val_sparse_categorical_accuracy: 0.7570\n",
            "Epoch 254/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0084 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 254: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 0.0084 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.4323 - val_sparse_categorical_accuracy: 0.7757\n",
            "Epoch 255/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0158 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 255: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 0.0158 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.3373 - val_sparse_categorical_accuracy: 0.7850\n",
            "Epoch 256/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0134 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 256: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 0.0134 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.3355 - val_sparse_categorical_accuracy: 0.7570\n",
            "Epoch 257/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0295 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 257: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 0.0295 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.1466 - val_sparse_categorical_accuracy: 0.7664\n",
            "Epoch 258/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0094 - sparse_categorical_accuracy: 0.9948\n",
            "Epoch 258: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 35ms/step - loss: 0.0088 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.2607 - val_sparse_categorical_accuracy: 0.7850\n",
            "Epoch 259/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0133 - sparse_categorical_accuracy: 0.9922\n",
            "Epoch 259: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 0.0120 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.2506 - val_sparse_categorical_accuracy: 0.8131\n",
            "Epoch 260/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0235 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 260: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 0.0235 - sparse_categorical_accuracy: 0.9930 - val_loss: 0.9363 - val_sparse_categorical_accuracy: 0.8224\n",
            "Epoch 261/900\n",
            "5/7 [====================>.........] - ETA: 0s - loss: 0.0016 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 261: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 34ms/step - loss: 0.0060 - sparse_categorical_accuracy: 0.9953 - val_loss: 0.9487 - val_sparse_categorical_accuracy: 0.8224\n",
            "Epoch 262/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0029 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 262: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 34ms/step - loss: 0.0029 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.9891 - val_sparse_categorical_accuracy: 0.8131\n",
            "Epoch 263/900\n",
            "5/7 [====================>.........] - ETA: 0s - loss: 0.0108 - sparse_categorical_accuracy: 0.9969\n",
            "Epoch 263: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 35ms/step - loss: 0.0104 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.0437 - val_sparse_categorical_accuracy: 0.7944\n",
            "Epoch 264/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0018 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 264: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 35ms/step - loss: 0.0017 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.0164 - val_sparse_categorical_accuracy: 0.8037\n",
            "Epoch 265/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0032 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 265: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.0030 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.0799 - val_sparse_categorical_accuracy: 0.8037\n",
            "Epoch 266/900\n",
            "5/7 [====================>.........] - ETA: 0s - loss: 0.0039 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 266: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 36ms/step - loss: 0.0033 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.1114 - val_sparse_categorical_accuracy: 0.7944\n",
            "Epoch 267/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0018 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 267: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0020 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.1173 - val_sparse_categorical_accuracy: 0.7944\n",
            "Epoch 268/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0146 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 268: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0146 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.0049 - val_sparse_categorical_accuracy: 0.7944\n",
            "Epoch 269/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0056 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 269: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0056 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.1566 - val_sparse_categorical_accuracy: 0.7570\n",
            "Epoch 270/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0160 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 270: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0160 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.2363 - val_sparse_categorical_accuracy: 0.7664\n",
            "Epoch 271/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0039 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 271: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0039 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.3187 - val_sparse_categorical_accuracy: 0.7757\n",
            "Epoch 272/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0044 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 272: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0044 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.3119 - val_sparse_categorical_accuracy: 0.8037\n",
            "Epoch 273/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0029 - sparse_categorical_accuracy: 0.9977    \n",
            "Epoch 273: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0029 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.2096 - val_sparse_categorical_accuracy: 0.8037\n",
            "Epoch 274/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0030 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 274: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0030 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.1741 - val_sparse_categorical_accuracy: 0.8037\n",
            "Epoch 275/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0023 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 275: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0023 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.1999 - val_sparse_categorical_accuracy: 0.7664\n",
            "Epoch 276/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0013 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 276: val_sparse_categorical_accuracy did not improve from 0.83178\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 0.0013 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.2181 - val_sparse_categorical_accuracy: 0.7757\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 242,
      "id": "1b676d1c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "1b676d1c",
        "outputId": "ef0dc604-19a7-4f63-dd0d-da67ca9fc0b4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Plot model accuracy over ephocs\n",
        "plt.plot(history.history['sparse_categorical_accuracy'])\n",
        "plt.plot(history.history['val_sparse_categorical_accuracy'])\n",
        "plt.ylim(0, 1)\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "id": "df8c9ade",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df8c9ade",
        "outputId": "194064ba-3dff-4162-f118-56a962d9478e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7a5244283640>"
            ]
          },
          "metadata": {},
          "execution_count": 243
        }
      ],
      "source": [
        "model.load_weights(checkpoint_path) #to load model with highest accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "id": "2f08c66c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f08c66c",
        "outputId": "4d250d3b-7019-4280-be70-23b072a8483d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 8ms/step - loss: 1.0685 - sparse_categorical_accuracy: 0.8318\n",
            "Pre-training accuracy: 83.1776%\n"
          ]
        }
      ],
      "source": [
        "# Calculate pre-training accuracy\n",
        "score = model.evaluate(X_test_scalled, y_test, verbose=1)\n",
        "accuracy = 100*score[1]\n",
        "\n",
        "print(\"Pre-training accuracy: %.4f%%\" % accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "id": "1bdfcfc8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bdfcfc8",
        "outputId": "9738391e-dde1-40dd-e43c-1d3343398108"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy:  1.0\n",
            "Testing Accuracy:  0.8317757248878479\n"
          ]
        }
      ],
      "source": [
        "# Evaluating the model on the training and testing set\n",
        "score = model.evaluate(X_train_scalled, y_train, verbose=0)\n",
        "print(\"Training Accuracy: \", score[1])\n",
        "\n",
        "score = model.evaluate(X_test_scalled, y_test, verbose=0)\n",
        "print(\"Testing Accuracy: \", score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "id": "eaac1550",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaac1550",
        "outputId": "3792b679-1425-4882-f179-9f6059c86fd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 1s 7ms/step\n"
          ]
        }
      ],
      "source": [
        "#Get predictions from model\n",
        "y_test_predictions = model.predict(X_test_scalled) # it will give the prediction data of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 247,
      "id": "8a9df249",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a9df249",
        "outputId": "fada169d-a4f2-4b43-fced-0c2c8d4b3d5a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(107, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 247
        }
      ],
      "source": [
        "y_test_predictions.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "id": "dda064ae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dda064ae",
        "outputId": "68feb7de-e6e2-424d-a5a4-5e54dab4a527"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.86565407e-05, 9.96329129e-01, 1.06762282e-05, 1.75824971e-03,\n",
              "        5.21549606e-04, 1.26109750e-03, 8.07419856e-05],\n",
              "       [3.10336134e-07, 9.99939203e-01, 6.13924328e-07, 7.74587988e-06,\n",
              "        9.75045714e-06, 4.03075937e-05, 1.99389183e-06],\n",
              "       [6.36039158e-06, 3.57252247e-07, 9.99950528e-01, 2.23399361e-08,\n",
              "        4.00022291e-05, 8.69009739e-07, 1.91575555e-06],\n",
              "       [2.90705619e-04, 1.04969022e-05, 9.99485612e-01, 2.36276628e-06,\n",
              "        6.31768708e-05, 1.85161662e-05, 1.29035281e-04],\n",
              "       [2.13234962e-05, 4.91758783e-06, 2.57716272e-02, 4.44518520e-07,\n",
              "        9.74133432e-01, 7.96087534e-06, 6.02548498e-05],\n",
              "       [5.60223671e-05, 1.85452478e-07, 9.99899030e-01, 9.63701119e-09,\n",
              "        4.38065872e-05, 2.78033639e-07, 5.58295540e-07],\n",
              "       [5.68116661e-07, 9.99392152e-01, 1.71248746e-06, 1.26123896e-05,\n",
              "        5.72402147e-04, 1.57143622e-05, 4.82004680e-06],\n",
              "       [1.78201753e-05, 3.70225649e-07, 9.99905944e-01, 1.94134451e-08,\n",
              "        7.43954661e-05, 8.04575507e-07, 6.09436313e-07],\n",
              "       [2.70747478e-06, 2.31863794e-07, 1.62895833e-06, 6.54161525e-08,\n",
              "        9.99980092e-01, 3.52106554e-07, 1.48481868e-05],\n",
              "       [1.82516510e-06, 9.93084431e-01, 4.05303763e-06, 1.82741176e-04,\n",
              "        1.61330918e-05, 6.69140788e-03, 1.94768199e-05],\n",
              "       [4.53980920e-06, 5.38168169e-05, 9.68369250e-07, 9.99819815e-01,\n",
              "        3.25933979e-05, 6.05201922e-06, 8.22374350e-05],\n",
              "       [2.46351169e-06, 4.25704580e-04, 7.77274693e-07, 9.99443710e-01,\n",
              "        1.73307471e-05, 5.11904000e-05, 5.87041359e-05],\n",
              "       [1.47221796e-03, 9.60596367e-07, 9.98507082e-01, 1.10058103e-07,\n",
              "        7.26089547e-06, 4.59019338e-06, 7.79496168e-06],\n",
              "       [3.57736317e-05, 4.85623957e-07, 9.99945641e-01, 2.53093209e-08,\n",
              "        1.60611489e-05, 1.52790938e-06, 5.40495535e-07],\n",
              "       [1.46371676e-04, 3.62922788e-06, 6.55379612e-04, 6.56572865e-07,\n",
              "        9.99186456e-01, 1.57790350e-06, 5.93556615e-06],\n",
              "       [5.03140086e-07, 4.57354508e-06, 1.23990546e-07, 9.99964118e-01,\n",
              "        5.29545196e-06, 8.17407113e-07, 2.45428037e-05],\n",
              "       [1.30343224e-05, 3.46504606e-07, 9.99970794e-01, 2.28456738e-08,\n",
              "        1.40195471e-05, 1.18659102e-06, 6.99759028e-07],\n",
              "       [3.26746449e-05, 5.28272437e-07, 9.99898553e-01, 4.20040926e-08,\n",
              "        6.49025096e-05, 2.36356414e-06, 9.60446414e-07],\n",
              "       [9.93614137e-01, 3.45906551e-06, 1.97411724e-03, 4.31692251e-06,\n",
              "        4.33118455e-03, 4.70558552e-06, 6.81194069e-05],\n",
              "       [4.77930662e-05, 3.96393403e-07, 9.99924064e-01, 5.45321477e-08,\n",
              "        2.50187531e-05, 1.29778800e-06, 1.45607862e-06],\n",
              "       [9.99196947e-01, 1.03899822e-06, 6.47721929e-04, 3.91113502e-07,\n",
              "        1.46981984e-04, 2.00820841e-06, 4.94383357e-06],\n",
              "       [9.98119891e-01, 8.07254310e-07, 1.82162109e-03, 1.14243187e-06,\n",
              "        3.91073991e-05, 2.57711349e-06, 1.48464542e-05],\n",
              "       [9.83299196e-01, 7.86657210e-07, 1.65319908e-02, 8.76200488e-07,\n",
              "        1.13487979e-04, 2.25001463e-06, 5.14545318e-05],\n",
              "       [9.84319513e-07, 1.03023136e-02, 3.25600672e-06, 1.31204240e-06,\n",
              "        1.43180910e-06, 9.89689708e-01, 1.05743254e-06],\n",
              "       [3.89608741e-02, 4.85154778e-05, 1.39017793e-04, 1.36077542e-05,\n",
              "        2.66361117e-01, 1.95506273e-05, 6.94457293e-01],\n",
              "       [7.30424217e-05, 7.46338701e-05, 2.95792051e-05, 2.04415483e-05,\n",
              "        9.98595417e-01, 1.19995663e-03, 6.89216040e-06],\n",
              "       [2.60879551e-06, 7.82769973e-07, 9.99942541e-01, 3.28485648e-08,\n",
              "        4.35271431e-05, 2.37814356e-06, 8.16599004e-06],\n",
              "       [8.16766381e-01, 6.92475805e-06, 1.82562247e-01, 4.26745692e-06,\n",
              "        2.11187566e-04, 1.47329029e-05, 4.34359623e-04],\n",
              "       [3.85230174e-04, 2.60075012e-06, 1.17050018e-03, 1.05961738e-06,\n",
              "        9.98429596e-01, 1.85290980e-06, 9.05158504e-06],\n",
              "       [1.17006046e-07, 8.27037657e-05, 2.22633844e-07, 1.21923492e-07,\n",
              "        1.66378911e-07, 9.99916315e-01, 3.59616394e-07],\n",
              "       [3.30078194e-07, 1.57204486e-05, 2.27176599e-07, 9.99924302e-01,\n",
              "        1.51925415e-05, 2.12212817e-05, 2.29228644e-05],\n",
              "       [1.03454704e-06, 2.69344338e-04, 1.43680165e-06, 1.46486273e-05,\n",
              "        1.96519591e-06, 9.99695539e-01, 1.60691106e-05],\n",
              "       [1.01535477e-06, 1.45416980e-05, 3.34494587e-07, 1.12741682e-05,\n",
              "        9.99968886e-01, 3.19678378e-07, 3.60766649e-06],\n",
              "       [1.23399950e-05, 9.97158170e-01, 9.19654485e-06, 1.66144676e-03,\n",
              "        9.95972892e-04, 1.20201214e-04, 4.26487968e-05],\n",
              "       [9.99717772e-01, 1.27382933e-07, 2.54371873e-04, 1.33467410e-07,\n",
              "        9.87046405e-06, 4.44737367e-07, 1.72365708e-05],\n",
              "       [4.20170318e-06, 2.19010253e-04, 9.58036594e-07, 9.99669671e-01,\n",
              "        3.72093564e-05, 8.33003469e-06, 6.05946952e-05],\n",
              "       [1.39818439e-05, 3.58565188e-07, 9.99975801e-01, 3.15229478e-08,\n",
              "        6.70077407e-06, 1.39897998e-06, 1.83685631e-06],\n",
              "       [9.98529792e-01, 2.37187635e-07, 1.40401651e-03, 4.04797902e-07,\n",
              "        1.25295219e-05, 8.26101143e-07, 5.22097835e-05],\n",
              "       [4.65946760e-05, 7.59391665e-01, 6.71471571e-05, 2.28272364e-01,\n",
              "        6.84572209e-04, 1.13035031e-02, 2.34078427e-04],\n",
              "       [7.89591253e-01, 4.68043299e-06, 2.10205644e-01, 1.79254550e-06,\n",
              "        1.01186233e-04, 1.62482338e-05, 7.91556740e-05],\n",
              "       [7.25037262e-06, 9.85836864e-01, 5.99544410e-06, 1.02746335e-03,\n",
              "        1.02276346e-04, 1.28876902e-02, 1.32426750e-04],\n",
              "       [6.50200400e-06, 8.07590550e-05, 1.86323030e-06, 9.99755204e-01,\n",
              "        4.78973707e-05, 1.04276469e-05, 9.73517890e-05],\n",
              "       [2.06692380e-06, 8.09472913e-05, 1.33035348e-06, 9.99786079e-01,\n",
              "        3.67951216e-05, 5.12959596e-05, 4.14292044e-05],\n",
              "       [1.35796098e-02, 1.29218155e-04, 8.80650532e-06, 1.26961371e-04,\n",
              "        6.99771285e-01, 3.13618375e-06, 2.86380947e-01],\n",
              "       [9.99613345e-01, 1.20410249e-07, 3.60169215e-04, 1.35152035e-07,\n",
              "        1.23560530e-05, 3.49667886e-07, 1.35542114e-05],\n",
              "       [2.13644739e-06, 9.27404642e-01, 2.83894860e-06, 1.01298992e-05,\n",
              "        1.98949074e-05, 7.25562051e-02, 4.14930901e-06],\n",
              "       [6.00613077e-07, 9.99894619e-01, 1.02390982e-06, 9.01011936e-06,\n",
              "        4.19377866e-05, 5.12778061e-05, 1.49195546e-06],\n",
              "       [9.98589814e-01, 4.63922078e-07, 1.10163330e-03, 1.56375023e-07,\n",
              "        2.67610856e-04, 6.54487110e-07, 3.95552415e-05],\n",
              "       [9.86499906e-01, 6.75805177e-06, 2.11070819e-05, 7.89126352e-06,\n",
              "        5.04869211e-04, 5.53065684e-06, 1.29540265e-02],\n",
              "       [2.52627680e-04, 1.29187436e-06, 9.99715269e-01, 1.28781693e-07,\n",
              "        4.56232783e-06, 1.96449746e-05, 6.40016879e-06],\n",
              "       [7.76641011e-01, 8.10449274e-06, 2.22840324e-01, 5.01554405e-06,\n",
              "        3.48398084e-04, 1.13148471e-05, 1.45768005e-04],\n",
              "       [2.20960624e-06, 9.97100532e-01, 2.21826076e-06, 6.08083201e-06,\n",
              "        1.61305448e-04, 2.71458924e-03, 1.30010631e-05],\n",
              "       [3.27445127e-06, 4.07683656e-05, 6.75368085e-07, 9.99864459e-01,\n",
              "        2.29674224e-05, 5.84286499e-06, 6.18604172e-05],\n",
              "       [5.55626741e-07, 3.48205549e-06, 2.20810790e-07, 4.24848139e-07,\n",
              "        9.99992847e-01, 2.04622680e-07, 2.23482584e-06],\n",
              "       [5.25482983e-06, 2.17556689e-07, 9.70340693e-07, 1.93453457e-06,\n",
              "        9.56210806e-06, 5.33006073e-07, 9.99981523e-01],\n",
              "       [2.53438993e-05, 9.69929516e-01, 1.61994431e-05, 2.91990127e-05,\n",
              "        2.75582429e-02, 2.36870814e-03, 7.27743027e-05],\n",
              "       [2.67818214e-05, 8.32661331e-01, 1.90636310e-05, 1.32015412e-05,\n",
              "        1.66783288e-01, 4.50185209e-04, 4.61832278e-05],\n",
              "       [3.54957223e-07, 5.80493179e-06, 2.76038946e-07, 9.99860883e-01,\n",
              "        2.73927108e-05, 4.30362161e-05, 6.22875450e-05],\n",
              "       [1.41148547e-07, 1.30638671e-06, 1.18154574e-07, 6.84703281e-08,\n",
              "        9.99997020e-01, 3.87631218e-07, 9.38409869e-07],\n",
              "       [5.43309056e-07, 7.07530009e-04, 3.01747014e-06, 1.51124786e-05,\n",
              "        4.96895263e-05, 9.99223232e-01, 8.70889721e-07],\n",
              "       [3.33938859e-02, 5.44704683e-03, 8.30638819e-05, 4.57259625e-01,\n",
              "        5.63315768e-03, 2.47204909e-04, 4.97936040e-01],\n",
              "       [3.45101466e-06, 6.54395917e-05, 3.72355544e-06, 9.88346875e-01,\n",
              "        1.07412161e-02, 1.26063445e-04, 7.13093672e-04],\n",
              "       [7.30332602e-07, 9.99710739e-01, 1.50583708e-06, 1.91757522e-06,\n",
              "        6.65007756e-05, 2.17687586e-04, 8.42014913e-07],\n",
              "       [4.67575039e-04, 1.67155224e-06, 9.98919010e-01, 2.43125356e-07,\n",
              "        6.04534522e-04, 5.08394169e-06, 1.89710681e-06],\n",
              "       [7.12767780e-01, 1.99547125e-04, 2.41668493e-01, 1.10586865e-04,\n",
              "        4.46753055e-02, 3.13917000e-04, 2.64440430e-04],\n",
              "       [1.88043371e-01, 6.10229745e-06, 8.11743319e-01, 3.28246574e-06,\n",
              "        1.22300538e-04, 1.42142844e-05, 6.74996263e-05],\n",
              "       [1.18013064e-04, 1.47358548e-06, 1.57884358e-07, 1.46702610e-06,\n",
              "        6.88494110e-05, 2.00435224e-07, 9.99809921e-01],\n",
              "       [9.99687672e-01, 2.20908078e-07, 2.58000509e-04, 8.08607012e-07,\n",
              "        2.56650419e-05, 7.08865002e-07, 2.69436896e-05],\n",
              "       [1.86763631e-04, 9.94946778e-01, 5.86867645e-06, 6.79791037e-06,\n",
              "        6.18330203e-04, 3.13574984e-03, 1.09969242e-03],\n",
              "       [1.17833197e-05, 2.10065791e-05, 3.18195489e-05, 3.64919879e-06,\n",
              "        2.99327367e-04, 1.56811552e-06, 9.99630809e-01],\n",
              "       [1.20964269e-05, 1.59356947e-04, 1.60824769e-04, 2.61611808e-06,\n",
              "        9.99442160e-01, 2.15296386e-04, 7.57666976e-06],\n",
              "       [5.27194345e-07, 8.12914277e-06, 2.78818407e-05, 5.79537812e-07,\n",
              "        9.99917507e-01, 4.54628389e-06, 4.08700689e-05],\n",
              "       [2.10842172e-05, 5.86936716e-03, 4.37506196e-06, 1.76783715e-07,\n",
              "        5.20127014e-06, 9.94096279e-01, 3.43858369e-06],\n",
              "       [1.26928033e-04, 2.09127893e-06, 9.99835134e-01, 2.76529278e-07,\n",
              "        2.12101640e-05, 8.98358758e-06, 5.45434250e-06],\n",
              "       [2.56594612e-05, 1.12731516e-06, 9.99895453e-01, 2.15838071e-07,\n",
              "        3.34064862e-05, 1.95874941e-06, 4.21612276e-05],\n",
              "       [7.27578640e-01, 4.60074943e-06, 2.71800846e-01, 2.47260073e-06,\n",
              "        3.74383788e-04, 5.88039802e-06, 2.33139537e-04],\n",
              "       [1.08988585e-04, 2.60619652e-02, 1.22114925e-05, 9.70545053e-01,\n",
              "        1.48586161e-03, 6.36095356e-05, 1.72223046e-03],\n",
              "       [4.60785259e-07, 9.99951601e-01, 2.88310531e-07, 1.22759593e-05,\n",
              "        2.74443300e-05, 4.74401531e-06, 3.16249270e-06],\n",
              "       [9.08161805e-04, 1.59455840e-06, 9.98906136e-01, 4.60310986e-07,\n",
              "        1.70574596e-04, 2.69002317e-06, 1.04015253e-05],\n",
              "       [3.11833064e-05, 3.22439754e-03, 1.02556776e-04, 8.54663551e-01,\n",
              "        4.58959304e-02, 9.52137783e-02, 8.68609815e-04],\n",
              "       [1.93051746e-05, 8.31389666e-01, 2.31055328e-05, 3.20539344e-03,\n",
              "        1.64555341e-01, 2.92225363e-04, 5.14952000e-04],\n",
              "       [8.69522978e-07, 8.69549467e-07, 2.27108700e-07, 1.08029980e-07,\n",
              "        9.99996424e-01, 7.67468720e-08, 1.46471064e-06],\n",
              "       [5.18806473e-07, 7.62201307e-05, 1.18299090e-06, 9.99336541e-01,\n",
              "        2.44985189e-04, 2.88677402e-04, 5.18110865e-05],\n",
              "       [1.42104250e-06, 9.24966156e-01, 1.01384639e-05, 4.58206378e-05,\n",
              "        1.92098058e-04, 7.47732148e-02, 1.11715881e-05],\n",
              "       [4.40681761e-05, 2.39351198e-01, 2.68779258e-05, 7.49696493e-01,\n",
              "        2.93946854e-04, 9.83730052e-03, 7.50090461e-04],\n",
              "       [3.97509098e-07, 9.31759458e-03, 3.13973578e-06, 7.50522304e-05,\n",
              "        1.52161483e-05, 9.90586579e-01, 2.01159173e-06],\n",
              "       [4.06286745e-07, 9.97027934e-01, 1.77821983e-06, 7.26639882e-06,\n",
              "        1.37800262e-05, 2.94769905e-03, 1.20528819e-06],\n",
              "       [1.51456217e-04, 7.15900001e-07, 9.99827087e-01, 1.47668544e-07,\n",
              "        1.61574389e-05, 2.61264927e-06, 1.87165995e-06],\n",
              "       [9.92533751e-05, 1.78309419e-05, 5.60959149e-03, 2.13702629e-06,\n",
              "        9.94195521e-01, 6.18359845e-05, 1.38088753e-05],\n",
              "       [1.27719468e-05, 3.60179074e-05, 5.37691585e-06, 9.76490021e-01,\n",
              "        2.17969995e-03, 1.64568250e-04, 2.11116076e-02],\n",
              "       [6.28134236e-03, 4.97378869e-06, 9.93616343e-01, 1.59285810e-06,\n",
              "        6.99505181e-05, 1.50007454e-05, 1.09076982e-05],\n",
              "       [2.36363334e-07, 1.27262574e-05, 2.30846717e-06, 3.62131516e-08,\n",
              "        1.11700189e-07, 9.99983668e-01, 8.94819777e-07],\n",
              "       [1.58956027e-06, 1.29226893e-01, 6.14048940e-06, 1.32024015e-05,\n",
              "        1.26351324e-05, 8.70736539e-01, 2.87495845e-06],\n",
              "       [2.51961581e-04, 7.46289606e-06, 9.99652743e-01, 1.50858125e-06,\n",
              "        3.86758293e-05, 2.71004610e-05, 2.05526685e-05],\n",
              "       [3.54114085e-01, 5.06473270e-06, 6.45140946e-01, 4.45381556e-06,\n",
              "        6.49276015e-04, 1.75935929e-05, 6.84772531e-05],\n",
              "       [9.99790013e-01, 2.37653722e-07, 1.62807104e-04, 9.41919154e-07,\n",
              "        2.43723589e-05, 7.79919048e-07, 2.08852071e-05],\n",
              "       [4.24340862e-04, 1.88628928e-05, 9.50048864e-03, 8.41605470e-06,\n",
              "        9.89932656e-01, 4.05942155e-05, 7.47287268e-05],\n",
              "       [1.74887027e-05, 1.31647869e-06, 8.11254449e-06, 1.20019640e-05,\n",
              "        1.28054462e-05, 3.39523854e-06, 9.99944925e-01],\n",
              "       [5.70215434e-07, 3.80241341e-04, 3.65565347e-06, 4.23483073e-07,\n",
              "        2.30916667e-05, 9.99590933e-01, 1.07688288e-06],\n",
              "       [7.50414756e-06, 2.90575175e-04, 1.86041962e-05, 6.62554230e-04,\n",
              "        6.08688388e-05, 9.98936117e-01, 2.37774129e-05],\n",
              "       [2.25523300e-02, 1.93115731e-04, 9.45569456e-01, 1.97457353e-04,\n",
              "        3.05759031e-02, 6.67354150e-04, 2.44414899e-04],\n",
              "       [9.73212361e-01, 1.09984630e-05, 1.09214301e-03, 3.70211055e-05,\n",
              "        2.55179424e-02, 9.09263781e-06, 1.20388337e-04],\n",
              "       [3.38302390e-03, 7.53954112e-07, 9.96561706e-01, 1.19441779e-07,\n",
              "        4.84246011e-05, 3.64412676e-06, 2.31848730e-06],\n",
              "       [7.49071361e-04, 1.54517852e-06, 9.99220133e-01, 2.49742101e-07,\n",
              "        1.75135356e-05, 9.36717061e-06, 2.08656093e-06],\n",
              "       [3.70074617e-04, 2.00804652e-06, 1.52526127e-06, 9.20907041e-06,\n",
              "        3.60930676e-06, 1.61268622e-06, 9.99611914e-01],\n",
              "       [3.11935495e-04, 2.83813893e-06, 9.99639273e-01, 6.10859104e-07,\n",
              "        1.41543906e-05, 2.33555493e-05, 7.88353827e-06],\n",
              "       [1.05077163e-06, 4.05701548e-02, 4.03111289e-06, 2.01468029e-05,\n",
              "        1.16791052e-05, 9.59389269e-01, 3.71680267e-06]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 248
        }
      ],
      "source": [
        "y_test_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "id": "996eb306",
      "metadata": {
        "id": "996eb306"
      },
      "outputs": [],
      "source": [
        "y_test_predictions=np.argmax(y_test_predictions,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "id": "5f9002eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f9002eb",
        "outputId": "42b3dab1-eff7-41e2-82eb-9e6ddcddea56"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 2, 2, 4, 2, 1, 2, 4, 1, 3, 3, 2, 2, 4, 3, 2, 2, 0, 2, 0, 0,\n",
              "       0, 5, 6, 4, 2, 0, 4, 5, 3, 5, 4, 1, 0, 3, 2, 0, 1, 0, 1, 3, 3, 4,\n",
              "       0, 1, 1, 0, 0, 2, 0, 1, 3, 4, 6, 1, 1, 3, 4, 5, 6, 3, 1, 2, 0, 2,\n",
              "       6, 0, 1, 6, 4, 4, 5, 2, 2, 0, 3, 1, 2, 3, 1, 4, 3, 1, 3, 5, 1, 2,\n",
              "       4, 3, 2, 5, 5, 2, 2, 0, 4, 6, 5, 5, 2, 0, 2, 2, 6, 2, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 250
        }
      ],
      "source": [
        "y_test_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "id": "2a98a885",
      "metadata": {
        "id": "2a98a885"
      },
      "outputs": [],
      "source": [
        "# df.replace({ 'happyness': 0, 'neutral': 1,'anger': 2,'sadness': 3, 'fear':4,'boredom':5,'disgust':6}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 252,
      "id": "d0b8e93d",
      "metadata": {
        "id": "d0b8e93d"
      },
      "outputs": [],
      "source": [
        "emotions={\n",
        " 0: 'happyness',\n",
        " 1: 'neutral',\n",
        " 2: 'anger',\n",
        " 3: 'sadness',\n",
        " 4: 'fear',\n",
        " 5: 'boredom',\n",
        " 6: 'disgust',\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 253,
      "id": "a32e6964",
      "metadata": {
        "id": "a32e6964"
      },
      "outputs": [],
      "source": [
        "label=[]\n",
        "for i in y_test_predictions:\n",
        "    label1=emotions[i]\n",
        "    label.append(label1)\n",
        "label\n",
        "y_pred_acc=np.array(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 254,
      "id": "c92b0963",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c92b0963",
        "outputId": "709e7add-d52c-41f7-86e4-ee39f40ac137"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['neutral', 'neutral', 'anger', 'anger', 'fear', 'anger', 'neutral',\n",
              "       'anger', 'fear', 'neutral', 'sadness', 'sadness', 'anger', 'anger',\n",
              "       'fear', 'sadness', 'anger', 'anger', 'happyness', 'anger',\n",
              "       'happyness', 'happyness', 'happyness', 'boredom', 'disgust',\n",
              "       'fear', 'anger', 'happyness', 'fear', 'boredom', 'sadness',\n",
              "       'boredom', 'fear', 'neutral', 'happyness', 'sadness', 'anger',\n",
              "       'happyness', 'neutral', 'happyness', 'neutral', 'sadness',\n",
              "       'sadness', 'fear', 'happyness', 'neutral', 'neutral', 'happyness',\n",
              "       'happyness', 'anger', 'happyness', 'neutral', 'sadness', 'fear',\n",
              "       'disgust', 'neutral', 'neutral', 'sadness', 'fear', 'boredom',\n",
              "       'disgust', 'sadness', 'neutral', 'anger', 'happyness', 'anger',\n",
              "       'disgust', 'happyness', 'neutral', 'disgust', 'fear', 'fear',\n",
              "       'boredom', 'anger', 'anger', 'happyness', 'sadness', 'neutral',\n",
              "       'anger', 'sadness', 'neutral', 'fear', 'sadness', 'neutral',\n",
              "       'sadness', 'boredom', 'neutral', 'anger', 'fear', 'sadness',\n",
              "       'anger', 'boredom', 'boredom', 'anger', 'anger', 'happyness',\n",
              "       'fear', 'disgust', 'boredom', 'boredom', 'anger', 'happyness',\n",
              "       'anger', 'anger', 'disgust', 'anger', 'boredom'], dtype='<U9')"
            ]
          },
          "metadata": {},
          "execution_count": 254
        }
      ],
      "source": [
        "y_pred_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 255,
      "id": "59bdabc1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59bdabc1",
        "outputId": "f5447f4a-a630-4635-a9c4-194c83819ba8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      1\n",
              "1      1\n",
              "2      2\n",
              "3      2\n",
              "4      4\n",
              "      ..\n",
              "102    0\n",
              "103    2\n",
              "104    6\n",
              "105    2\n",
              "106    5\n",
              "Name: 0.1, Length: 107, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 255
        }
      ],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 256,
      "id": "0dec7231",
      "metadata": {
        "id": "0dec7231"
      },
      "outputs": [],
      "source": [
        "emotion={\n",
        " 0: 'happyness',\n",
        " 1: 'neutral',\n",
        " 2: 'anger',\n",
        " 3: 'sadness',\n",
        " 4: 'fear',\n",
        " 5: 'boredom',\n",
        " 6: 'disgust',\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 257,
      "id": "06e8caee",
      "metadata": {
        "id": "06e8caee"
      },
      "outputs": [],
      "source": [
        "label_test=[]\n",
        "for i in y_test:\n",
        "    label_test.append(emotion[i])\n",
        "label_test\n",
        "y_true_accu=np.array(label_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 258,
      "id": "9562d2b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9562d2b0",
        "outputId": "a4a1e72f-695d-453e-e665-3c1c3e71eb1d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['neutral', 'neutral', 'anger', 'anger', 'fear', 'anger', 'neutral',\n",
              "       'anger', 'fear', 'neutral', 'sadness', 'sadness', 'anger', 'anger',\n",
              "       'fear', 'sadness', 'anger', 'anger', 'happyness', 'anger',\n",
              "       'happyness', 'happyness', 'disgust', 'boredom', 'disgust', 'fear',\n",
              "       'anger', 'happyness', 'fear', 'boredom', 'sadness', 'boredom',\n",
              "       'fear', 'neutral', 'happyness', 'sadness', 'anger', 'happyness',\n",
              "       'neutral', 'happyness', 'boredom', 'sadness', 'sadness', 'disgust',\n",
              "       'happyness', 'neutral', 'neutral', 'happyness', 'fear', 'anger',\n",
              "       'happyness', 'boredom', 'sadness', 'fear', 'disgust', 'neutral',\n",
              "       'neutral', 'sadness', 'fear', 'boredom', 'neutral', 'sadness',\n",
              "       'boredom', 'anger', 'happyness', 'anger', 'disgust', 'anger',\n",
              "       'boredom', 'disgust', 'fear', 'disgust', 'boredom', 'anger',\n",
              "       'anger', 'anger', 'boredom', 'neutral', 'happyness', 'boredom',\n",
              "       'neutral', 'fear', 'sadness', 'neutral', 'sadness', 'neutral',\n",
              "       'neutral', 'anger', 'boredom', 'sadness', 'anger', 'boredom',\n",
              "       'boredom', 'anger', 'anger', 'happyness', 'fear', 'disgust',\n",
              "       'boredom', 'boredom', 'anger', 'fear', 'happyness', 'anger',\n",
              "       'disgust', 'anger', 'boredom'], dtype='<U9')"
            ]
          },
          "metadata": {},
          "execution_count": 258
        }
      ],
      "source": [
        "y_true_accu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "id": "8def2194",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8def2194",
        "outputId": "65903e03-ad17-4342-8b34-6dcb088b6c40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 83.18%\n"
          ]
        }
      ],
      "source": [
        "#DataFlair - Calculate the accuracy of our model\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy=accuracy_score(y_true=y_true_accu, y_pred=y_pred_acc)\n",
        "\n",
        "#DataFlair - Print the accuracy\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 260,
      "id": "0397dc13",
      "metadata": {
        "id": "0397dc13"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm=confusion_matrix(y_true=y_true_accu, y_pred=y_pred_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 261,
      "id": "7cee98a1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cee98a1",
        "outputId": "95de73a3-cbdd-4678-8184-b5845efcb705"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.92      0.92      0.92        25\n",
            "     boredom       0.91      0.59      0.71        17\n",
            "     disgust       0.86      0.67      0.75         9\n",
            "        fear       0.79      0.85      0.81        13\n",
            "   happyness       0.71      0.86      0.77        14\n",
            "     neutral       0.78      0.88      0.82        16\n",
            "     sadness       0.87      1.00      0.93        13\n",
            "\n",
            "    accuracy                           0.83       107\n",
            "   macro avg       0.83      0.82      0.82       107\n",
            "weighted avg       0.84      0.83      0.83       107\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true_accu,y_pred_acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 261,
      "id": "73b3090a",
      "metadata": {
        "id": "73b3090a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 261,
      "id": "4537d18f",
      "metadata": {
        "id": "4537d18f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}