{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 106,
      "id": "e1f95dfd",
      "metadata": {
        "id": "e1f95dfd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import seaborn as sns\n",
        "import librosa\n",
        "import librosa.display\n",
        "import IPython.display as pld\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xe7uVNwQ3Ppw",
        "outputId": "1e5b806a-c963-4e71-bc3c-28ca463261e1"
      },
      "id": "Xe7uVNwQ3Ppw",
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "id": "603a1332",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "603a1332",
        "outputId": "79896ed2-b83c-456f-b936-b8d6d6d731af"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Unnamed: 0          0          1          2          3          4  \\\n",
              "0             0 -482.45233  62.835957  -0.348998  26.264595   2.978607   \n",
              "1             1 -469.48477  88.400730  -7.127512  29.156132   5.335554   \n",
              "2             2 -434.88647  41.972150 -29.416862  18.537344  -4.156565   \n",
              "3             3 -447.12630  86.114920   4.772520  38.097256   8.324276   \n",
              "4             4 -451.45264  71.335200  12.223010  32.649555   3.997801   \n",
              "..          ...        ...        ...        ...        ...        ...   \n",
              "423         423 -429.41583  51.232320  -4.127894  38.715740   2.253632   \n",
              "424         424 -478.05527   5.705422 -24.631056  27.622614 -19.235449   \n",
              "425         425 -418.62490  57.015880   6.383415  47.618423  -8.051490   \n",
              "426         426 -467.15588  52.217026  10.470471  47.414780   8.690019   \n",
              "427         427 -526.19570  11.317784 -18.942173  29.540787 -28.057306   \n",
              "\n",
              "             5          6          7          8  ...       250       251  \\\n",
              "0     6.900927 -13.006243   0.273391  -9.196591  ...  0.675823  0.684034   \n",
              "1     7.147227  -6.727572  -8.307674  -3.364513  ...  0.635522  0.544438   \n",
              "2     5.257353 -11.410935  -8.983023 -11.285996  ...  0.648380  0.675295   \n",
              "3     8.538317  -4.507682  -7.680664  -7.317249  ...  0.615263  0.566964   \n",
              "4    15.200773  -2.812883  -1.384373  -6.467040  ...  0.719856  0.667424   \n",
              "..         ...        ...        ...        ...  ...       ...       ...   \n",
              "423  -2.197497 -15.981533 -14.225066 -17.897070  ...  0.589742  0.609879   \n",
              "424  -7.040019 -13.293165  -9.629133 -14.067036  ...  0.590422  0.481957   \n",
              "425   4.213936 -15.029120  -2.448467 -10.805821  ...  0.569263  0.551809   \n",
              "426  15.601270  -2.032935   4.985774  -7.432734  ...  0.523647  0.509723   \n",
              "427   1.299599 -16.251814  -7.512440 -23.191568  ...  0.528861  0.501070   \n",
              "\n",
              "          252       253       254       255       256       257       258  0.1  \n",
              "0    0.627376 -0.001574  0.012645 -0.027069  0.019313 -0.002252 -0.006220    0  \n",
              "1    0.532856 -0.018488  0.011973 -0.011038  0.079758 -0.021044 -0.019445    1  \n",
              "2    0.560000 -0.010576 -0.000228  0.011102 -0.074188  0.012116 -0.000779    2  \n",
              "3    0.605103 -0.008389 -0.051995 -0.056544 -0.020014  0.013964  0.008349    1  \n",
              "4    0.570905  0.003044 -0.006101 -0.009038 -0.049699  0.027615  0.021319    3  \n",
              "..        ...       ...       ...       ...       ...       ...       ...  ...  \n",
              "423  0.515997 -0.019026  0.000340 -0.022717 -0.020690 -0.002970 -0.001101    6  \n",
              "424  0.481437 -0.011216 -0.031485 -0.029936  0.042160 -0.013815  0.009839    0  \n",
              "425  0.576764  0.016046 -0.025610  0.025426 -0.094371  0.008999 -0.004985    5  \n",
              "426  0.498566  0.051532  0.015525 -0.006052  0.018201  0.004361 -0.021649    3  \n",
              "427  0.496900  0.007856  0.005917 -0.045393 -0.004246 -0.001628  0.015051    2  \n",
              "\n",
              "[428 rows x 261 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-caec497a-bfc9-44a4-b12a-1ea5c59c642c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>...</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "      <th>257</th>\n",
              "      <th>258</th>\n",
              "      <th>0.1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-482.45233</td>\n",
              "      <td>62.835957</td>\n",
              "      <td>-0.348998</td>\n",
              "      <td>26.264595</td>\n",
              "      <td>2.978607</td>\n",
              "      <td>6.900927</td>\n",
              "      <td>-13.006243</td>\n",
              "      <td>0.273391</td>\n",
              "      <td>-9.196591</td>\n",
              "      <td>...</td>\n",
              "      <td>0.675823</td>\n",
              "      <td>0.684034</td>\n",
              "      <td>0.627376</td>\n",
              "      <td>-0.001574</td>\n",
              "      <td>0.012645</td>\n",
              "      <td>-0.027069</td>\n",
              "      <td>0.019313</td>\n",
              "      <td>-0.002252</td>\n",
              "      <td>-0.006220</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>-469.48477</td>\n",
              "      <td>88.400730</td>\n",
              "      <td>-7.127512</td>\n",
              "      <td>29.156132</td>\n",
              "      <td>5.335554</td>\n",
              "      <td>7.147227</td>\n",
              "      <td>-6.727572</td>\n",
              "      <td>-8.307674</td>\n",
              "      <td>-3.364513</td>\n",
              "      <td>...</td>\n",
              "      <td>0.635522</td>\n",
              "      <td>0.544438</td>\n",
              "      <td>0.532856</td>\n",
              "      <td>-0.018488</td>\n",
              "      <td>0.011973</td>\n",
              "      <td>-0.011038</td>\n",
              "      <td>0.079758</td>\n",
              "      <td>-0.021044</td>\n",
              "      <td>-0.019445</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>-434.88647</td>\n",
              "      <td>41.972150</td>\n",
              "      <td>-29.416862</td>\n",
              "      <td>18.537344</td>\n",
              "      <td>-4.156565</td>\n",
              "      <td>5.257353</td>\n",
              "      <td>-11.410935</td>\n",
              "      <td>-8.983023</td>\n",
              "      <td>-11.285996</td>\n",
              "      <td>...</td>\n",
              "      <td>0.648380</td>\n",
              "      <td>0.675295</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>-0.010576</td>\n",
              "      <td>-0.000228</td>\n",
              "      <td>0.011102</td>\n",
              "      <td>-0.074188</td>\n",
              "      <td>0.012116</td>\n",
              "      <td>-0.000779</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>-447.12630</td>\n",
              "      <td>86.114920</td>\n",
              "      <td>4.772520</td>\n",
              "      <td>38.097256</td>\n",
              "      <td>8.324276</td>\n",
              "      <td>8.538317</td>\n",
              "      <td>-4.507682</td>\n",
              "      <td>-7.680664</td>\n",
              "      <td>-7.317249</td>\n",
              "      <td>...</td>\n",
              "      <td>0.615263</td>\n",
              "      <td>0.566964</td>\n",
              "      <td>0.605103</td>\n",
              "      <td>-0.008389</td>\n",
              "      <td>-0.051995</td>\n",
              "      <td>-0.056544</td>\n",
              "      <td>-0.020014</td>\n",
              "      <td>0.013964</td>\n",
              "      <td>0.008349</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>-451.45264</td>\n",
              "      <td>71.335200</td>\n",
              "      <td>12.223010</td>\n",
              "      <td>32.649555</td>\n",
              "      <td>3.997801</td>\n",
              "      <td>15.200773</td>\n",
              "      <td>-2.812883</td>\n",
              "      <td>-1.384373</td>\n",
              "      <td>-6.467040</td>\n",
              "      <td>...</td>\n",
              "      <td>0.719856</td>\n",
              "      <td>0.667424</td>\n",
              "      <td>0.570905</td>\n",
              "      <td>0.003044</td>\n",
              "      <td>-0.006101</td>\n",
              "      <td>-0.009038</td>\n",
              "      <td>-0.049699</td>\n",
              "      <td>0.027615</td>\n",
              "      <td>0.021319</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>423</th>\n",
              "      <td>423</td>\n",
              "      <td>-429.41583</td>\n",
              "      <td>51.232320</td>\n",
              "      <td>-4.127894</td>\n",
              "      <td>38.715740</td>\n",
              "      <td>2.253632</td>\n",
              "      <td>-2.197497</td>\n",
              "      <td>-15.981533</td>\n",
              "      <td>-14.225066</td>\n",
              "      <td>-17.897070</td>\n",
              "      <td>...</td>\n",
              "      <td>0.589742</td>\n",
              "      <td>0.609879</td>\n",
              "      <td>0.515997</td>\n",
              "      <td>-0.019026</td>\n",
              "      <td>0.000340</td>\n",
              "      <td>-0.022717</td>\n",
              "      <td>-0.020690</td>\n",
              "      <td>-0.002970</td>\n",
              "      <td>-0.001101</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424</th>\n",
              "      <td>424</td>\n",
              "      <td>-478.05527</td>\n",
              "      <td>5.705422</td>\n",
              "      <td>-24.631056</td>\n",
              "      <td>27.622614</td>\n",
              "      <td>-19.235449</td>\n",
              "      <td>-7.040019</td>\n",
              "      <td>-13.293165</td>\n",
              "      <td>-9.629133</td>\n",
              "      <td>-14.067036</td>\n",
              "      <td>...</td>\n",
              "      <td>0.590422</td>\n",
              "      <td>0.481957</td>\n",
              "      <td>0.481437</td>\n",
              "      <td>-0.011216</td>\n",
              "      <td>-0.031485</td>\n",
              "      <td>-0.029936</td>\n",
              "      <td>0.042160</td>\n",
              "      <td>-0.013815</td>\n",
              "      <td>0.009839</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>425</th>\n",
              "      <td>425</td>\n",
              "      <td>-418.62490</td>\n",
              "      <td>57.015880</td>\n",
              "      <td>6.383415</td>\n",
              "      <td>47.618423</td>\n",
              "      <td>-8.051490</td>\n",
              "      <td>4.213936</td>\n",
              "      <td>-15.029120</td>\n",
              "      <td>-2.448467</td>\n",
              "      <td>-10.805821</td>\n",
              "      <td>...</td>\n",
              "      <td>0.569263</td>\n",
              "      <td>0.551809</td>\n",
              "      <td>0.576764</td>\n",
              "      <td>0.016046</td>\n",
              "      <td>-0.025610</td>\n",
              "      <td>0.025426</td>\n",
              "      <td>-0.094371</td>\n",
              "      <td>0.008999</td>\n",
              "      <td>-0.004985</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>426</th>\n",
              "      <td>426</td>\n",
              "      <td>-467.15588</td>\n",
              "      <td>52.217026</td>\n",
              "      <td>10.470471</td>\n",
              "      <td>47.414780</td>\n",
              "      <td>8.690019</td>\n",
              "      <td>15.601270</td>\n",
              "      <td>-2.032935</td>\n",
              "      <td>4.985774</td>\n",
              "      <td>-7.432734</td>\n",
              "      <td>...</td>\n",
              "      <td>0.523647</td>\n",
              "      <td>0.509723</td>\n",
              "      <td>0.498566</td>\n",
              "      <td>0.051532</td>\n",
              "      <td>0.015525</td>\n",
              "      <td>-0.006052</td>\n",
              "      <td>0.018201</td>\n",
              "      <td>0.004361</td>\n",
              "      <td>-0.021649</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>427</th>\n",
              "      <td>427</td>\n",
              "      <td>-526.19570</td>\n",
              "      <td>11.317784</td>\n",
              "      <td>-18.942173</td>\n",
              "      <td>29.540787</td>\n",
              "      <td>-28.057306</td>\n",
              "      <td>1.299599</td>\n",
              "      <td>-16.251814</td>\n",
              "      <td>-7.512440</td>\n",
              "      <td>-23.191568</td>\n",
              "      <td>...</td>\n",
              "      <td>0.528861</td>\n",
              "      <td>0.501070</td>\n",
              "      <td>0.496900</td>\n",
              "      <td>0.007856</td>\n",
              "      <td>0.005917</td>\n",
              "      <td>-0.045393</td>\n",
              "      <td>-0.004246</td>\n",
              "      <td>-0.001628</td>\n",
              "      <td>0.015051</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>428 rows × 261 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-caec497a-bfc9-44a4-b12a-1ea5c59c642c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-caec497a-bfc9-44a4-b12a-1ea5c59c642c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-caec497a-bfc9-44a4-b12a-1ea5c59c642c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-af3c6d6d-4f40-4293-a084-64f7fc9f7495\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-af3c6d6d-4f40-4293-a084-64f7fc9f7495')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-af3c6d6d-4f40-4293-a084-64f7fc9f7495 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df1"
            }
          },
          "metadata": {},
          "execution_count": 108
        }
      ],
      "source": [
        "df1=pd.read_csv('/content/drive/MyDrive/DATASETS/EmoDB Dataset/train_fold2.csv')\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "id": "a08acf9d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "a08acf9d",
        "outputId": "e61258e2-5c4f-4061-dcaa-f30ea4948c5d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Unnamed: 0          0           1          2          3          4  \\\n",
              "0             0 -454.89886   45.067265  -0.193278  15.111545   3.080835   \n",
              "1             1 -445.90384  103.783450   1.365116  27.908356  -4.542189   \n",
              "2             2 -432.85320   94.606660  -5.319786  25.696900   0.194088   \n",
              "3             3 -414.28708   45.442410   5.925002  21.436172   0.354569   \n",
              "4             4 -434.16900   78.900970   4.866675  45.928330   4.804515   \n",
              "..          ...        ...         ...        ...        ...        ...   \n",
              "102         102 -392.46520   73.364350  -3.410112  28.893300  -9.239878   \n",
              "103         103 -431.76166   50.142090   3.661988  45.098984 -13.927658   \n",
              "104         104 -403.50656   30.989262 -12.756512  27.214573 -10.573180   \n",
              "105         105 -427.36716   51.473750   4.835125  40.900140   6.937289   \n",
              "106         106 -437.97220    5.289146 -22.667547  25.394840 -28.545520   \n",
              "\n",
              "             5          6         7          8  ...       250       251  \\\n",
              "0     4.204241 -10.440731 -6.615343 -16.249382  ...  0.575784  0.523305   \n",
              "1    10.230102 -10.717600 -8.279192  -2.135225  ...  0.659291  0.604083   \n",
              "2     7.087550  -1.224768 -5.595039 -10.824135  ...  0.605255  0.605432   \n",
              "3     7.393221  -7.657958 -4.724375 -18.931787  ...  0.618056  0.525700   \n",
              "4     2.177262 -13.456310 -5.863212  -3.584935  ...  0.660314  0.630964   \n",
              "..         ...        ...       ...        ...  ...       ...       ...   \n",
              "102  17.295076 -14.857638 -8.589843 -15.333829  ...  0.555782  0.517858   \n",
              "103  -0.977742 -16.576310  0.029880  -0.389472  ...  0.607035  0.589502   \n",
              "104  -0.137991 -15.470394 -8.865036 -16.384440  ...  0.531090  0.567376   \n",
              "105  13.700687  -8.331753 -0.583414  -6.279562  ...  0.638155  0.574542   \n",
              "106   0.428451 -11.650926 -8.022680 -18.337156  ...  0.495079  0.539387   \n",
              "\n",
              "          252       253       254       255       256       257       258  0.1  \n",
              "0    0.423375  0.009395 -0.025547 -0.035554 -0.025885  0.011198 -0.000163    0  \n",
              "1    0.500168  0.006800 -0.019560 -0.049193 -0.089028  0.020237  0.023672    5  \n",
              "2    0.592474 -0.008104 -0.009647 -0.046115 -0.007378  0.006855 -0.004688    1  \n",
              "3    0.500340 -0.018470 -0.007622 -0.006954  0.023004  0.002193 -0.005318    0  \n",
              "4    0.564337 -0.004686  0.006942  0.026904  0.006124  0.004141  0.017759    5  \n",
              "..        ...       ...       ...       ...       ...       ...       ...  ...  \n",
              "102  0.473756  0.000104 -0.007319 -0.020273  0.040238  0.012407 -0.014820    1  \n",
              "103  0.506217  0.006985  0.040552  0.005959 -0.065906 -0.023141 -0.005444    5  \n",
              "104  0.470224 -0.005500  0.013583  0.040782  0.003971  0.017743  0.026657    4  \n",
              "105  0.503516  0.019632  0.046315  0.074801  0.018078  0.022948 -0.016365    3  \n",
              "106  0.527394 -0.001086  0.013948  0.013274 -0.056855 -0.001004 -0.001355    2  \n",
              "\n",
              "[107 rows x 261 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5389daa6-079f-4161-aa08-c50dbc953de8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>...</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "      <th>257</th>\n",
              "      <th>258</th>\n",
              "      <th>0.1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-454.89886</td>\n",
              "      <td>45.067265</td>\n",
              "      <td>-0.193278</td>\n",
              "      <td>15.111545</td>\n",
              "      <td>3.080835</td>\n",
              "      <td>4.204241</td>\n",
              "      <td>-10.440731</td>\n",
              "      <td>-6.615343</td>\n",
              "      <td>-16.249382</td>\n",
              "      <td>...</td>\n",
              "      <td>0.575784</td>\n",
              "      <td>0.523305</td>\n",
              "      <td>0.423375</td>\n",
              "      <td>0.009395</td>\n",
              "      <td>-0.025547</td>\n",
              "      <td>-0.035554</td>\n",
              "      <td>-0.025885</td>\n",
              "      <td>0.011198</td>\n",
              "      <td>-0.000163</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>-445.90384</td>\n",
              "      <td>103.783450</td>\n",
              "      <td>1.365116</td>\n",
              "      <td>27.908356</td>\n",
              "      <td>-4.542189</td>\n",
              "      <td>10.230102</td>\n",
              "      <td>-10.717600</td>\n",
              "      <td>-8.279192</td>\n",
              "      <td>-2.135225</td>\n",
              "      <td>...</td>\n",
              "      <td>0.659291</td>\n",
              "      <td>0.604083</td>\n",
              "      <td>0.500168</td>\n",
              "      <td>0.006800</td>\n",
              "      <td>-0.019560</td>\n",
              "      <td>-0.049193</td>\n",
              "      <td>-0.089028</td>\n",
              "      <td>0.020237</td>\n",
              "      <td>0.023672</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>-432.85320</td>\n",
              "      <td>94.606660</td>\n",
              "      <td>-5.319786</td>\n",
              "      <td>25.696900</td>\n",
              "      <td>0.194088</td>\n",
              "      <td>7.087550</td>\n",
              "      <td>-1.224768</td>\n",
              "      <td>-5.595039</td>\n",
              "      <td>-10.824135</td>\n",
              "      <td>...</td>\n",
              "      <td>0.605255</td>\n",
              "      <td>0.605432</td>\n",
              "      <td>0.592474</td>\n",
              "      <td>-0.008104</td>\n",
              "      <td>-0.009647</td>\n",
              "      <td>-0.046115</td>\n",
              "      <td>-0.007378</td>\n",
              "      <td>0.006855</td>\n",
              "      <td>-0.004688</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>-414.28708</td>\n",
              "      <td>45.442410</td>\n",
              "      <td>5.925002</td>\n",
              "      <td>21.436172</td>\n",
              "      <td>0.354569</td>\n",
              "      <td>7.393221</td>\n",
              "      <td>-7.657958</td>\n",
              "      <td>-4.724375</td>\n",
              "      <td>-18.931787</td>\n",
              "      <td>...</td>\n",
              "      <td>0.618056</td>\n",
              "      <td>0.525700</td>\n",
              "      <td>0.500340</td>\n",
              "      <td>-0.018470</td>\n",
              "      <td>-0.007622</td>\n",
              "      <td>-0.006954</td>\n",
              "      <td>0.023004</td>\n",
              "      <td>0.002193</td>\n",
              "      <td>-0.005318</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>-434.16900</td>\n",
              "      <td>78.900970</td>\n",
              "      <td>4.866675</td>\n",
              "      <td>45.928330</td>\n",
              "      <td>4.804515</td>\n",
              "      <td>2.177262</td>\n",
              "      <td>-13.456310</td>\n",
              "      <td>-5.863212</td>\n",
              "      <td>-3.584935</td>\n",
              "      <td>...</td>\n",
              "      <td>0.660314</td>\n",
              "      <td>0.630964</td>\n",
              "      <td>0.564337</td>\n",
              "      <td>-0.004686</td>\n",
              "      <td>0.006942</td>\n",
              "      <td>0.026904</td>\n",
              "      <td>0.006124</td>\n",
              "      <td>0.004141</td>\n",
              "      <td>0.017759</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>102</td>\n",
              "      <td>-392.46520</td>\n",
              "      <td>73.364350</td>\n",
              "      <td>-3.410112</td>\n",
              "      <td>28.893300</td>\n",
              "      <td>-9.239878</td>\n",
              "      <td>17.295076</td>\n",
              "      <td>-14.857638</td>\n",
              "      <td>-8.589843</td>\n",
              "      <td>-15.333829</td>\n",
              "      <td>...</td>\n",
              "      <td>0.555782</td>\n",
              "      <td>0.517858</td>\n",
              "      <td>0.473756</td>\n",
              "      <td>0.000104</td>\n",
              "      <td>-0.007319</td>\n",
              "      <td>-0.020273</td>\n",
              "      <td>0.040238</td>\n",
              "      <td>0.012407</td>\n",
              "      <td>-0.014820</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>103</td>\n",
              "      <td>-431.76166</td>\n",
              "      <td>50.142090</td>\n",
              "      <td>3.661988</td>\n",
              "      <td>45.098984</td>\n",
              "      <td>-13.927658</td>\n",
              "      <td>-0.977742</td>\n",
              "      <td>-16.576310</td>\n",
              "      <td>0.029880</td>\n",
              "      <td>-0.389472</td>\n",
              "      <td>...</td>\n",
              "      <td>0.607035</td>\n",
              "      <td>0.589502</td>\n",
              "      <td>0.506217</td>\n",
              "      <td>0.006985</td>\n",
              "      <td>0.040552</td>\n",
              "      <td>0.005959</td>\n",
              "      <td>-0.065906</td>\n",
              "      <td>-0.023141</td>\n",
              "      <td>-0.005444</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>104</td>\n",
              "      <td>-403.50656</td>\n",
              "      <td>30.989262</td>\n",
              "      <td>-12.756512</td>\n",
              "      <td>27.214573</td>\n",
              "      <td>-10.573180</td>\n",
              "      <td>-0.137991</td>\n",
              "      <td>-15.470394</td>\n",
              "      <td>-8.865036</td>\n",
              "      <td>-16.384440</td>\n",
              "      <td>...</td>\n",
              "      <td>0.531090</td>\n",
              "      <td>0.567376</td>\n",
              "      <td>0.470224</td>\n",
              "      <td>-0.005500</td>\n",
              "      <td>0.013583</td>\n",
              "      <td>0.040782</td>\n",
              "      <td>0.003971</td>\n",
              "      <td>0.017743</td>\n",
              "      <td>0.026657</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>105</td>\n",
              "      <td>-427.36716</td>\n",
              "      <td>51.473750</td>\n",
              "      <td>4.835125</td>\n",
              "      <td>40.900140</td>\n",
              "      <td>6.937289</td>\n",
              "      <td>13.700687</td>\n",
              "      <td>-8.331753</td>\n",
              "      <td>-0.583414</td>\n",
              "      <td>-6.279562</td>\n",
              "      <td>...</td>\n",
              "      <td>0.638155</td>\n",
              "      <td>0.574542</td>\n",
              "      <td>0.503516</td>\n",
              "      <td>0.019632</td>\n",
              "      <td>0.046315</td>\n",
              "      <td>0.074801</td>\n",
              "      <td>0.018078</td>\n",
              "      <td>0.022948</td>\n",
              "      <td>-0.016365</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>106</td>\n",
              "      <td>-437.97220</td>\n",
              "      <td>5.289146</td>\n",
              "      <td>-22.667547</td>\n",
              "      <td>25.394840</td>\n",
              "      <td>-28.545520</td>\n",
              "      <td>0.428451</td>\n",
              "      <td>-11.650926</td>\n",
              "      <td>-8.022680</td>\n",
              "      <td>-18.337156</td>\n",
              "      <td>...</td>\n",
              "      <td>0.495079</td>\n",
              "      <td>0.539387</td>\n",
              "      <td>0.527394</td>\n",
              "      <td>-0.001086</td>\n",
              "      <td>0.013948</td>\n",
              "      <td>0.013274</td>\n",
              "      <td>-0.056855</td>\n",
              "      <td>-0.001004</td>\n",
              "      <td>-0.001355</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>107 rows × 261 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5389daa6-079f-4161-aa08-c50dbc953de8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5389daa6-079f-4161-aa08-c50dbc953de8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5389daa6-079f-4161-aa08-c50dbc953de8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1b6d72a2-6bc3-4210-9b3d-85f94d4a9223\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1b6d72a2-6bc3-4210-9b3d-85f94d4a9223')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1b6d72a2-6bc3-4210-9b3d-85f94d4a9223 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df2"
            }
          },
          "metadata": {},
          "execution_count": 109
        }
      ],
      "source": [
        "df2=pd.read_csv('/content/drive/MyDrive/DATASETS/EmoDB Dataset/test_fold2.csv')\n",
        "df2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "id": "a3d750f7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3d750f7",
        "outputId": "7a537707-21e0-4575-c2d7-a70a434c5b93"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-482.45233, -469.48477, -434.88647, -447.1263 , -451.45264,\n",
              "       -433.33972, -465.2206 , -407.61963, -478.92007, -438.46402,\n",
              "       -451.66434, -415.74384, -444.03302, -449.64343, -448.45706,\n",
              "       -447.862  , -447.8849 , -405.8289 , -414.1051 , -415.96158,\n",
              "       -425.45212, -403.34033, -452.2124 , -491.33237, -433.93875,\n",
              "       -456.17935, -381.5805 , -454.38022, -437.03818, -508.38065,\n",
              "       -440.73105, -525.19977, -451.39062, -447.41296, -474.65   ,\n",
              "       -456.05096, -474.31326, -390.37305, -446.3631 , -436.3282 ,\n",
              "       -433.23328, -439.48215, -466.99695, -466.81842, -494.3395 ,\n",
              "       -426.0529 , -430.6762 , -482.82642, -448.18274, -410.33658,\n",
              "       -446.7974 , -453.8969 , -453.46445, -376.84555, -436.69876,\n",
              "       -493.1862 , -442.59415, -454.48465, -468.04486, -456.72043,\n",
              "       -416.0819 , -418.3227 , -423.0657 , -478.63577, -483.07538,\n",
              "       -434.9256 , -435.4896 , -399.51852, -402.0103 , -464.13718,\n",
              "       -461.44766, -474.8092 , -503.61353, -453.04343, -482.67584,\n",
              "       -409.22214, -531.9213 , -455.4235 , -456.9332 , -512.6878 ,\n",
              "       -485.50922, -427.06485, -418.001  , -456.90128, -492.80527,\n",
              "       -488.90784, -477.71838, -468.5587 , -503.9724 , -439.49243,\n",
              "       -447.12366, -504.06378, -418.71957, -414.93512, -503.28616,\n",
              "       -460.4001 , -453.48303, -488.23538, -432.9396 , -459.55142,\n",
              "       -492.041  , -407.6214 , -386.1615 , -463.09274, -458.41788,\n",
              "       -447.04764, -439.2523 , -446.3131 , -405.84958, -378.2114 ,\n",
              "       -390.5131 , -470.87045, -470.00208, -470.88403, -413.56833,\n",
              "       -445.18192, -473.9847 , -433.5633 , -390.45312, -446.96768,\n",
              "       -434.1118 , -410.16275, -406.1605 , -448.96103, -429.79407,\n",
              "       -420.2247 , -401.80896, -428.3561 , -405.93726, -420.10004,\n",
              "       -389.66595, -345.723  , -403.9027 , -402.585  , -434.36496,\n",
              "       -472.72705, -412.15396, -409.9087 , -413.8373 , -359.38922,\n",
              "       -391.26596, -370.994  , -449.62973, -365.9771 , -400.4402 ,\n",
              "       -464.32104, -418.5449 , -412.62817, -521.55493, -414.99844,\n",
              "       -421.95407, -497.26077, -463.6061 , -420.12646, -466.10834,\n",
              "       -513.68   , -467.4495 , -457.78012, -488.26974, -461.97726,\n",
              "       -460.52005, -464.94745, -460.19077, -451.02377, -452.5104 ,\n",
              "       -547.34924, -426.89627, -421.20755, -443.90628, -481.4349 ,\n",
              "       -408.07852, -473.36694, -467.0893 , -422.1737 , -473.6945 ,\n",
              "       -411.14822, -470.8944 , -479.00146, -413.97577, -453.5756 ,\n",
              "       -462.8844 , -471.02194, -513.1609 , -517.8032 , -479.2676 ,\n",
              "       -481.2874 , -410.65417, -441.25516, -458.29306, -462.08606,\n",
              "       -464.69373, -470.0053 , -482.287  , -416.1406 , -434.42883,\n",
              "       -474.72525, -376.26874, -413.52936, -418.1163 , -405.00757,\n",
              "       -435.22946, -443.34912, -470.02222, -439.98254, -417.20303,\n",
              "       -377.4015 , -385.19785, -454.7903 , -457.7515 , -422.8841 ,\n",
              "       -446.06177, -382.74545, -475.37103, -484.20596, -466.87482,\n",
              "       -420.7548 , -423.8322 , -452.5595 , -443.76602, -422.04797,\n",
              "       -392.5137 , -464.00873, -375.1225 , -409.2057 , -431.6872 ,\n",
              "       -454.1528 , -405.38095, -419.83594, -407.77988, -455.91553,\n",
              "       -395.2997 , -453.7782 , -393.9526 , -423.10638, -439.494  ,\n",
              "       -403.3263 , -386.62506, -496.95926, -421.93167, -471.28308,\n",
              "       -473.18396, -464.56476, -399.32123, -413.09387, -391.08258,\n",
              "       -481.6817 , -380.43036, -410.75797, -433.9074 , -405.67502,\n",
              "       -461.75302, -404.93027, -403.1864 , -392.26648, -452.835  ,\n",
              "       -448.45685, -413.7153 , -394.94235, -421.18414, -389.36047,\n",
              "       -437.94302, -421.31525, -425.45074, -441.02634, -412.16312,\n",
              "       -438.37827, -412.8278 , -453.3949 , -406.41467, -400.93872,\n",
              "       -427.86765, -459.23962, -460.4038 , -465.9359 , -421.23648,\n",
              "       -484.93997, -481.92627, -474.5434 , -442.24258, -502.14194,\n",
              "       -417.8694 , -487.77856, -491.2516 , -448.38986, -443.2527 ,\n",
              "       -487.9756 , -446.5495 , -441.62894, -426.17914, -504.21188,\n",
              "       -464.8163 , -431.91074, -466.36984, -471.5478 , -420.90936,\n",
              "       -436.0145 , -493.87128, -421.56473, -429.95325, -443.65494,\n",
              "       -449.66565, -440.50952, -484.456  , -428.66187, -424.6715 ,\n",
              "       -427.8873 , -412.55634, -466.64697, -449.63315, -463.29285,\n",
              "       -379.56708, -483.3338 , -478.16122, -441.50995, -440.85495,\n",
              "       -454.05173, -431.52588, -385.76328, -442.1517 , -412.06152,\n",
              "       -462.45517, -469.98242, -405.81763, -459.4864 , -378.77286,\n",
              "       -462.3047 , -434.34827, -386.1994 , -472.4163 , -419.2449 ,\n",
              "       -469.46295, -477.94263, -379.51093, -363.5735 , -414.5706 ,\n",
              "       -435.86343, -446.7613 , -442.37482, -415.02478, -449.23508,\n",
              "       -485.03238, -471.95856, -440.10852, -421.0112 , -439.35873,\n",
              "       -460.42752, -403.8571 , -412.05164, -372.68307, -372.10165,\n",
              "       -429.20193, -393.0507 , -403.01038, -451.00165, -412.0556 ,\n",
              "       -408.7152 , -426.08667, -332.69867, -411.78372, -414.32397,\n",
              "       -467.7443 , -428.96658, -395.0607 , -476.0034 , -452.18256,\n",
              "       -456.82553, -468.89825, -397.55426, -404.50937, -408.56897,\n",
              "       -425.47263, -393.92816, -437.02637, -457.2802 , -412.13974,\n",
              "       -467.06317, -420.7758 , -416.08627, -432.17923, -392.51147,\n",
              "       -542.9345 , -404.80206, -417.062  , -465.24496, -432.88263,\n",
              "       -404.11523, -393.34085, -530.95605, -476.9987 , -411.51193,\n",
              "       -498.8898 , -457.2986 , -492.2898 , -441.25778, -477.11566,\n",
              "       -411.96274, -457.94778, -524.4419 , -360.47025, -422.18552,\n",
              "       -358.59656, -542.5666 , -396.28677, -423.83383, -527.49243,\n",
              "       -408.97797, -390.08295, -465.86484, -462.9359 , -450.64505,\n",
              "       -435.4015 , -406.23282, -479.6645 , -427.612  , -441.11627,\n",
              "       -397.74173, -447.6558 , -491.1511 , -442.5056 , -475.25198,\n",
              "       -506.06097, -392.73694, -505.94534, -429.41583, -478.05527,\n",
              "       -418.6249 , -467.15588, -526.1957 ])"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ],
      "source": [
        "df1['0'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "id": "72e890ce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72e890ce",
        "outputId": "dd1d1e32-0dec-4875-8c3e-f618a6ab509f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-454.89886, -445.90384, -432.8532 , -414.28708, -434.169  ,\n",
              "       -440.71924, -448.62344, -423.79208, -457.93448, -521.504  ,\n",
              "       -446.97107, -446.98584, -461.82983, -425.79813, -464.07745,\n",
              "       -432.09225, -500.27402, -465.1539 , -424.5544 , -469.68045,\n",
              "       -444.93484, -456.18542, -437.33456, -514.9054 , -449.0555 ,\n",
              "       -403.73657, -405.75662, -377.2399 , -433.1373 , -434.17294,\n",
              "       -448.68094, -423.73795, -447.18866, -446.1366 , -514.8218 ,\n",
              "       -364.07712, -373.46188, -427.5882 , -437.44412, -440.67435,\n",
              "       -416.39185, -454.6912 , -443.02942, -470.54938, -407.5873 ,\n",
              "       -409.21838, -469.223  , -443.0332 , -513.6494 , -434.86633,\n",
              "       -435.5582 , -501.81232, -444.86087, -443.18646, -441.78366,\n",
              "       -402.7788 , -467.03882, -428.71326, -438.01385, -423.7153 ,\n",
              "       -459.8976 , -407.01892, -428.13434, -365.09543, -416.07724,\n",
              "       -438.8606 , -447.04645, -387.3086 , -420.1675 , -409.21655,\n",
              "       -414.18164, -468.193  , -444.60718, -418.8131 , -439.90536,\n",
              "       -445.24417, -446.16428, -367.194  , -522.2874 , -423.3954 ,\n",
              "       -461.70255, -470.61038, -459.56256, -454.7633 , -489.41888,\n",
              "       -449.0907 , -432.5484 , -407.74323, -404.7512 , -394.76035,\n",
              "       -398.8699 , -463.91916, -423.54108, -471.60022, -464.36145,\n",
              "       -436.95248, -455.44485, -458.9268 , -427.65176, -386.80893,\n",
              "       -485.59613, -464.9679 , -392.4652 , -431.76166, -403.50656,\n",
              "       -427.36716, -437.9722 ])"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ],
      "source": [
        "df2['0'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "id": "caeabe41",
      "metadata": {
        "id": "caeabe41"
      },
      "outputs": [],
      "source": [
        "x_train=df1.iloc[:,0:(df1.shape[1]-1)]\n",
        "y_train=df1.iloc[:,-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "id": "08d5857c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "08d5857c",
        "outputId": "5838f182-3f22-4a5b-9691-db10ddcd4cac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Unnamed: 0          0          1          2          3          4  \\\n",
              "0             0 -482.45233  62.835957  -0.348998  26.264595   2.978607   \n",
              "1             1 -469.48477  88.400730  -7.127512  29.156132   5.335554   \n",
              "2             2 -434.88647  41.972150 -29.416862  18.537344  -4.156565   \n",
              "3             3 -447.12630  86.114920   4.772520  38.097256   8.324276   \n",
              "4             4 -451.45264  71.335200  12.223010  32.649555   3.997801   \n",
              "..          ...        ...        ...        ...        ...        ...   \n",
              "423         423 -429.41583  51.232320  -4.127894  38.715740   2.253632   \n",
              "424         424 -478.05527   5.705422 -24.631056  27.622614 -19.235449   \n",
              "425         425 -418.62490  57.015880   6.383415  47.618423  -8.051490   \n",
              "426         426 -467.15588  52.217026  10.470471  47.414780   8.690019   \n",
              "427         427 -526.19570  11.317784 -18.942173  29.540787 -28.057306   \n",
              "\n",
              "             5          6          7          8  ...       249       250  \\\n",
              "0     6.900927 -13.006243   0.273391  -9.196591  ...  0.635609  0.675823   \n",
              "1     7.147227  -6.727572  -8.307674  -3.364513  ...  0.646426  0.635522   \n",
              "2     5.257353 -11.410935  -8.983023 -11.285996  ...  0.600825  0.648380   \n",
              "3     8.538317  -4.507682  -7.680664  -7.317249  ...  0.698287  0.615263   \n",
              "4    15.200773  -2.812883  -1.384373  -6.467040  ...  0.696758  0.719856   \n",
              "..         ...        ...        ...        ...  ...       ...       ...   \n",
              "423  -2.197497 -15.981533 -14.225066 -17.897070  ...  0.556236  0.589742   \n",
              "424  -7.040019 -13.293165  -9.629133 -14.067036  ...  0.597085  0.590422   \n",
              "425   4.213936 -15.029120  -2.448467 -10.805821  ...  0.566316  0.569263   \n",
              "426  15.601270  -2.032935   4.985774  -7.432734  ...  0.531997  0.523647   \n",
              "427   1.299599 -16.251814  -7.512440 -23.191568  ...  0.497285  0.528861   \n",
              "\n",
              "          251       252       253       254       255       256       257  \\\n",
              "0    0.684034  0.627376 -0.001574  0.012645 -0.027069  0.019313 -0.002252   \n",
              "1    0.544438  0.532856 -0.018488  0.011973 -0.011038  0.079758 -0.021044   \n",
              "2    0.675295  0.560000 -0.010576 -0.000228  0.011102 -0.074188  0.012116   \n",
              "3    0.566964  0.605103 -0.008389 -0.051995 -0.056544 -0.020014  0.013964   \n",
              "4    0.667424  0.570905  0.003044 -0.006101 -0.009038 -0.049699  0.027615   \n",
              "..        ...       ...       ...       ...       ...       ...       ...   \n",
              "423  0.609879  0.515997 -0.019026  0.000340 -0.022717 -0.020690 -0.002970   \n",
              "424  0.481957  0.481437 -0.011216 -0.031485 -0.029936  0.042160 -0.013815   \n",
              "425  0.551809  0.576764  0.016046 -0.025610  0.025426 -0.094371  0.008999   \n",
              "426  0.509723  0.498566  0.051532  0.015525 -0.006052  0.018201  0.004361   \n",
              "427  0.501070  0.496900  0.007856  0.005917 -0.045393 -0.004246 -0.001628   \n",
              "\n",
              "          258  \n",
              "0   -0.006220  \n",
              "1   -0.019445  \n",
              "2   -0.000779  \n",
              "3    0.008349  \n",
              "4    0.021319  \n",
              "..        ...  \n",
              "423 -0.001101  \n",
              "424  0.009839  \n",
              "425 -0.004985  \n",
              "426 -0.021649  \n",
              "427  0.015051  \n",
              "\n",
              "[428 rows x 260 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8091e06b-f3d1-442d-a086-a2e517ebeb16\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>...</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "      <th>257</th>\n",
              "      <th>258</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-482.45233</td>\n",
              "      <td>62.835957</td>\n",
              "      <td>-0.348998</td>\n",
              "      <td>26.264595</td>\n",
              "      <td>2.978607</td>\n",
              "      <td>6.900927</td>\n",
              "      <td>-13.006243</td>\n",
              "      <td>0.273391</td>\n",
              "      <td>-9.196591</td>\n",
              "      <td>...</td>\n",
              "      <td>0.635609</td>\n",
              "      <td>0.675823</td>\n",
              "      <td>0.684034</td>\n",
              "      <td>0.627376</td>\n",
              "      <td>-0.001574</td>\n",
              "      <td>0.012645</td>\n",
              "      <td>-0.027069</td>\n",
              "      <td>0.019313</td>\n",
              "      <td>-0.002252</td>\n",
              "      <td>-0.006220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>-469.48477</td>\n",
              "      <td>88.400730</td>\n",
              "      <td>-7.127512</td>\n",
              "      <td>29.156132</td>\n",
              "      <td>5.335554</td>\n",
              "      <td>7.147227</td>\n",
              "      <td>-6.727572</td>\n",
              "      <td>-8.307674</td>\n",
              "      <td>-3.364513</td>\n",
              "      <td>...</td>\n",
              "      <td>0.646426</td>\n",
              "      <td>0.635522</td>\n",
              "      <td>0.544438</td>\n",
              "      <td>0.532856</td>\n",
              "      <td>-0.018488</td>\n",
              "      <td>0.011973</td>\n",
              "      <td>-0.011038</td>\n",
              "      <td>0.079758</td>\n",
              "      <td>-0.021044</td>\n",
              "      <td>-0.019445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>-434.88647</td>\n",
              "      <td>41.972150</td>\n",
              "      <td>-29.416862</td>\n",
              "      <td>18.537344</td>\n",
              "      <td>-4.156565</td>\n",
              "      <td>5.257353</td>\n",
              "      <td>-11.410935</td>\n",
              "      <td>-8.983023</td>\n",
              "      <td>-11.285996</td>\n",
              "      <td>...</td>\n",
              "      <td>0.600825</td>\n",
              "      <td>0.648380</td>\n",
              "      <td>0.675295</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>-0.010576</td>\n",
              "      <td>-0.000228</td>\n",
              "      <td>0.011102</td>\n",
              "      <td>-0.074188</td>\n",
              "      <td>0.012116</td>\n",
              "      <td>-0.000779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>-447.12630</td>\n",
              "      <td>86.114920</td>\n",
              "      <td>4.772520</td>\n",
              "      <td>38.097256</td>\n",
              "      <td>8.324276</td>\n",
              "      <td>8.538317</td>\n",
              "      <td>-4.507682</td>\n",
              "      <td>-7.680664</td>\n",
              "      <td>-7.317249</td>\n",
              "      <td>...</td>\n",
              "      <td>0.698287</td>\n",
              "      <td>0.615263</td>\n",
              "      <td>0.566964</td>\n",
              "      <td>0.605103</td>\n",
              "      <td>-0.008389</td>\n",
              "      <td>-0.051995</td>\n",
              "      <td>-0.056544</td>\n",
              "      <td>-0.020014</td>\n",
              "      <td>0.013964</td>\n",
              "      <td>0.008349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>-451.45264</td>\n",
              "      <td>71.335200</td>\n",
              "      <td>12.223010</td>\n",
              "      <td>32.649555</td>\n",
              "      <td>3.997801</td>\n",
              "      <td>15.200773</td>\n",
              "      <td>-2.812883</td>\n",
              "      <td>-1.384373</td>\n",
              "      <td>-6.467040</td>\n",
              "      <td>...</td>\n",
              "      <td>0.696758</td>\n",
              "      <td>0.719856</td>\n",
              "      <td>0.667424</td>\n",
              "      <td>0.570905</td>\n",
              "      <td>0.003044</td>\n",
              "      <td>-0.006101</td>\n",
              "      <td>-0.009038</td>\n",
              "      <td>-0.049699</td>\n",
              "      <td>0.027615</td>\n",
              "      <td>0.021319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>423</th>\n",
              "      <td>423</td>\n",
              "      <td>-429.41583</td>\n",
              "      <td>51.232320</td>\n",
              "      <td>-4.127894</td>\n",
              "      <td>38.715740</td>\n",
              "      <td>2.253632</td>\n",
              "      <td>-2.197497</td>\n",
              "      <td>-15.981533</td>\n",
              "      <td>-14.225066</td>\n",
              "      <td>-17.897070</td>\n",
              "      <td>...</td>\n",
              "      <td>0.556236</td>\n",
              "      <td>0.589742</td>\n",
              "      <td>0.609879</td>\n",
              "      <td>0.515997</td>\n",
              "      <td>-0.019026</td>\n",
              "      <td>0.000340</td>\n",
              "      <td>-0.022717</td>\n",
              "      <td>-0.020690</td>\n",
              "      <td>-0.002970</td>\n",
              "      <td>-0.001101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424</th>\n",
              "      <td>424</td>\n",
              "      <td>-478.05527</td>\n",
              "      <td>5.705422</td>\n",
              "      <td>-24.631056</td>\n",
              "      <td>27.622614</td>\n",
              "      <td>-19.235449</td>\n",
              "      <td>-7.040019</td>\n",
              "      <td>-13.293165</td>\n",
              "      <td>-9.629133</td>\n",
              "      <td>-14.067036</td>\n",
              "      <td>...</td>\n",
              "      <td>0.597085</td>\n",
              "      <td>0.590422</td>\n",
              "      <td>0.481957</td>\n",
              "      <td>0.481437</td>\n",
              "      <td>-0.011216</td>\n",
              "      <td>-0.031485</td>\n",
              "      <td>-0.029936</td>\n",
              "      <td>0.042160</td>\n",
              "      <td>-0.013815</td>\n",
              "      <td>0.009839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>425</th>\n",
              "      <td>425</td>\n",
              "      <td>-418.62490</td>\n",
              "      <td>57.015880</td>\n",
              "      <td>6.383415</td>\n",
              "      <td>47.618423</td>\n",
              "      <td>-8.051490</td>\n",
              "      <td>4.213936</td>\n",
              "      <td>-15.029120</td>\n",
              "      <td>-2.448467</td>\n",
              "      <td>-10.805821</td>\n",
              "      <td>...</td>\n",
              "      <td>0.566316</td>\n",
              "      <td>0.569263</td>\n",
              "      <td>0.551809</td>\n",
              "      <td>0.576764</td>\n",
              "      <td>0.016046</td>\n",
              "      <td>-0.025610</td>\n",
              "      <td>0.025426</td>\n",
              "      <td>-0.094371</td>\n",
              "      <td>0.008999</td>\n",
              "      <td>-0.004985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>426</th>\n",
              "      <td>426</td>\n",
              "      <td>-467.15588</td>\n",
              "      <td>52.217026</td>\n",
              "      <td>10.470471</td>\n",
              "      <td>47.414780</td>\n",
              "      <td>8.690019</td>\n",
              "      <td>15.601270</td>\n",
              "      <td>-2.032935</td>\n",
              "      <td>4.985774</td>\n",
              "      <td>-7.432734</td>\n",
              "      <td>...</td>\n",
              "      <td>0.531997</td>\n",
              "      <td>0.523647</td>\n",
              "      <td>0.509723</td>\n",
              "      <td>0.498566</td>\n",
              "      <td>0.051532</td>\n",
              "      <td>0.015525</td>\n",
              "      <td>-0.006052</td>\n",
              "      <td>0.018201</td>\n",
              "      <td>0.004361</td>\n",
              "      <td>-0.021649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>427</th>\n",
              "      <td>427</td>\n",
              "      <td>-526.19570</td>\n",
              "      <td>11.317784</td>\n",
              "      <td>-18.942173</td>\n",
              "      <td>29.540787</td>\n",
              "      <td>-28.057306</td>\n",
              "      <td>1.299599</td>\n",
              "      <td>-16.251814</td>\n",
              "      <td>-7.512440</td>\n",
              "      <td>-23.191568</td>\n",
              "      <td>...</td>\n",
              "      <td>0.497285</td>\n",
              "      <td>0.528861</td>\n",
              "      <td>0.501070</td>\n",
              "      <td>0.496900</td>\n",
              "      <td>0.007856</td>\n",
              "      <td>0.005917</td>\n",
              "      <td>-0.045393</td>\n",
              "      <td>-0.004246</td>\n",
              "      <td>-0.001628</td>\n",
              "      <td>0.015051</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>428 rows × 260 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8091e06b-f3d1-442d-a086-a2e517ebeb16')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8091e06b-f3d1-442d-a086-a2e517ebeb16 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8091e06b-f3d1-442d-a086-a2e517ebeb16');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e3f73c8e-fef4-4451-a315-ad62e8f644ee\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e3f73c8e-fef4-4451-a315-ad62e8f644ee')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e3f73c8e-fef4-4451-a315-ad62e8f644ee button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "x_train"
            }
          },
          "metadata": {},
          "execution_count": 113
        }
      ],
      "source": [
        "x_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "id": "d2127f10",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2127f10",
        "outputId": "01c6b03c-6af2-419a-fb49-8594e12f1efa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      0\n",
              "1      1\n",
              "2      2\n",
              "3      1\n",
              "4      3\n",
              "      ..\n",
              "423    6\n",
              "424    0\n",
              "425    5\n",
              "426    3\n",
              "427    2\n",
              "Name: 0.1, Length: 428, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "id": "7311d746",
      "metadata": {
        "id": "7311d746"
      },
      "outputs": [],
      "source": [
        "x_test=df2.iloc[:,0:(df2.shape[1]-1)]\n",
        "y_test=df2.iloc[:,-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "id": "ae86f98c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "ae86f98c",
        "outputId": "aafad374-ea78-4ed9-de73-d42830745342"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Unnamed: 0          0           1          2          3          4  \\\n",
              "0             0 -454.89886   45.067265  -0.193278  15.111545   3.080835   \n",
              "1             1 -445.90384  103.783450   1.365116  27.908356  -4.542189   \n",
              "2             2 -432.85320   94.606660  -5.319786  25.696900   0.194088   \n",
              "3             3 -414.28708   45.442410   5.925002  21.436172   0.354569   \n",
              "4             4 -434.16900   78.900970   4.866675  45.928330   4.804515   \n",
              "..          ...        ...         ...        ...        ...        ...   \n",
              "102         102 -392.46520   73.364350  -3.410112  28.893300  -9.239878   \n",
              "103         103 -431.76166   50.142090   3.661988  45.098984 -13.927658   \n",
              "104         104 -403.50656   30.989262 -12.756512  27.214573 -10.573180   \n",
              "105         105 -427.36716   51.473750   4.835125  40.900140   6.937289   \n",
              "106         106 -437.97220    5.289146 -22.667547  25.394840 -28.545520   \n",
              "\n",
              "             5          6         7          8  ...       249       250  \\\n",
              "0     4.204241 -10.440731 -6.615343 -16.249382  ...  0.506934  0.575784   \n",
              "1    10.230102 -10.717600 -8.279192  -2.135225  ...  0.590438  0.659291   \n",
              "2     7.087550  -1.224768 -5.595039 -10.824135  ...  0.622093  0.605255   \n",
              "3     7.393221  -7.657958 -4.724375 -18.931787  ...  0.648508  0.618056   \n",
              "4     2.177262 -13.456310 -5.863212  -3.584935  ...  0.658636  0.660314   \n",
              "..         ...        ...       ...        ...  ...       ...       ...   \n",
              "102  17.295076 -14.857638 -8.589843 -15.333829  ...  0.571422  0.555782   \n",
              "103  -0.977742 -16.576310  0.029880  -0.389472  ...  0.583082  0.607035   \n",
              "104  -0.137991 -15.470394 -8.865036 -16.384440  ...  0.457819  0.531090   \n",
              "105  13.700687  -8.331753 -0.583414  -6.279562  ...  0.671834  0.638155   \n",
              "106   0.428451 -11.650926 -8.022680 -18.337156  ...  0.510255  0.495079   \n",
              "\n",
              "          251       252       253       254       255       256       257  \\\n",
              "0    0.523305  0.423375  0.009395 -0.025547 -0.035554 -0.025885  0.011198   \n",
              "1    0.604083  0.500168  0.006800 -0.019560 -0.049193 -0.089028  0.020237   \n",
              "2    0.605432  0.592474 -0.008104 -0.009647 -0.046115 -0.007378  0.006855   \n",
              "3    0.525700  0.500340 -0.018470 -0.007622 -0.006954  0.023004  0.002193   \n",
              "4    0.630964  0.564337 -0.004686  0.006942  0.026904  0.006124  0.004141   \n",
              "..        ...       ...       ...       ...       ...       ...       ...   \n",
              "102  0.517858  0.473756  0.000104 -0.007319 -0.020273  0.040238  0.012407   \n",
              "103  0.589502  0.506217  0.006985  0.040552  0.005959 -0.065906 -0.023141   \n",
              "104  0.567376  0.470224 -0.005500  0.013583  0.040782  0.003971  0.017743   \n",
              "105  0.574542  0.503516  0.019632  0.046315  0.074801  0.018078  0.022948   \n",
              "106  0.539387  0.527394 -0.001086  0.013948  0.013274 -0.056855 -0.001004   \n",
              "\n",
              "          258  \n",
              "0   -0.000163  \n",
              "1    0.023672  \n",
              "2   -0.004688  \n",
              "3   -0.005318  \n",
              "4    0.017759  \n",
              "..        ...  \n",
              "102 -0.014820  \n",
              "103 -0.005444  \n",
              "104  0.026657  \n",
              "105 -0.016365  \n",
              "106 -0.001355  \n",
              "\n",
              "[107 rows x 260 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2ea346e7-eb8d-45f3-b36e-7ad46d7bd745\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>...</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "      <th>257</th>\n",
              "      <th>258</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-454.89886</td>\n",
              "      <td>45.067265</td>\n",
              "      <td>-0.193278</td>\n",
              "      <td>15.111545</td>\n",
              "      <td>3.080835</td>\n",
              "      <td>4.204241</td>\n",
              "      <td>-10.440731</td>\n",
              "      <td>-6.615343</td>\n",
              "      <td>-16.249382</td>\n",
              "      <td>...</td>\n",
              "      <td>0.506934</td>\n",
              "      <td>0.575784</td>\n",
              "      <td>0.523305</td>\n",
              "      <td>0.423375</td>\n",
              "      <td>0.009395</td>\n",
              "      <td>-0.025547</td>\n",
              "      <td>-0.035554</td>\n",
              "      <td>-0.025885</td>\n",
              "      <td>0.011198</td>\n",
              "      <td>-0.000163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>-445.90384</td>\n",
              "      <td>103.783450</td>\n",
              "      <td>1.365116</td>\n",
              "      <td>27.908356</td>\n",
              "      <td>-4.542189</td>\n",
              "      <td>10.230102</td>\n",
              "      <td>-10.717600</td>\n",
              "      <td>-8.279192</td>\n",
              "      <td>-2.135225</td>\n",
              "      <td>...</td>\n",
              "      <td>0.590438</td>\n",
              "      <td>0.659291</td>\n",
              "      <td>0.604083</td>\n",
              "      <td>0.500168</td>\n",
              "      <td>0.006800</td>\n",
              "      <td>-0.019560</td>\n",
              "      <td>-0.049193</td>\n",
              "      <td>-0.089028</td>\n",
              "      <td>0.020237</td>\n",
              "      <td>0.023672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>-432.85320</td>\n",
              "      <td>94.606660</td>\n",
              "      <td>-5.319786</td>\n",
              "      <td>25.696900</td>\n",
              "      <td>0.194088</td>\n",
              "      <td>7.087550</td>\n",
              "      <td>-1.224768</td>\n",
              "      <td>-5.595039</td>\n",
              "      <td>-10.824135</td>\n",
              "      <td>...</td>\n",
              "      <td>0.622093</td>\n",
              "      <td>0.605255</td>\n",
              "      <td>0.605432</td>\n",
              "      <td>0.592474</td>\n",
              "      <td>-0.008104</td>\n",
              "      <td>-0.009647</td>\n",
              "      <td>-0.046115</td>\n",
              "      <td>-0.007378</td>\n",
              "      <td>0.006855</td>\n",
              "      <td>-0.004688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>-414.28708</td>\n",
              "      <td>45.442410</td>\n",
              "      <td>5.925002</td>\n",
              "      <td>21.436172</td>\n",
              "      <td>0.354569</td>\n",
              "      <td>7.393221</td>\n",
              "      <td>-7.657958</td>\n",
              "      <td>-4.724375</td>\n",
              "      <td>-18.931787</td>\n",
              "      <td>...</td>\n",
              "      <td>0.648508</td>\n",
              "      <td>0.618056</td>\n",
              "      <td>0.525700</td>\n",
              "      <td>0.500340</td>\n",
              "      <td>-0.018470</td>\n",
              "      <td>-0.007622</td>\n",
              "      <td>-0.006954</td>\n",
              "      <td>0.023004</td>\n",
              "      <td>0.002193</td>\n",
              "      <td>-0.005318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>-434.16900</td>\n",
              "      <td>78.900970</td>\n",
              "      <td>4.866675</td>\n",
              "      <td>45.928330</td>\n",
              "      <td>4.804515</td>\n",
              "      <td>2.177262</td>\n",
              "      <td>-13.456310</td>\n",
              "      <td>-5.863212</td>\n",
              "      <td>-3.584935</td>\n",
              "      <td>...</td>\n",
              "      <td>0.658636</td>\n",
              "      <td>0.660314</td>\n",
              "      <td>0.630964</td>\n",
              "      <td>0.564337</td>\n",
              "      <td>-0.004686</td>\n",
              "      <td>0.006942</td>\n",
              "      <td>0.026904</td>\n",
              "      <td>0.006124</td>\n",
              "      <td>0.004141</td>\n",
              "      <td>0.017759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>102</td>\n",
              "      <td>-392.46520</td>\n",
              "      <td>73.364350</td>\n",
              "      <td>-3.410112</td>\n",
              "      <td>28.893300</td>\n",
              "      <td>-9.239878</td>\n",
              "      <td>17.295076</td>\n",
              "      <td>-14.857638</td>\n",
              "      <td>-8.589843</td>\n",
              "      <td>-15.333829</td>\n",
              "      <td>...</td>\n",
              "      <td>0.571422</td>\n",
              "      <td>0.555782</td>\n",
              "      <td>0.517858</td>\n",
              "      <td>0.473756</td>\n",
              "      <td>0.000104</td>\n",
              "      <td>-0.007319</td>\n",
              "      <td>-0.020273</td>\n",
              "      <td>0.040238</td>\n",
              "      <td>0.012407</td>\n",
              "      <td>-0.014820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>103</td>\n",
              "      <td>-431.76166</td>\n",
              "      <td>50.142090</td>\n",
              "      <td>3.661988</td>\n",
              "      <td>45.098984</td>\n",
              "      <td>-13.927658</td>\n",
              "      <td>-0.977742</td>\n",
              "      <td>-16.576310</td>\n",
              "      <td>0.029880</td>\n",
              "      <td>-0.389472</td>\n",
              "      <td>...</td>\n",
              "      <td>0.583082</td>\n",
              "      <td>0.607035</td>\n",
              "      <td>0.589502</td>\n",
              "      <td>0.506217</td>\n",
              "      <td>0.006985</td>\n",
              "      <td>0.040552</td>\n",
              "      <td>0.005959</td>\n",
              "      <td>-0.065906</td>\n",
              "      <td>-0.023141</td>\n",
              "      <td>-0.005444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>104</td>\n",
              "      <td>-403.50656</td>\n",
              "      <td>30.989262</td>\n",
              "      <td>-12.756512</td>\n",
              "      <td>27.214573</td>\n",
              "      <td>-10.573180</td>\n",
              "      <td>-0.137991</td>\n",
              "      <td>-15.470394</td>\n",
              "      <td>-8.865036</td>\n",
              "      <td>-16.384440</td>\n",
              "      <td>...</td>\n",
              "      <td>0.457819</td>\n",
              "      <td>0.531090</td>\n",
              "      <td>0.567376</td>\n",
              "      <td>0.470224</td>\n",
              "      <td>-0.005500</td>\n",
              "      <td>0.013583</td>\n",
              "      <td>0.040782</td>\n",
              "      <td>0.003971</td>\n",
              "      <td>0.017743</td>\n",
              "      <td>0.026657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>105</td>\n",
              "      <td>-427.36716</td>\n",
              "      <td>51.473750</td>\n",
              "      <td>4.835125</td>\n",
              "      <td>40.900140</td>\n",
              "      <td>6.937289</td>\n",
              "      <td>13.700687</td>\n",
              "      <td>-8.331753</td>\n",
              "      <td>-0.583414</td>\n",
              "      <td>-6.279562</td>\n",
              "      <td>...</td>\n",
              "      <td>0.671834</td>\n",
              "      <td>0.638155</td>\n",
              "      <td>0.574542</td>\n",
              "      <td>0.503516</td>\n",
              "      <td>0.019632</td>\n",
              "      <td>0.046315</td>\n",
              "      <td>0.074801</td>\n",
              "      <td>0.018078</td>\n",
              "      <td>0.022948</td>\n",
              "      <td>-0.016365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>106</td>\n",
              "      <td>-437.97220</td>\n",
              "      <td>5.289146</td>\n",
              "      <td>-22.667547</td>\n",
              "      <td>25.394840</td>\n",
              "      <td>-28.545520</td>\n",
              "      <td>0.428451</td>\n",
              "      <td>-11.650926</td>\n",
              "      <td>-8.022680</td>\n",
              "      <td>-18.337156</td>\n",
              "      <td>...</td>\n",
              "      <td>0.510255</td>\n",
              "      <td>0.495079</td>\n",
              "      <td>0.539387</td>\n",
              "      <td>0.527394</td>\n",
              "      <td>-0.001086</td>\n",
              "      <td>0.013948</td>\n",
              "      <td>0.013274</td>\n",
              "      <td>-0.056855</td>\n",
              "      <td>-0.001004</td>\n",
              "      <td>-0.001355</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>107 rows × 260 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2ea346e7-eb8d-45f3-b36e-7ad46d7bd745')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2ea346e7-eb8d-45f3-b36e-7ad46d7bd745 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2ea346e7-eb8d-45f3-b36e-7ad46d7bd745');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4deb161e-fe4e-4f3e-af1f-1b1fc9f7b524\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4deb161e-fe4e-4f3e-af1f-1b1fc9f7b524')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4deb161e-fe4e-4f3e-af1f-1b1fc9f7b524 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "x_test"
            }
          },
          "metadata": {},
          "execution_count": 116
        }
      ],
      "source": [
        "x_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "id": "62579094",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62579094",
        "outputId": "bdc0939e-1ba8-42ac-a0ed-6c14a97e08a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      0\n",
              "1      5\n",
              "2      1\n",
              "3      0\n",
              "4      5\n",
              "      ..\n",
              "102    1\n",
              "103    5\n",
              "104    4\n",
              "105    3\n",
              "106    2\n",
              "Name: 0.1, Length: 107, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "id": "3beb4ef2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "3beb4ef2",
        "outputId": "8a857627-c595-48fd-a783-4286b396da14"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: ylabel='count'>"
            ]
          },
          "metadata": {},
          "execution_count": 118
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "sns.countplot(df1['0'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "id": "d58a5824",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "d58a5824",
        "outputId": "ce2c34d0-ecc6-4240-c2b4-70718908b7f9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: ylabel='count'>"
            ]
          },
          "metadata": {},
          "execution_count": 119
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "sns.countplot(df2['0'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "id": "f4bf0f42",
      "metadata": {
        "id": "f4bf0f42"
      },
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "id": "5bd3a370",
      "metadata": {
        "id": "5bd3a370"
      },
      "outputs": [],
      "source": [
        "# x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "id": "7b9f98bc",
      "metadata": {
        "id": "7b9f98bc"
      },
      "outputs": [],
      "source": [
        "# x_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "id": "c8d669e4",
      "metadata": {
        "id": "c8d669e4"
      },
      "outputs": [],
      "source": [
        "# y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "id": "3e5f00f5",
      "metadata": {
        "id": "3e5f00f5"
      },
      "outputs": [],
      "source": [
        "# x_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "id": "6d201bc1",
      "metadata": {
        "id": "6d201bc1"
      },
      "outputs": [],
      "source": [
        "# y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "id": "000082a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "000082a7",
        "outputId": "924d2c26-c878-42b9-ba80-c7ce71e1ebc3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 5, 1, 2, 3, 4, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ],
      "source": [
        "#Check unique values for y_test\n",
        "y_test.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "id": "0c50dd63",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c50dd63",
        "outputId": "3a45a41b-733e-4b65-9a33-271e27222c7c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ],
      "source": [
        "#Check unique values for y_train\n",
        "y_train.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "id": "2f69e286",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f69e286",
        "outputId": "92e70351-2e9d-4ef2-b42e-5cc925459130"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 1.0726817042606516,\n",
              " 1: 0.9705215419501134,\n",
              " 2: 0.6053748231966054,\n",
              " 3: 1.2228571428571429,\n",
              " 4: 1.1116883116883116,\n",
              " 5: 0.9406593406593406,\n",
              " 6: 1.6525096525096525}"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ],
      "source": [
        "from sklearn.utils import compute_class_weight\n",
        "class_weights = compute_class_weight(\n",
        "                                        class_weight = \"balanced\",\n",
        "                                        classes = np.unique(y_train),\n",
        "                                        y = y_train\n",
        "                                    )\n",
        "class_weights = dict(zip(np.unique(y_train), class_weights))\n",
        "class_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "id": "87b553a9",
      "metadata": {
        "id": "87b553a9"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "#Normalize the data\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(x_train)\n",
        "X_train_scalled = scaler.transform(x_train)\n",
        "X_test_scalled = scaler.transform(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "id": "7b62969d",
      "metadata": {
        "id": "7b62969d"
      },
      "outputs": [],
      "source": [
        "#Import packages for CNN\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, Conv1D\n",
        "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, BatchNormalization, Flatten, MaxPooling1D\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.regularizers import l2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "id": "c598744f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c598744f",
        "outputId": "4352e1d1-8c24-4ae0-e342-cc8f48b05010"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[700, 662, 972, 921]\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "res = []\n",
        "for j in range(4):\n",
        "    res.append(random.randint(300, 1000))\n",
        "# res.sort(reverse=True)\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(260, 1)))\n",
        "model.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='sigmoid'))\n",
        "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# model.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='sigmoid'))\n",
        "# model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Dropout(0.2))\n",
        "\n",
        "# model.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='sigmoid'))\n",
        "# model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Dropout(0.3))\n",
        "##\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "model.add(LSTM(256))\n",
        "\n",
        "\n",
        "model.add(Dense(256, activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(256, activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "##\n",
        "# model.add(Dense(64, activation='sigmoid'))\n",
        "# # model.add(BatchNormalization())\n",
        "# model.add(Dropout(0.3))\n",
        "\n",
        "# model.add(Dense(32, activation='sigmoid'))\n",
        "# # model.add(BatchNormalization())\n",
        "# model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(7, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Q6P4AQ4Mrdt",
        "outputId": "30df7dcf-d44e-4f32-d256-a0fe07136ec7"
      },
      "id": "_Q6P4AQ4Mrdt",
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_8 (Conv1D)           (None, 260, 256)          1536      \n",
            "                                                                 \n",
            " max_pooling1d_8 (MaxPoolin  (None, 130, 256)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " batch_normalization_8 (Bat  (None, 130, 256)          1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv1d_9 (Conv1D)           (None, 130, 256)          327936    \n",
            "                                                                 \n",
            " max_pooling1d_9 (MaxPoolin  (None, 65, 256)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " batch_normalization_9 (Bat  (None, 65, 256)           1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 65, 256)           0         \n",
            "                                                                 \n",
            " conv1d_10 (Conv1D)          (None, 65, 256)           327936    \n",
            "                                                                 \n",
            " max_pooling1d_10 (MaxPooli  (None, 33, 256)           0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_10 (Ba  (None, 33, 256)           1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 33, 256)           0         \n",
            "                                                                 \n",
            " conv1d_11 (Conv1D)          (None, 33, 128)           163968    \n",
            "                                                                 \n",
            " max_pooling1d_11 (MaxPooli  (None, 17, 128)           0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_11 (Ba  (None, 17, 128)           512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 17, 128)           0         \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 17, 256)           394240    \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 256)               525312    \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 7)                 1799      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1877895 (7.16 MB)\n",
            "Trainable params: 1876103 (7.16 MB)\n",
            "Non-trainable params: 1792 (7.00 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tempfile\n",
        "initial_weights = os.path.join(tempfile.mkdtemp(), 'initial_weights')\n",
        "model.save_weights(initial_weights)"
      ],
      "metadata": {
        "id": "JCEBlr_YMzuM"
      },
      "id": "JCEBlr_YMzuM",
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(initial_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8l3JMDetM3-D",
        "outputId": "411e10d2-bd7e-4a17-afcd-b8f4663374c0"
      },
      "id": "8l3JMDetM3-D",
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7a5313ef79d0>"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimiser = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=optimiser,\n",
        "              loss='sparse_categorical_crossentropy',                             #CategoricalCrossentropy\n",
        "              metrics=['SparseCategoricalAccuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WglAV3TMM7KM",
        "outputId": "e13838d2-2a6d-47ae-cea0-19d3bfdfc92b"
      },
      "id": "WglAV3TMM7KM",
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_8 (Conv1D)           (None, 260, 256)          1536      \n",
            "                                                                 \n",
            " max_pooling1d_8 (MaxPoolin  (None, 130, 256)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " batch_normalization_8 (Bat  (None, 130, 256)          1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv1d_9 (Conv1D)           (None, 130, 256)          327936    \n",
            "                                                                 \n",
            " max_pooling1d_9 (MaxPoolin  (None, 65, 256)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " batch_normalization_9 (Bat  (None, 65, 256)           1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 65, 256)           0         \n",
            "                                                                 \n",
            " conv1d_10 (Conv1D)          (None, 65, 256)           327936    \n",
            "                                                                 \n",
            " max_pooling1d_10 (MaxPooli  (None, 33, 256)           0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_10 (Ba  (None, 33, 256)           1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 33, 256)           0         \n",
            "                                                                 \n",
            " conv1d_11 (Conv1D)          (None, 33, 128)           163968    \n",
            "                                                                 \n",
            " max_pooling1d_11 (MaxPooli  (None, 17, 128)           0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_11 (Ba  (None, 17, 128)           512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 17, 128)           0         \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 17, 256)           394240    \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 256)               525312    \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 7)                 1799      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1877895 (7.16 MB)\n",
            "Trainable params: 1876103 (7.16 MB)\n",
            "Non-trainable params: 1792 (7.00 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path='cnn_lstm_emodb3.ckpt'\n",
        "checkpoint_dir=os.path.dirname(checkpoint_path)\n",
        "callback1=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, monitor='val_sparse_categorical_accuracy', verbose=1,\n",
        "   save_best_only=True,save_weights_only=True,)\n",
        "callback2=tf.keras.callbacks.EarlyStopping(monitor='val_sparse_categorical_accuracy',min_delta=0, patience=50, verbose=0, mode='auto',baseline=None,restore_best_weights=True)\n",
        "cp_callback=[callback1,callback2]"
      ],
      "metadata": {
        "id": "Kg0_S4kRM8DV"
      },
      "id": "Kg0_S4kRM8DV",
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train_scalled, y_train, validation_data=(X_test_scalled, y_test), batch_size=64, epochs=900, verbose=1,class_weight=class_weights,callbacks=cp_callback)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giDUxkTnNGXy",
        "outputId": "0d94911d-3c4b-4981-c181-b1c548e3e8af"
      },
      "id": "giDUxkTnNGXy",
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/900\n",
            "5/7 [====================>.........] - ETA: 0s - loss: 1.9639 - sparse_categorical_accuracy: 0.2031\n",
            "Epoch 1: val_sparse_categorical_accuracy improved from -inf to 0.14953, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 8s 203ms/step - loss: 1.9296 - sparse_categorical_accuracy: 0.1986 - val_loss: 1.9446 - val_sparse_categorical_accuracy: 0.1495\n",
            "Epoch 2/900\n",
            "5/7 [====================>.........] - ETA: 0s - loss: 1.9095 - sparse_categorical_accuracy: 0.2281\n",
            "Epoch 2: val_sparse_categorical_accuracy did not improve from 0.14953\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 1.8775 - sparse_categorical_accuracy: 0.2313 - val_loss: 1.9457 - val_sparse_categorical_accuracy: 0.1495\n",
            "Epoch 3/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 1.8393 - sparse_categorical_accuracy: 0.2630\n",
            "Epoch 3: val_sparse_categorical_accuracy did not improve from 0.14953\n",
            "7/7 [==============================] - 0s 30ms/step - loss: 1.8384 - sparse_categorical_accuracy: 0.2757 - val_loss: 1.9470 - val_sparse_categorical_accuracy: 0.1495\n",
            "Epoch 4/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.7983 - sparse_categorical_accuracy: 0.2710\n",
            "Epoch 4: val_sparse_categorical_accuracy did not improve from 0.14953\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 1.7983 - sparse_categorical_accuracy: 0.2710 - val_loss: 1.9484 - val_sparse_categorical_accuracy: 0.1495\n",
            "Epoch 5/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.7692 - sparse_categorical_accuracy: 0.2897\n",
            "Epoch 5: val_sparse_categorical_accuracy did not improve from 0.14953\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 1.7692 - sparse_categorical_accuracy: 0.2897 - val_loss: 1.9499 - val_sparse_categorical_accuracy: 0.1495\n",
            "Epoch 6/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.7315 - sparse_categorical_accuracy: 0.3271\n",
            "Epoch 6: val_sparse_categorical_accuracy did not improve from 0.14953\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 1.7315 - sparse_categorical_accuracy: 0.3271 - val_loss: 1.9523 - val_sparse_categorical_accuracy: 0.1495\n",
            "Epoch 7/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.6962 - sparse_categorical_accuracy: 0.3692\n",
            "Epoch 7: val_sparse_categorical_accuracy did not improve from 0.14953\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 1.6962 - sparse_categorical_accuracy: 0.3692 - val_loss: 1.9563 - val_sparse_categorical_accuracy: 0.1495\n",
            "Epoch 8/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.6350 - sparse_categorical_accuracy: 0.3551\n",
            "Epoch 8: val_sparse_categorical_accuracy did not improve from 0.14953\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 1.6350 - sparse_categorical_accuracy: 0.3551 - val_loss: 1.9619 - val_sparse_categorical_accuracy: 0.1495\n",
            "Epoch 9/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.5661 - sparse_categorical_accuracy: 0.4252\n",
            "Epoch 9: val_sparse_categorical_accuracy did not improve from 0.14953\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 1.5661 - sparse_categorical_accuracy: 0.4252 - val_loss: 1.9729 - val_sparse_categorical_accuracy: 0.1495\n",
            "Epoch 10/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.4706 - sparse_categorical_accuracy: 0.4533\n",
            "Epoch 10: val_sparse_categorical_accuracy did not improve from 0.14953\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 1.4706 - sparse_categorical_accuracy: 0.4533 - val_loss: 1.9903 - val_sparse_categorical_accuracy: 0.1495\n",
            "Epoch 11/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.3592 - sparse_categorical_accuracy: 0.4860\n",
            "Epoch 11: val_sparse_categorical_accuracy did not improve from 0.14953\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 1.3592 - sparse_categorical_accuracy: 0.4860 - val_loss: 2.0174 - val_sparse_categorical_accuracy: 0.1495\n",
            "Epoch 12/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.2335 - sparse_categorical_accuracy: 0.5607\n",
            "Epoch 12: val_sparse_categorical_accuracy did not improve from 0.14953\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 1.2335 - sparse_categorical_accuracy: 0.5607 - val_loss: 2.0612 - val_sparse_categorical_accuracy: 0.1495\n",
            "Epoch 13/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.1032 - sparse_categorical_accuracy: 0.6028\n",
            "Epoch 13: val_sparse_categorical_accuracy did not improve from 0.14953\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 1.1032 - sparse_categorical_accuracy: 0.6028 - val_loss: 2.0442 - val_sparse_categorical_accuracy: 0.1495\n",
            "Epoch 14/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.0494 - sparse_categorical_accuracy: 0.6005\n",
            "Epoch 14: val_sparse_categorical_accuracy did not improve from 0.14953\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 1.0494 - sparse_categorical_accuracy: 0.6005 - val_loss: 2.0355 - val_sparse_categorical_accuracy: 0.1495\n",
            "Epoch 15/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.8973 - sparse_categorical_accuracy: 0.6262\n",
            "Epoch 15: val_sparse_categorical_accuracy improved from 0.14953 to 0.16822, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 51ms/step - loss: 0.8973 - sparse_categorical_accuracy: 0.6262 - val_loss: 1.9625 - val_sparse_categorical_accuracy: 0.1682\n",
            "Epoch 16/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.8177 - sparse_categorical_accuracy: 0.6706\n",
            "Epoch 16: val_sparse_categorical_accuracy improved from 0.16822 to 0.25234, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 51ms/step - loss: 0.8177 - sparse_categorical_accuracy: 0.6706 - val_loss: 1.8609 - val_sparse_categorical_accuracy: 0.2523\n",
            "Epoch 17/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.7511 - sparse_categorical_accuracy: 0.6893\n",
            "Epoch 17: val_sparse_categorical_accuracy did not improve from 0.25234\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.7511 - sparse_categorical_accuracy: 0.6893 - val_loss: 1.9930 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 18/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.6966 - sparse_categorical_accuracy: 0.6986\n",
            "Epoch 18: val_sparse_categorical_accuracy did not improve from 0.25234\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.6966 - sparse_categorical_accuracy: 0.6986 - val_loss: 2.5696 - val_sparse_categorical_accuracy: 0.2336\n",
            "Epoch 19/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.6482 - sparse_categorical_accuracy: 0.7173\n",
            "Epoch 19: val_sparse_categorical_accuracy did not improve from 0.25234\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.6482 - sparse_categorical_accuracy: 0.7173 - val_loss: 2.5537 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 20/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.6650 - sparse_categorical_accuracy: 0.6963\n",
            "Epoch 20: val_sparse_categorical_accuracy did not improve from 0.25234\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.6650 - sparse_categorical_accuracy: 0.6963 - val_loss: 2.7454 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 21/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.5418 - sparse_categorical_accuracy: 0.7804\n",
            "Epoch 21: val_sparse_categorical_accuracy did not improve from 0.25234\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.5418 - sparse_categorical_accuracy: 0.7804 - val_loss: 3.1537 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 22/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.4986 - sparse_categorical_accuracy: 0.7944\n",
            "Epoch 22: val_sparse_categorical_accuracy improved from 0.25234 to 0.26168, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.4986 - sparse_categorical_accuracy: 0.7944 - val_loss: 3.5855 - val_sparse_categorical_accuracy: 0.2617\n",
            "Epoch 23/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.4765 - sparse_categorical_accuracy: 0.8014\n",
            "Epoch 23: val_sparse_categorical_accuracy did not improve from 0.26168\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 0.4765 - sparse_categorical_accuracy: 0.8014 - val_loss: 3.6978 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 24/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.4016 - sparse_categorical_accuracy: 0.8505\n",
            "Epoch 24: val_sparse_categorical_accuracy did not improve from 0.26168\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 0.4016 - sparse_categorical_accuracy: 0.8505 - val_loss: 3.7362 - val_sparse_categorical_accuracy: 0.1589\n",
            "Epoch 25/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.4013 - sparse_categorical_accuracy: 0.8131\n",
            "Epoch 25: val_sparse_categorical_accuracy did not improve from 0.26168\n",
            "7/7 [==============================] - 0s 30ms/step - loss: 0.4013 - sparse_categorical_accuracy: 0.8131 - val_loss: 3.8559 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 26/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.3868 - sparse_categorical_accuracy: 0.8178\n",
            "Epoch 26: val_sparse_categorical_accuracy did not improve from 0.26168\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 0.3868 - sparse_categorical_accuracy: 0.8178 - val_loss: 4.1562 - val_sparse_categorical_accuracy: 0.1963\n",
            "Epoch 27/900\n",
            "5/7 [====================>.........] - ETA: 0s - loss: 0.3354 - sparse_categorical_accuracy: 0.8625\n",
            "Epoch 27: val_sparse_categorical_accuracy did not improve from 0.26168\n",
            "7/7 [==============================] - 0s 34ms/step - loss: 0.3397 - sparse_categorical_accuracy: 0.8598 - val_loss: 4.0273 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 28/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.3267 - sparse_categorical_accuracy: 0.8388\n",
            "Epoch 28: val_sparse_categorical_accuracy did not improve from 0.26168\n",
            "7/7 [==============================] - 0s 30ms/step - loss: 0.3267 - sparse_categorical_accuracy: 0.8388 - val_loss: 4.1445 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 29/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.2572 - sparse_categorical_accuracy: 0.9019\n",
            "Epoch 29: val_sparse_categorical_accuracy did not improve from 0.26168\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 0.2572 - sparse_categorical_accuracy: 0.9019 - val_loss: 4.3401 - val_sparse_categorical_accuracy: 0.1402\n",
            "Epoch 30/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.2648 - sparse_categorical_accuracy: 0.9065\n",
            "Epoch 30: val_sparse_categorical_accuracy did not improve from 0.26168\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 0.2648 - sparse_categorical_accuracy: 0.9065 - val_loss: 4.3593 - val_sparse_categorical_accuracy: 0.2617\n",
            "Epoch 31/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.2438 - sparse_categorical_accuracy: 0.8949\n",
            "Epoch 31: val_sparse_categorical_accuracy improved from 0.26168 to 0.27103, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 73ms/step - loss: 0.2438 - sparse_categorical_accuracy: 0.8949 - val_loss: 4.2970 - val_sparse_categorical_accuracy: 0.2710\n",
            "Epoch 32/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.1951 - sparse_categorical_accuracy: 0.9141\n",
            "Epoch 32: val_sparse_categorical_accuracy did not improve from 0.27103\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.1880 - sparse_categorical_accuracy: 0.9182 - val_loss: 4.4436 - val_sparse_categorical_accuracy: 0.1402\n",
            "Epoch 33/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.2205 - sparse_categorical_accuracy: 0.9159\n",
            "Epoch 33: val_sparse_categorical_accuracy did not improve from 0.27103\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 0.2205 - sparse_categorical_accuracy: 0.9159 - val_loss: 4.4748 - val_sparse_categorical_accuracy: 0.1589\n",
            "Epoch 34/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.2335 - sparse_categorical_accuracy: 0.9062\n",
            "Epoch 34: val_sparse_categorical_accuracy did not improve from 0.27103\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.2248 - sparse_categorical_accuracy: 0.9112 - val_loss: 4.5677 - val_sparse_categorical_accuracy: 0.1495\n",
            "Epoch 35/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1627 - sparse_categorical_accuracy: 0.9556\n",
            "Epoch 35: val_sparse_categorical_accuracy did not improve from 0.27103\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.1627 - sparse_categorical_accuracy: 0.9556 - val_loss: 4.4905 - val_sparse_categorical_accuracy: 0.1963\n",
            "Epoch 36/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1487 - sparse_categorical_accuracy: 0.9579\n",
            "Epoch 36: val_sparse_categorical_accuracy did not improve from 0.27103\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.1487 - sparse_categorical_accuracy: 0.9579 - val_loss: 4.5720 - val_sparse_categorical_accuracy: 0.1776\n",
            "Epoch 37/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1495 - sparse_categorical_accuracy: 0.9509\n",
            "Epoch 37: val_sparse_categorical_accuracy did not improve from 0.27103\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.1495 - sparse_categorical_accuracy: 0.9509 - val_loss: 4.7131 - val_sparse_categorical_accuracy: 0.1682\n",
            "Epoch 38/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1518 - sparse_categorical_accuracy: 0.9393\n",
            "Epoch 38: val_sparse_categorical_accuracy did not improve from 0.27103\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.1518 - sparse_categorical_accuracy: 0.9393 - val_loss: 5.0171 - val_sparse_categorical_accuracy: 0.1776\n",
            "Epoch 39/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1384 - sparse_categorical_accuracy: 0.9486\n",
            "Epoch 39: val_sparse_categorical_accuracy did not improve from 0.27103\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.1384 - sparse_categorical_accuracy: 0.9486 - val_loss: 4.8873 - val_sparse_categorical_accuracy: 0.1402\n",
            "Epoch 40/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1252 - sparse_categorical_accuracy: 0.9486\n",
            "Epoch 40: val_sparse_categorical_accuracy did not improve from 0.27103\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.1252 - sparse_categorical_accuracy: 0.9486 - val_loss: 5.2901 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 41/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1162 - sparse_categorical_accuracy: 0.9533\n",
            "Epoch 41: val_sparse_categorical_accuracy improved from 0.27103 to 0.30841, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 51ms/step - loss: 0.1162 - sparse_categorical_accuracy: 0.9533 - val_loss: 4.8519 - val_sparse_categorical_accuracy: 0.3084\n",
            "Epoch 42/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0848 - sparse_categorical_accuracy: 0.9790\n",
            "Epoch 42: val_sparse_categorical_accuracy improved from 0.30841 to 0.31776, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 54ms/step - loss: 0.0848 - sparse_categorical_accuracy: 0.9790 - val_loss: 4.8545 - val_sparse_categorical_accuracy: 0.3178\n",
            "Epoch 43/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1329 - sparse_categorical_accuracy: 0.9556\n",
            "Epoch 43: val_sparse_categorical_accuracy did not improve from 0.31776\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.1329 - sparse_categorical_accuracy: 0.9556 - val_loss: 5.3069 - val_sparse_categorical_accuracy: 0.1495\n",
            "Epoch 44/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.1391 - sparse_categorical_accuracy: 0.9427\n",
            "Epoch 44: val_sparse_categorical_accuracy did not improve from 0.31776\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.1425 - sparse_categorical_accuracy: 0.9369 - val_loss: 5.1691 - val_sparse_categorical_accuracy: 0.1869\n",
            "Epoch 45/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1013 - sparse_categorical_accuracy: 0.9673\n",
            "Epoch 45: val_sparse_categorical_accuracy did not improve from 0.31776\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.1013 - sparse_categorical_accuracy: 0.9673 - val_loss: 6.1226 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 46/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1296 - sparse_categorical_accuracy: 0.9439\n",
            "Epoch 46: val_sparse_categorical_accuracy did not improve from 0.31776\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.1296 - sparse_categorical_accuracy: 0.9439 - val_loss: 4.7946 - val_sparse_categorical_accuracy: 0.3084\n",
            "Epoch 47/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1145 - sparse_categorical_accuracy: 0.9650\n",
            "Epoch 47: val_sparse_categorical_accuracy improved from 0.31776 to 0.33645, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 53ms/step - loss: 0.1145 - sparse_categorical_accuracy: 0.9650 - val_loss: 4.9245 - val_sparse_categorical_accuracy: 0.3364\n",
            "Epoch 48/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0605 - sparse_categorical_accuracy: 0.9836\n",
            "Epoch 48: val_sparse_categorical_accuracy did not improve from 0.33645\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0605 - sparse_categorical_accuracy: 0.9836 - val_loss: 5.2576 - val_sparse_categorical_accuracy: 0.2710\n",
            "Epoch 49/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1072 - sparse_categorical_accuracy: 0.9603\n",
            "Epoch 49: val_sparse_categorical_accuracy did not improve from 0.33645\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.1072 - sparse_categorical_accuracy: 0.9603 - val_loss: 5.8402 - val_sparse_categorical_accuracy: 0.2523\n",
            "Epoch 50/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0640 - sparse_categorical_accuracy: 0.9790\n",
            "Epoch 50: val_sparse_categorical_accuracy did not improve from 0.33645\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0640 - sparse_categorical_accuracy: 0.9790 - val_loss: 5.2581 - val_sparse_categorical_accuracy: 0.2617\n",
            "Epoch 51/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0929 - sparse_categorical_accuracy: 0.9790\n",
            "Epoch 51: val_sparse_categorical_accuracy did not improve from 0.33645\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0929 - sparse_categorical_accuracy: 0.9790 - val_loss: 6.1475 - val_sparse_categorical_accuracy: 0.1682\n",
            "Epoch 52/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0655 - sparse_categorical_accuracy: 0.9790\n",
            "Epoch 52: val_sparse_categorical_accuracy did not improve from 0.33645\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0655 - sparse_categorical_accuracy: 0.9790 - val_loss: 5.7242 - val_sparse_categorical_accuracy: 0.1963\n",
            "Epoch 53/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0621 - sparse_categorical_accuracy: 0.9696\n",
            "Epoch 53: val_sparse_categorical_accuracy did not improve from 0.33645\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0621 - sparse_categorical_accuracy: 0.9696 - val_loss: 5.2148 - val_sparse_categorical_accuracy: 0.2430\n",
            "Epoch 54/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0821 - sparse_categorical_accuracy: 0.9743\n",
            "Epoch 54: val_sparse_categorical_accuracy did not improve from 0.33645\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0821 - sparse_categorical_accuracy: 0.9743 - val_loss: 5.4692 - val_sparse_categorical_accuracy: 0.2430\n",
            "Epoch 55/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0784 - sparse_categorical_accuracy: 0.9790\n",
            "Epoch 55: val_sparse_categorical_accuracy improved from 0.33645 to 0.34579, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 50ms/step - loss: 0.0784 - sparse_categorical_accuracy: 0.9790 - val_loss: 4.9392 - val_sparse_categorical_accuracy: 0.3458\n",
            "Epoch 56/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0664 - sparse_categorical_accuracy: 0.9766\n",
            "Epoch 56: val_sparse_categorical_accuracy did not improve from 0.34579\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0664 - sparse_categorical_accuracy: 0.9766 - val_loss: 5.0652 - val_sparse_categorical_accuracy: 0.3271\n",
            "Epoch 57/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0548 - sparse_categorical_accuracy: 0.9813\n",
            "Epoch 57: val_sparse_categorical_accuracy did not improve from 0.34579\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0548 - sparse_categorical_accuracy: 0.9813 - val_loss: 4.8527 - val_sparse_categorical_accuracy: 0.3458\n",
            "Epoch 58/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0473 - sparse_categorical_accuracy: 0.9836\n",
            "Epoch 58: val_sparse_categorical_accuracy did not improve from 0.34579\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0473 - sparse_categorical_accuracy: 0.9836 - val_loss: 5.0962 - val_sparse_categorical_accuracy: 0.2804\n",
            "Epoch 59/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0525 - sparse_categorical_accuracy: 0.9813\n",
            "Epoch 59: val_sparse_categorical_accuracy did not improve from 0.34579\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0525 - sparse_categorical_accuracy: 0.9813 - val_loss: 5.2802 - val_sparse_categorical_accuracy: 0.2617\n",
            "Epoch 60/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0678 - sparse_categorical_accuracy: 0.9743\n",
            "Epoch 60: val_sparse_categorical_accuracy did not improve from 0.34579\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0678 - sparse_categorical_accuracy: 0.9743 - val_loss: 5.2440 - val_sparse_categorical_accuracy: 0.2897\n",
            "Epoch 61/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0664 - sparse_categorical_accuracy: 0.9790\n",
            "Epoch 61: val_sparse_categorical_accuracy improved from 0.34579 to 0.38318, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 53ms/step - loss: 0.0664 - sparse_categorical_accuracy: 0.9790 - val_loss: 4.8603 - val_sparse_categorical_accuracy: 0.3832\n",
            "Epoch 62/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0772 - sparse_categorical_accuracy: 0.9766\n",
            "Epoch 62: val_sparse_categorical_accuracy did not improve from 0.38318\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0772 - sparse_categorical_accuracy: 0.9766 - val_loss: 5.0328 - val_sparse_categorical_accuracy: 0.3271\n",
            "Epoch 63/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0953 - sparse_categorical_accuracy: 0.9720\n",
            "Epoch 63: val_sparse_categorical_accuracy did not improve from 0.38318\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0953 - sparse_categorical_accuracy: 0.9720 - val_loss: 4.7812 - val_sparse_categorical_accuracy: 0.3084\n",
            "Epoch 64/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0735 - sparse_categorical_accuracy: 0.9720\n",
            "Epoch 64: val_sparse_categorical_accuracy did not improve from 0.38318\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0735 - sparse_categorical_accuracy: 0.9720 - val_loss: 4.6169 - val_sparse_categorical_accuracy: 0.3271\n",
            "Epoch 65/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0921 - sparse_categorical_accuracy: 0.9790\n",
            "Epoch 65: val_sparse_categorical_accuracy did not improve from 0.38318\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0921 - sparse_categorical_accuracy: 0.9790 - val_loss: 4.6910 - val_sparse_categorical_accuracy: 0.3551\n",
            "Epoch 66/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0558 - sparse_categorical_accuracy: 0.9743\n",
            "Epoch 66: val_sparse_categorical_accuracy did not improve from 0.38318\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0558 - sparse_categorical_accuracy: 0.9743 - val_loss: 4.7485 - val_sparse_categorical_accuracy: 0.3551\n",
            "Epoch 67/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0667 - sparse_categorical_accuracy: 0.9743\n",
            "Epoch 67: val_sparse_categorical_accuracy did not improve from 0.38318\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0667 - sparse_categorical_accuracy: 0.9743 - val_loss: 4.6050 - val_sparse_categorical_accuracy: 0.2897\n",
            "Epoch 68/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0529 - sparse_categorical_accuracy: 0.9813\n",
            "Epoch 68: val_sparse_categorical_accuracy did not improve from 0.38318\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0529 - sparse_categorical_accuracy: 0.9813 - val_loss: 4.2862 - val_sparse_categorical_accuracy: 0.3178\n",
            "Epoch 69/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0955 - sparse_categorical_accuracy: 0.9696\n",
            "Epoch 69: val_sparse_categorical_accuracy did not improve from 0.38318\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0955 - sparse_categorical_accuracy: 0.9696 - val_loss: 4.3534 - val_sparse_categorical_accuracy: 0.3178\n",
            "Epoch 70/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0452 - sparse_categorical_accuracy: 0.9860\n",
            "Epoch 70: val_sparse_categorical_accuracy did not improve from 0.38318\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0452 - sparse_categorical_accuracy: 0.9860 - val_loss: 4.3287 - val_sparse_categorical_accuracy: 0.3645\n",
            "Epoch 71/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0697 - sparse_categorical_accuracy: 0.9743\n",
            "Epoch 71: val_sparse_categorical_accuracy did not improve from 0.38318\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0697 - sparse_categorical_accuracy: 0.9743 - val_loss: 4.0513 - val_sparse_categorical_accuracy: 0.3645\n",
            "Epoch 72/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0252 - sparse_categorical_accuracy: 0.9883\n",
            "Epoch 72: val_sparse_categorical_accuracy did not improve from 0.38318\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0252 - sparse_categorical_accuracy: 0.9883 - val_loss: 3.7991 - val_sparse_categorical_accuracy: 0.3364\n",
            "Epoch 73/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0349 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 73: val_sparse_categorical_accuracy did not improve from 0.38318\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0349 - sparse_categorical_accuracy: 0.9930 - val_loss: 4.4644 - val_sparse_categorical_accuracy: 0.2430\n",
            "Epoch 74/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0323 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 74: val_sparse_categorical_accuracy did not improve from 0.38318\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0323 - sparse_categorical_accuracy: 0.9907 - val_loss: 4.1300 - val_sparse_categorical_accuracy: 0.3271\n",
            "Epoch 75/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0148 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 75: val_sparse_categorical_accuracy did not improve from 0.38318\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0148 - sparse_categorical_accuracy: 0.9977 - val_loss: 3.8934 - val_sparse_categorical_accuracy: 0.3178\n",
            "Epoch 76/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0185 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 76: val_sparse_categorical_accuracy did not improve from 0.38318\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0185 - sparse_categorical_accuracy: 0.9953 - val_loss: 3.9158 - val_sparse_categorical_accuracy: 0.3645\n",
            "Epoch 77/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0223 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 77: val_sparse_categorical_accuracy did not improve from 0.38318\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0223 - sparse_categorical_accuracy: 0.9953 - val_loss: 3.8049 - val_sparse_categorical_accuracy: 0.3364\n",
            "Epoch 78/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0151 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 78: val_sparse_categorical_accuracy did not improve from 0.38318\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0151 - sparse_categorical_accuracy: 0.9953 - val_loss: 3.6033 - val_sparse_categorical_accuracy: 0.3832\n",
            "Epoch 79/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0216 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 79: val_sparse_categorical_accuracy improved from 0.38318 to 0.44860, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 50ms/step - loss: 0.0216 - sparse_categorical_accuracy: 0.9907 - val_loss: 3.3116 - val_sparse_categorical_accuracy: 0.4486\n",
            "Epoch 80/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0478 - sparse_categorical_accuracy: 0.9860\n",
            "Epoch 80: val_sparse_categorical_accuracy improved from 0.44860 to 0.49533, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 58ms/step - loss: 0.0478 - sparse_categorical_accuracy: 0.9860 - val_loss: 3.0051 - val_sparse_categorical_accuracy: 0.4953\n",
            "Epoch 81/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0133 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 81: val_sparse_categorical_accuracy improved from 0.49533 to 0.50467, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.0161 - sparse_categorical_accuracy: 0.9977 - val_loss: 2.9640 - val_sparse_categorical_accuracy: 0.5047\n",
            "Epoch 82/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0182 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 82: val_sparse_categorical_accuracy did not improve from 0.50467\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.0182 - sparse_categorical_accuracy: 0.9953 - val_loss: 2.9352 - val_sparse_categorical_accuracy: 0.5047\n",
            "Epoch 83/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0224 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 83: val_sparse_categorical_accuracy improved from 0.50467 to 0.51402, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 62ms/step - loss: 0.0224 - sparse_categorical_accuracy: 0.9930 - val_loss: 2.9136 - val_sparse_categorical_accuracy: 0.5140\n",
            "Epoch 84/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0407 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 84: val_sparse_categorical_accuracy did not improve from 0.51402\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 0.0407 - sparse_categorical_accuracy: 0.9930 - val_loss: 3.1243 - val_sparse_categorical_accuracy: 0.4953\n",
            "Epoch 85/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0174 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 85: val_sparse_categorical_accuracy improved from 0.51402 to 0.52336, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0174 - sparse_categorical_accuracy: 0.9907 - val_loss: 2.8753 - val_sparse_categorical_accuracy: 0.5234\n",
            "Epoch 86/900\n",
            "5/7 [====================>.........] - ETA: 0s - loss: 0.0434 - sparse_categorical_accuracy: 0.9812\n",
            "Epoch 86: val_sparse_categorical_accuracy did not improve from 0.52336\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.0335 - sparse_categorical_accuracy: 0.9860 - val_loss: 2.8228 - val_sparse_categorical_accuracy: 0.5140\n",
            "Epoch 87/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0299 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 87: val_sparse_categorical_accuracy did not improve from 0.52336\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.0299 - sparse_categorical_accuracy: 0.9907 - val_loss: 2.8019 - val_sparse_categorical_accuracy: 0.5047\n",
            "Epoch 88/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0533 - sparse_categorical_accuracy: 0.9792\n",
            "Epoch 88: val_sparse_categorical_accuracy did not improve from 0.52336\n",
            "7/7 [==============================] - 0s 34ms/step - loss: 0.0510 - sparse_categorical_accuracy: 0.9790 - val_loss: 2.6962 - val_sparse_categorical_accuracy: 0.4953\n",
            "Epoch 89/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0264 - sparse_categorical_accuracy: 0.9896\n",
            "Epoch 89: val_sparse_categorical_accuracy improved from 0.52336 to 0.53271, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0274 - sparse_categorical_accuracy: 0.9907 - val_loss: 2.4037 - val_sparse_categorical_accuracy: 0.5327\n",
            "Epoch 90/900\n",
            "5/7 [====================>.........] - ETA: 0s - loss: 0.0310 - sparse_categorical_accuracy: 0.9875\n",
            "Epoch 90: val_sparse_categorical_accuracy improved from 0.53271 to 0.60748, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 58ms/step - loss: 0.0298 - sparse_categorical_accuracy: 0.9860 - val_loss: 2.1694 - val_sparse_categorical_accuracy: 0.6075\n",
            "Epoch 91/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0172 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 91: val_sparse_categorical_accuracy improved from 0.60748 to 0.63551, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 50ms/step - loss: 0.0172 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.8412 - val_sparse_categorical_accuracy: 0.6355\n",
            "Epoch 92/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0173 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 92: val_sparse_categorical_accuracy improved from 0.63551 to 0.67290, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 52ms/step - loss: 0.0173 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.6768 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 93/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0175 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 93: val_sparse_categorical_accuracy improved from 0.67290 to 0.68224, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 52ms/step - loss: 0.0175 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.7305 - val_sparse_categorical_accuracy: 0.6822\n",
            "Epoch 94/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0228 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 94: val_sparse_categorical_accuracy did not improve from 0.68224\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0228 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.7173 - val_sparse_categorical_accuracy: 0.6822\n",
            "Epoch 95/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0320 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 95: val_sparse_categorical_accuracy did not improve from 0.68224\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0320 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.6511 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 96/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0393 - sparse_categorical_accuracy: 0.9813\n",
            "Epoch 96: val_sparse_categorical_accuracy did not improve from 0.68224\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0393 - sparse_categorical_accuracy: 0.9813 - val_loss: 1.5918 - val_sparse_categorical_accuracy: 0.6636\n",
            "Epoch 97/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0192 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 97: val_sparse_categorical_accuracy improved from 0.68224 to 0.69159, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 55ms/step - loss: 0.0192 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.5726 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 98/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0244 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 98: val_sparse_categorical_accuracy improved from 0.69159 to 0.71963, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 53ms/step - loss: 0.0244 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.5811 - val_sparse_categorical_accuracy: 0.7196\n",
            "Epoch 99/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0372 - sparse_categorical_accuracy: 0.9860\n",
            "Epoch 99: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0372 - sparse_categorical_accuracy: 0.9860 - val_loss: 1.5649 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 100/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0461 - sparse_categorical_accuracy: 0.9813\n",
            "Epoch 100: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0461 - sparse_categorical_accuracy: 0.9813 - val_loss: 1.7079 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 101/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0303 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 101: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0303 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.6900 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 102/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0324 - sparse_categorical_accuracy: 0.9860\n",
            "Epoch 102: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0324 - sparse_categorical_accuracy: 0.9860 - val_loss: 1.6932 - val_sparse_categorical_accuracy: 0.6636\n",
            "Epoch 103/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0265 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 103: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0265 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.7341 - val_sparse_categorical_accuracy: 0.6636\n",
            "Epoch 104/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0232 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 104: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0232 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.7046 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 105/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0087 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 105: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0087 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.7645 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 106/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0095 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 106: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0095 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.7538 - val_sparse_categorical_accuracy: 0.6822\n",
            "Epoch 107/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0061 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 107: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0061 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.8134 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 108/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0089 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 108: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0089 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.7735 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 109/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0058 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 109: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0058 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.7237 - val_sparse_categorical_accuracy: 0.6822\n",
            "Epoch 110/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0159 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 110: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0159 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.7384 - val_sparse_categorical_accuracy: 0.6822\n",
            "Epoch 111/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0049 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 111: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0049 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.8686 - val_sparse_categorical_accuracy: 0.6822\n",
            "Epoch 112/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0363 - sparse_categorical_accuracy: 0.9860\n",
            "Epoch 112: val_sparse_categorical_accuracy improved from 0.71963 to 0.72897, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 52ms/step - loss: 0.0363 - sparse_categorical_accuracy: 0.9860 - val_loss: 1.7828 - val_sparse_categorical_accuracy: 0.7290\n",
            "Epoch 113/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0139 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 113: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0139 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.7484 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 114/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0380 - sparse_categorical_accuracy: 0.9883\n",
            "Epoch 114: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0380 - sparse_categorical_accuracy: 0.9883 - val_loss: 1.6931 - val_sparse_categorical_accuracy: 0.7103\n",
            "Epoch 115/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0349 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 115: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0349 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.6165 - val_sparse_categorical_accuracy: 0.7103\n",
            "Epoch 116/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1246 - sparse_categorical_accuracy: 0.9673\n",
            "Epoch 116: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.1246 - sparse_categorical_accuracy: 0.9673 - val_loss: 1.7906 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 117/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0427 - sparse_categorical_accuracy: 0.9860\n",
            "Epoch 117: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0427 - sparse_categorical_accuracy: 0.9860 - val_loss: 1.7335 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 118/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0397 - sparse_categorical_accuracy: 0.9883\n",
            "Epoch 118: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0397 - sparse_categorical_accuracy: 0.9883 - val_loss: 1.7700 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 119/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0587 - sparse_categorical_accuracy: 0.9790\n",
            "Epoch 119: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0587 - sparse_categorical_accuracy: 0.9790 - val_loss: 1.6332 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 120/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0219 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 120: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0219 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.6107 - val_sparse_categorical_accuracy: 0.7103\n",
            "Epoch 121/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0434 - sparse_categorical_accuracy: 0.9836\n",
            "Epoch 121: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0434 - sparse_categorical_accuracy: 0.9836 - val_loss: 1.5995 - val_sparse_categorical_accuracy: 0.7196\n",
            "Epoch 122/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0276 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 122: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0276 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.6441 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 123/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0158 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 123: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0158 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.6576 - val_sparse_categorical_accuracy: 0.6636\n",
            "Epoch 124/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0293 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 124: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0293 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.7219 - val_sparse_categorical_accuracy: 0.6636\n",
            "Epoch 125/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0370 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 125: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0370 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.6381 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 126/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0258 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 126: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0258 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.7995 - val_sparse_categorical_accuracy: 0.7196\n",
            "Epoch 127/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0310 - sparse_categorical_accuracy: 0.9883\n",
            "Epoch 127: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0310 - sparse_categorical_accuracy: 0.9883 - val_loss: 1.6207 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 128/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0103 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 128: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0103 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.5250 - val_sparse_categorical_accuracy: 0.7103\n",
            "Epoch 129/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0252 - sparse_categorical_accuracy: 0.9883\n",
            "Epoch 129: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0252 - sparse_categorical_accuracy: 0.9883 - val_loss: 1.4296 - val_sparse_categorical_accuracy: 0.7103\n",
            "Epoch 130/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0412 - sparse_categorical_accuracy: 0.9883\n",
            "Epoch 130: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0412 - sparse_categorical_accuracy: 0.9883 - val_loss: 1.6279 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 131/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0302 - sparse_categorical_accuracy: 0.9860\n",
            "Epoch 131: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0302 - sparse_categorical_accuracy: 0.9860 - val_loss: 1.5297 - val_sparse_categorical_accuracy: 0.7103\n",
            "Epoch 132/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0208 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 132: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0208 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.5403 - val_sparse_categorical_accuracy: 0.7103\n",
            "Epoch 133/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0114 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 133: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0114 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.5200 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 134/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0168 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 134: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0168 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.5749 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 135/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0181 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 135: val_sparse_categorical_accuracy improved from 0.72897 to 0.73832, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 52ms/step - loss: 0.0181 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.4419 - val_sparse_categorical_accuracy: 0.7383\n",
            "Epoch 136/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0331 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 136: val_sparse_categorical_accuracy did not improve from 0.73832\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0331 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.3046 - val_sparse_categorical_accuracy: 0.7383\n",
            "Epoch 137/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0164 - sparse_categorical_accuracy: 0.9974\n",
            "Epoch 137: val_sparse_categorical_accuracy improved from 0.73832 to 0.74766, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0152 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.3418 - val_sparse_categorical_accuracy: 0.7477\n",
            "Epoch 138/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0076 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 138: val_sparse_categorical_accuracy did not improve from 0.74766\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.0074 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.3890 - val_sparse_categorical_accuracy: 0.7383\n",
            "Epoch 139/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0049 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 139: val_sparse_categorical_accuracy did not improve from 0.74766\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.0046 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.4529 - val_sparse_categorical_accuracy: 0.7196\n",
            "Epoch 140/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0148 - sparse_categorical_accuracy: 0.9922\n",
            "Epoch 140: val_sparse_categorical_accuracy did not improve from 0.74766\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.0134 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.4859 - val_sparse_categorical_accuracy: 0.7290\n",
            "Epoch 141/900\n",
            "5/7 [====================>.........] - ETA: 0s - loss: 0.0025 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 141: val_sparse_categorical_accuracy did not improve from 0.74766\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 0.0026 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.4607 - val_sparse_categorical_accuracy: 0.7196\n",
            "Epoch 142/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0025 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 142: val_sparse_categorical_accuracy did not improve from 0.74766\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.0025 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.4705 - val_sparse_categorical_accuracy: 0.7196\n",
            "Epoch 143/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0013 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 143: val_sparse_categorical_accuracy did not improve from 0.74766\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 0.0013 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.4930 - val_sparse_categorical_accuracy: 0.7290\n",
            "Epoch 144/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0030 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 144: val_sparse_categorical_accuracy did not improve from 0.74766\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 0.0030 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.4778 - val_sparse_categorical_accuracy: 0.7383\n",
            "Epoch 145/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0063 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 145: val_sparse_categorical_accuracy did not improve from 0.74766\n",
            "7/7 [==============================] - 0s 30ms/step - loss: 0.0063 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.6274 - val_sparse_categorical_accuracy: 0.7103\n",
            "Epoch 146/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0026 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 146: val_sparse_categorical_accuracy did not improve from 0.74766\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 0.0026 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.7589 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 147/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0112 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 147: val_sparse_categorical_accuracy did not improve from 0.74766\n",
            "7/7 [==============================] - 0s 39ms/step - loss: 0.0112 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.6351 - val_sparse_categorical_accuracy: 0.7196\n",
            "Epoch 148/900\n",
            "5/7 [====================>.........] - ETA: 0s - loss: 0.0053 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 148: val_sparse_categorical_accuracy did not improve from 0.74766\n",
            "7/7 [==============================] - 0s 34ms/step - loss: 0.0042 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.6082 - val_sparse_categorical_accuracy: 0.7196\n",
            "Epoch 149/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0038 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 149: val_sparse_categorical_accuracy did not improve from 0.74766\n",
            "7/7 [==============================] - 0s 36ms/step - loss: 0.0036 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.6133 - val_sparse_categorical_accuracy: 0.7290\n",
            "Epoch 150/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0044 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 150: val_sparse_categorical_accuracy did not improve from 0.74766\n",
            "7/7 [==============================] - 0s 35ms/step - loss: 0.0044 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.7075 - val_sparse_categorical_accuracy: 0.7103\n",
            "Epoch 151/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0263 - sparse_categorical_accuracy: 0.9896\n",
            "Epoch 151: val_sparse_categorical_accuracy did not improve from 0.74766\n",
            "7/7 [==============================] - 0s 30ms/step - loss: 0.0237 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.6960 - val_sparse_categorical_accuracy: 0.7290\n",
            "Epoch 152/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0064 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 152: val_sparse_categorical_accuracy did not improve from 0.74766\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0064 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.6036 - val_sparse_categorical_accuracy: 0.7196\n",
            "Epoch 153/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0039 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 153: val_sparse_categorical_accuracy did not improve from 0.74766\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0040 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.5672 - val_sparse_categorical_accuracy: 0.7103\n",
            "Epoch 154/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0152 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 154: val_sparse_categorical_accuracy did not improve from 0.74766\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0152 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.7327 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 155/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0058 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 155: val_sparse_categorical_accuracy did not improve from 0.74766\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0058 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.7838 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 156/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0059 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 156: val_sparse_categorical_accuracy did not improve from 0.74766\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0059 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.6484 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 157/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0035 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 157: val_sparse_categorical_accuracy did not improve from 0.74766\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0035 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.5328 - val_sparse_categorical_accuracy: 0.7290\n",
            "Epoch 158/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0081 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 158: val_sparse_categorical_accuracy did not improve from 0.74766\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0081 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.5647 - val_sparse_categorical_accuracy: 0.7103\n",
            "Epoch 159/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0031 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 159: val_sparse_categorical_accuracy did not improve from 0.74766\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0031 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.6449 - val_sparse_categorical_accuracy: 0.7290\n",
            "Epoch 160/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0230 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 160: val_sparse_categorical_accuracy did not improve from 0.74766\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0230 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.7079 - val_sparse_categorical_accuracy: 0.7290\n",
            "Epoch 161/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0154 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 161: val_sparse_categorical_accuracy did not improve from 0.74766\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0154 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.5266 - val_sparse_categorical_accuracy: 0.7383\n",
            "Epoch 162/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0063 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 162: val_sparse_categorical_accuracy did not improve from 0.74766\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0063 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.6029 - val_sparse_categorical_accuracy: 0.7103\n",
            "Epoch 163/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0074 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 163: val_sparse_categorical_accuracy did not improve from 0.74766\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0074 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.8451 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 164/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0039 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 164: val_sparse_categorical_accuracy did not improve from 0.74766\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0039 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.8959 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 165/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0267 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 165: val_sparse_categorical_accuracy did not improve from 0.74766\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0267 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.4867 - val_sparse_categorical_accuracy: 0.7290\n",
            "Epoch 166/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0086 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 166: val_sparse_categorical_accuracy did not improve from 0.74766\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0086 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.4774 - val_sparse_categorical_accuracy: 0.7103\n",
            "Epoch 167/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0072 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 167: val_sparse_categorical_accuracy did not improve from 0.74766\n",
            "7/7 [==============================] - 0s 30ms/step - loss: 0.0072 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.6027 - val_sparse_categorical_accuracy: 0.7103\n",
            "Epoch 168/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0081 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 168: val_sparse_categorical_accuracy did not improve from 0.74766\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0081 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.6522 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 169/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0073 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 169: val_sparse_categorical_accuracy did not improve from 0.74766\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0073 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.6060 - val_sparse_categorical_accuracy: 0.7290\n",
            "Epoch 170/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0133 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 170: val_sparse_categorical_accuracy did not improve from 0.74766\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0133 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.4868 - val_sparse_categorical_accuracy: 0.7383\n",
            "Epoch 171/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0080 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 171: val_sparse_categorical_accuracy did not improve from 0.74766\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0080 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.4262 - val_sparse_categorical_accuracy: 0.7383\n",
            "Epoch 172/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0096 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 172: val_sparse_categorical_accuracy did not improve from 0.74766\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0096 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.4876 - val_sparse_categorical_accuracy: 0.7383\n",
            "Epoch 173/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0055 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 173: val_sparse_categorical_accuracy did not improve from 0.74766\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0055 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.4323 - val_sparse_categorical_accuracy: 0.7383\n",
            "Epoch 174/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0016 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 174: val_sparse_categorical_accuracy improved from 0.74766 to 0.76636, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 52ms/step - loss: 0.0016 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.4061 - val_sparse_categorical_accuracy: 0.7664\n",
            "Epoch 175/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0035 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 175: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0035 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.4014 - val_sparse_categorical_accuracy: 0.7477\n",
            "Epoch 176/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0044 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 176: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0044 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.5970 - val_sparse_categorical_accuracy: 0.7103\n",
            "Epoch 177/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0041 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 177: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0041 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.6614 - val_sparse_categorical_accuracy: 0.7103\n",
            "Epoch 178/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0229 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 178: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0229 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.6500 - val_sparse_categorical_accuracy: 0.7290\n",
            "Epoch 179/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0033 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 179: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0033 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.5217 - val_sparse_categorical_accuracy: 0.7290\n",
            "Epoch 180/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0053 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 180: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0053 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.5159 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 181/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0065 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 181: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0065 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.4757 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 182/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0026 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 182: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0026 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.4318 - val_sparse_categorical_accuracy: 0.7103\n",
            "Epoch 183/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0025 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 183: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0025 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.4178 - val_sparse_categorical_accuracy: 0.7196\n",
            "Epoch 184/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0090 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 184: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0090 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.3659 - val_sparse_categorical_accuracy: 0.7290\n",
            "Epoch 185/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0107 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 185: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0107 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.6757 - val_sparse_categorical_accuracy: 0.7196\n",
            "Epoch 186/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0133 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 186: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0133 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.6535 - val_sparse_categorical_accuracy: 0.7290\n",
            "Epoch 187/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0117 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 187: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0117 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.4652 - val_sparse_categorical_accuracy: 0.7290\n",
            "Epoch 188/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0061 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 188: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0061 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.5254 - val_sparse_categorical_accuracy: 0.7477\n",
            "Epoch 189/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0120 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 189: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0120 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.4722 - val_sparse_categorical_accuracy: 0.7477\n",
            "Epoch 190/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0090 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 190: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0090 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.4270 - val_sparse_categorical_accuracy: 0.7196\n",
            "Epoch 191/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0103 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 191: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0103 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.5072 - val_sparse_categorical_accuracy: 0.7103\n",
            "Epoch 192/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0074 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 192: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 30ms/step - loss: 0.0074 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.5545 - val_sparse_categorical_accuracy: 0.7290\n",
            "Epoch 193/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0194 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 193: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0194 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.6367 - val_sparse_categorical_accuracy: 0.7383\n",
            "Epoch 194/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0029 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 194: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0029 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.5724 - val_sparse_categorical_accuracy: 0.7570\n",
            "Epoch 195/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0028 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 195: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0028 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.5018 - val_sparse_categorical_accuracy: 0.7383\n",
            "Epoch 196/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0190 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 196: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0190 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.4309 - val_sparse_categorical_accuracy: 0.7664\n",
            "Epoch 197/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0031 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 197: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0031 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.3780 - val_sparse_categorical_accuracy: 0.7570\n",
            "Epoch 198/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0180 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 198: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0180 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.4082 - val_sparse_categorical_accuracy: 0.7477\n",
            "Epoch 199/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0054 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 199: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0054 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.3181 - val_sparse_categorical_accuracy: 0.7477\n",
            "Epoch 200/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0196 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 200: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0196 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.2803 - val_sparse_categorical_accuracy: 0.7196\n",
            "Epoch 201/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0013 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 201: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 0.0013 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.4129 - val_sparse_categorical_accuracy: 0.7196\n",
            "Epoch 202/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0162 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 202: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 0.0162 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.3470 - val_sparse_categorical_accuracy: 0.7103\n",
            "Epoch 203/900\n",
            "5/7 [====================>.........] - ETA: 0s - loss: 0.0067 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 203: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.0053 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.3738 - val_sparse_categorical_accuracy: 0.7290\n",
            "Epoch 204/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0144 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 204: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 0.0144 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.2898 - val_sparse_categorical_accuracy: 0.7196\n",
            "Epoch 205/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0049 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 205: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 34ms/step - loss: 0.0050 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.3413 - val_sparse_categorical_accuracy: 0.7570\n",
            "Epoch 206/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0094 - sparse_categorical_accuracy: 0.9974\n",
            "Epoch 206: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 0.0086 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.3924 - val_sparse_categorical_accuracy: 0.7570\n",
            "Epoch 207/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0050 - sparse_categorical_accuracy: 0.9974    \n",
            "Epoch 207: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.0045 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.3799 - val_sparse_categorical_accuracy: 0.7290\n",
            "Epoch 208/900\n",
            "5/7 [====================>.........] - ETA: 0s - loss: 0.0035 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 208: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 35ms/step - loss: 0.0059 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.3284 - val_sparse_categorical_accuracy: 0.7383\n",
            "Epoch 209/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0019 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 209: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 30ms/step - loss: 0.0019 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.3521 - val_sparse_categorical_accuracy: 0.7664\n",
            "Epoch 210/900\n",
            "5/7 [====================>.........] - ETA: 0s - loss: 8.9582e-04 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 210: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.0025 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.4077 - val_sparse_categorical_accuracy: 0.7570\n",
            "Epoch 211/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0104 - sparse_categorical_accuracy: 0.9974\n",
            "Epoch 211: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 35ms/step - loss: 0.0097 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.3530 - val_sparse_categorical_accuracy: 0.7570\n",
            "Epoch 212/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0234 - sparse_categorical_accuracy: 0.9922\n",
            "Epoch 212: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.0230 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.3576 - val_sparse_categorical_accuracy: 0.7196\n",
            "Epoch 213/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0082 - sparse_categorical_accuracy: 0.9974\n",
            "Epoch 213: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.0114 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.7674 - val_sparse_categorical_accuracy: 0.6636\n",
            "Epoch 214/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0273 - sparse_categorical_accuracy: 0.9836\n",
            "Epoch 214: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 36ms/step - loss: 0.0273 - sparse_categorical_accuracy: 0.9836 - val_loss: 1.5343 - val_sparse_categorical_accuracy: 0.7196\n",
            "Epoch 215/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0096 - sparse_categorical_accuracy: 0.9948\n",
            "Epoch 215: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 34ms/step - loss: 0.0087 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.8162 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 216/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0331 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 216: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 0.0331 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.7996 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 217/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0192 - sparse_categorical_accuracy: 0.9922\n",
            "Epoch 217: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 0.0178 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.6043 - val_sparse_categorical_accuracy: 0.7383\n",
            "Epoch 218/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0065 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 218: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 0.0065 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.5707 - val_sparse_categorical_accuracy: 0.7477\n",
            "Epoch 219/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0047 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 219: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0047 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.5381 - val_sparse_categorical_accuracy: 0.7477\n",
            "Epoch 220/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0125 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 220: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0125 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.4408 - val_sparse_categorical_accuracy: 0.7383\n",
            "Epoch 221/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0053 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 221: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0053 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.4529 - val_sparse_categorical_accuracy: 0.7570\n",
            "Epoch 222/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0176 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 222: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0176 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.4231 - val_sparse_categorical_accuracy: 0.7570\n",
            "Epoch 223/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0172 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 223: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0172 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.5218 - val_sparse_categorical_accuracy: 0.7477\n",
            "Epoch 224/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0231 - sparse_categorical_accuracy: 0.9860\n",
            "Epoch 224: val_sparse_categorical_accuracy did not improve from 0.76636\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 0.0231 - sparse_categorical_accuracy: 0.9860 - val_loss: 1.5786 - val_sparse_categorical_accuracy: 0.7290\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "id": "1b676d1c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "1b676d1c",
        "outputId": "de0dec3f-ad2a-4260-f3f8-00d0aa7dd21d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Plot model accuracy over ephocs\n",
        "plt.plot(history.history['sparse_categorical_accuracy'])\n",
        "plt.plot(history.history['val_sparse_categorical_accuracy'])\n",
        "plt.ylim(0, 1)\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "id": "df8c9ade",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df8c9ade",
        "outputId": "92aeae3c-2f04-4fc1-ee1a-7ef9294bdb6f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7a531bbb49a0>"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ],
      "source": [
        "model.load_weights(checkpoint_path) #to load model with highest accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "id": "2f08c66c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f08c66c",
        "outputId": "47a0d46d-c815-458a-86b5-a7786b80a4ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 6ms/step - loss: 1.4061 - sparse_categorical_accuracy: 0.7664\n",
            "Pre-training accuracy: 76.6355%\n"
          ]
        }
      ],
      "source": [
        "# Calculate pre-training accuracy\n",
        "score = model.evaluate(X_test_scalled, y_test, verbose=1)\n",
        "accuracy = 100*score[1]\n",
        "\n",
        "print(\"Pre-training accuracy: %.4f%%\" % accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "id": "1bdfcfc8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bdfcfc8",
        "outputId": "801fa981-0d7e-4b6b-f7f3-51aa62bbdc44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy:  1.0\n",
            "Testing Accuracy:  0.7663551568984985\n"
          ]
        }
      ],
      "source": [
        "# Evaluating the model on the training and testing set\n",
        "score = model.evaluate(X_train_scalled, y_train, verbose=0)\n",
        "print(\"Training Accuracy: \", score[1])\n",
        "\n",
        "score = model.evaluate(X_test_scalled, y_test, verbose=0)\n",
        "print(\"Testing Accuracy: \", score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "id": "eaac1550",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaac1550",
        "outputId": "1396df07-538f-4863-d172-fc6001629a77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 1s 5ms/step\n"
          ]
        }
      ],
      "source": [
        "#Get predictions from model\n",
        "y_test_predictions = model.predict(X_test_scalled) # it will give the prediction data of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "id": "8a9df249",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a9df249",
        "outputId": "90ae0c3c-ed0d-40a4-8c2a-7980076eda6b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(107, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ],
      "source": [
        "y_test_predictions.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "id": "dda064ae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dda064ae",
        "outputId": "9c587017-48b8-4b57-b30f-40d39da07a89"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9.98760700e-01, 1.36483914e-06, 1.17405818e-03, 7.60009755e-07,\n",
              "        4.70875348e-05, 1.01475625e-05, 5.99745135e-06],\n",
              "       [1.43729503e-05, 9.44888651e-01, 9.21402625e-06, 3.12803400e-04,\n",
              "        6.39944410e-05, 5.46984524e-02, 1.24267171e-05],\n",
              "       [8.02832346e-07, 9.99937177e-01, 2.67730371e-07, 1.90627779e-05,\n",
              "        1.57051127e-05, 2.43530121e-05, 2.62668595e-06],\n",
              "       [2.42035482e-02, 1.90413575e-05, 3.18137492e-04, 5.12680845e-06,\n",
              "        9.75404441e-01, 9.53444032e-06, 4.01461621e-05],\n",
              "       [6.69360088e-05, 1.61058325e-02, 4.35283582e-05, 4.63465694e-05,\n",
              "        5.02028015e-05, 4.58283524e-04, 9.83228862e-01],\n",
              "       [4.26998995e-05, 1.08169242e-08, 9.99950051e-01, 6.53988138e-07,\n",
              "        5.05582011e-06, 1.81082012e-08, 1.58133525e-06],\n",
              "       [1.65319568e-06, 9.99113500e-01, 5.09084430e-07, 7.90607555e-06,\n",
              "        1.30777371e-05, 8.61819950e-04, 1.41699081e-06],\n",
              "       [2.69366683e-05, 9.76701975e-01, 1.29660948e-05, 4.66801430e-05,\n",
              "        6.42740779e-05, 2.18666513e-02, 1.28065271e-03],\n",
              "       [4.43562458e-04, 1.14175862e-07, 9.99532938e-01, 3.71416445e-06,\n",
              "        1.29547843e-05, 3.11419711e-07, 6.41227598e-06],\n",
              "       [2.99392198e-03, 7.16009140e-07, 9.96819258e-01, 1.58840940e-05,\n",
              "        1.34090631e-04, 1.50536300e-06, 3.45601075e-05],\n",
              "       [9.98299539e-01, 2.67254709e-05, 8.57027626e-05, 1.11962572e-06,\n",
              "        1.47520518e-03, 9.02992542e-05, 2.13850453e-05],\n",
              "       [9.99966383e-01, 2.15047535e-07, 2.60780907e-05, 5.00178636e-08,\n",
              "        1.43780937e-06, 2.71947101e-06, 3.07798678e-06],\n",
              "       [4.28081307e-07, 6.45826140e-06, 1.64789981e-05, 9.99916196e-01,\n",
              "        4.38784155e-05, 3.59302362e-07, 1.61760563e-05],\n",
              "       [4.44897963e-03, 1.02257961e-02, 1.01185222e-04, 1.04474282e-04,\n",
              "        2.15024964e-04, 3.13545577e-03, 9.81769085e-01],\n",
              "       [1.20404948e-04, 7.14539468e-01, 1.14995628e-05, 4.61999953e-05,\n",
              "        1.76663918e-04, 2.84885496e-01, 2.20319373e-04],\n",
              "       [7.44884543e-04, 8.94789040e-01, 8.13591105e-05, 1.20713994e-04,\n",
              "        2.76632444e-03, 2.49724463e-02, 7.65252709e-02],\n",
              "       [1.39366341e-06, 2.14026695e-05, 1.79157487e-05, 9.99912381e-01,\n",
              "        3.27941598e-05, 2.11057727e-06, 1.20949881e-05],\n",
              "       [9.99934435e-01, 1.15165619e-06, 2.35917105e-05, 1.44759028e-07,\n",
              "        9.45032480e-06, 2.69975953e-05, 4.15161821e-06],\n",
              "       [2.32378807e-05, 8.04645658e-01, 5.79329844e-06, 6.63681640e-05,\n",
              "        5.93340701e-05, 1.95180133e-01, 1.94911700e-05],\n",
              "       [5.70500409e-03, 1.24030151e-07, 9.94263232e-01, 2.09104951e-06,\n",
              "        1.42470990e-05, 2.92267913e-07, 1.49708694e-05],\n",
              "       [8.38862538e-01, 4.08046966e-04, 6.50979811e-04, 8.33641479e-06,\n",
              "        1.59711093e-01, 1.78752802e-04, 1.80287971e-04],\n",
              "       [1.49250536e-05, 8.29144381e-04, 3.40797510e-06, 2.94311894e-06,\n",
              "        5.49427659e-06, 9.99141455e-01, 2.60693423e-06],\n",
              "       [1.16477668e-05, 2.69936379e-02, 3.45113790e-06, 7.74475939e-06,\n",
              "        1.07275819e-05, 9.72970486e-01, 2.35880566e-06],\n",
              "       [7.63238072e-01, 8.73424287e-05, 2.32635885e-01, 2.50180223e-04,\n",
              "        2.25478713e-03, 1.01753860e-03, 5.16145898e-04],\n",
              "       [4.93467132e-06, 7.60001512e-07, 9.62832928e-06, 1.08583583e-06,\n",
              "        1.35044593e-05, 3.52986405e-08, 9.99969959e-01],\n",
              "       [2.37562253e-05, 3.14091027e-01, 4.62401385e-06, 2.50002831e-05,\n",
              "        3.58480902e-05, 6.85812891e-01, 6.83746839e-06],\n",
              "       [1.94333916e-06, 9.98457432e-01, 4.81036352e-07, 1.48522968e-05,\n",
              "        1.00263715e-05, 1.51129928e-03, 3.98515067e-06],\n",
              "       [5.77654755e-05, 4.09128606e-01, 4.24145837e-05, 3.01376480e-04,\n",
              "        1.74021663e-03, 5.88572979e-01, 1.56616647e-04],\n",
              "       [5.18605812e-04, 6.01629927e-06, 9.98130500e-01, 4.53511966e-05,\n",
              "        9.28816386e-04, 5.15648708e-06, 3.65569693e-04],\n",
              "       [3.54412805e-06, 9.87218380e-01, 2.57879151e-06, 5.05231619e-05,\n",
              "        1.91171039e-05, 1.27033219e-02, 2.58322689e-06],\n",
              "       [1.70807232e-06, 2.70502042e-05, 1.99070782e-04, 9.95435178e-01,\n",
              "        7.54294801e-04, 1.97511031e-06, 3.58071551e-03],\n",
              "       [5.62687637e-05, 7.71036412e-07, 9.97005045e-01, 2.08830770e-05,\n",
              "        1.95876369e-03, 3.83669459e-07, 9.57807293e-04],\n",
              "       [4.17897571e-03, 3.54469739e-06, 9.90125775e-01, 2.62552912e-05,\n",
              "        5.06459922e-03, 1.55284658e-06, 5.99350140e-04],\n",
              "       [4.66860774e-06, 9.98250544e-01, 5.19741661e-06, 4.73967084e-04,\n",
              "        4.97228357e-05, 1.20899593e-03, 6.90086745e-06],\n",
              "       [2.39648893e-02, 1.30399185e-05, 9.61112499e-01, 2.26692791e-04,\n",
              "        1.39222872e-02, 1.12810203e-05, 7.49218219e-04],\n",
              "       [5.04662894e-05, 6.55609620e-05, 1.31484203e-03, 2.44058137e-05,\n",
              "        9.96139467e-01, 1.54676291e-05, 2.38982611e-03],\n",
              "       [5.44579007e-06, 1.65607358e-04, 1.60205525e-06, 7.48358900e-07,\n",
              "        1.84922158e-06, 9.99822438e-01, 2.30372984e-06],\n",
              "       [9.10455526e-07, 4.25500366e-05, 1.12692514e-05, 9.99912262e-01,\n",
              "        1.31777169e-05, 1.79672736e-06, 1.78717019e-05],\n",
              "       [3.16319602e-05, 1.63297535e-08, 9.99954104e-01, 6.24037057e-07,\n",
              "        1.00915522e-05, 1.47686103e-08, 3.55481006e-06],\n",
              "       [9.67453889e-06, 5.08019403e-02, 6.92517788e-05, 9.48007882e-01,\n",
              "        8.00729671e-04, 1.04724004e-05, 3.00064188e-04],\n",
              "       [2.91889796e-06, 9.92002487e-01, 1.87813748e-06, 2.46117797e-05,\n",
              "        1.23392356e-05, 7.94167444e-03, 1.40875627e-05],\n",
              "       [1.83991455e-02, 3.12630534e-01, 3.26321897e-04, 6.97259733e-04,\n",
              "        6.59146011e-01, 1.79503602e-03, 7.00570690e-03],\n",
              "       [9.26387111e-06, 3.38668201e-06, 7.76260160e-04, 3.39399594e-06,\n",
              "        9.99176681e-01, 5.98662666e-08, 3.08442795e-05],\n",
              "       [2.12478728e-04, 7.12025070e-08, 9.99741733e-01, 2.84492580e-06,\n",
              "        3.85196981e-05, 1.22852455e-07, 4.20725519e-06],\n",
              "       [2.01697685e-06, 9.22819600e-05, 2.03540345e-04, 1.26685724e-01,\n",
              "        2.95499165e-04, 1.34485254e-06, 8.72719586e-01],\n",
              "       [2.79424376e-06, 1.31118657e-06, 4.65407811e-06, 7.14496252e-07,\n",
              "        9.99989033e-01, 4.62093901e-08, 1.53727899e-06],\n",
              "       [3.01346328e-04, 5.90820477e-08, 9.99675632e-01, 1.88190177e-06,\n",
              "        1.53486890e-05, 1.21357104e-07, 5.60015451e-06],\n",
              "       [1.03591028e-05, 4.30719322e-03, 2.92324557e-06, 3.80242386e-06,\n",
              "        8.15433395e-06, 9.95663464e-01, 4.06431900e-06],\n",
              "       [3.95649022e-06, 6.95085691e-05, 2.71276385e-05, 9.99839664e-01,\n",
              "        3.73462753e-05, 8.18980971e-06, 1.41695200e-05],\n",
              "       [6.10881045e-07, 6.89634999e-06, 2.17567249e-05, 3.32353375e-05,\n",
              "        9.99925971e-01, 4.32342731e-08, 1.16090423e-05],\n",
              "       [2.64936511e-06, 9.96236145e-01, 1.95899884e-06, 2.73445221e-05,\n",
              "        2.12243194e-05, 3.70821264e-03, 2.44012995e-06],\n",
              "       [3.63525942e-05, 6.65175961e-03, 9.45266293e-05, 9.88686442e-01,\n",
              "        3.68659873e-03, 1.83585042e-04, 6.60697930e-04],\n",
              "       [5.96097379e-06, 7.93567967e-09, 9.99977708e-01, 7.21724746e-07,\n",
              "        1.26130299e-05, 6.35166231e-09, 3.00215220e-06],\n",
              "       [8.86413336e-01, 7.04544084e-03, 2.15880980e-04, 1.57511931e-05,\n",
              "        7.18602038e-04, 1.05465375e-01, 1.25641804e-04],\n",
              "       [5.00292288e-07, 2.65857616e-05, 4.89018930e-06, 9.99947071e-01,\n",
              "        1.52137518e-05, 5.30984266e-07, 5.23380868e-06],\n",
              "       [1.75082704e-07, 7.05537502e-07, 5.21884294e-06, 3.29968293e-06,\n",
              "        9.99989390e-01, 2.15452260e-08, 1.18155879e-06],\n",
              "       [6.31054118e-02, 6.83907056e-05, 2.90286064e-01, 3.69130401e-04,\n",
              "        5.68024576e-01, 6.25538541e-06, 7.81401694e-02],\n",
              "       [1.32149271e-05, 2.30182540e-08, 9.99962568e-01, 1.53098233e-06,\n",
              "        2.00155810e-05, 3.02983345e-08, 2.59730655e-06],\n",
              "       [4.36609844e-05, 1.34258912e-07, 9.99437630e-01, 5.64197990e-06,\n",
              "        5.06583019e-04, 1.92661574e-07, 6.16416855e-06],\n",
              "       [4.04827297e-04, 1.03652292e-05, 9.96261775e-01, 2.34406689e-04,\n",
              "        2.88015860e-03, 1.90450373e-05, 1.89326631e-04],\n",
              "       [2.74387894e-05, 1.57933489e-08, 9.99946117e-01, 1.47290825e-06,\n",
              "        2.23593779e-05, 2.11829487e-08, 2.61555056e-06],\n",
              "       [6.89759418e-06, 4.24180968e-07, 6.85483858e-04, 6.26841711e-06,\n",
              "        2.02187639e-05, 3.24825216e-08, 9.99280751e-01],\n",
              "       [3.32115014e-05, 6.04507849e-02, 7.77701916e-06, 2.62743397e-05,\n",
              "        3.20596700e-05, 9.39412296e-01, 3.75741729e-05],\n",
              "       [1.79456256e-04, 9.26744309e-04, 7.62066105e-04, 1.27882406e-04,\n",
              "        9.76878032e-02, 8.47700285e-05, 9.00231302e-01],\n",
              "       [9.99843240e-01, 6.29036833e-07, 1.24468454e-04, 2.37635803e-07,\n",
              "        1.73259723e-05, 9.59869976e-06, 4.54045266e-06],\n",
              "       [5.14026124e-06, 9.92557764e-01, 1.71475187e-06, 3.33048120e-05,\n",
              "        2.47296048e-05, 7.36955181e-03, 7.80161463e-06],\n",
              "       [1.30641053e-03, 7.74262105e-07, 9.98409092e-01, 1.46500643e-05,\n",
              "        2.46811833e-04, 2.12832242e-06, 2.01645762e-05],\n",
              "       [7.98931342e-06, 5.42622474e-05, 3.51450726e-05, 4.99353395e-04,\n",
              "        2.84747584e-05, 3.81998907e-05, 9.99336541e-01],\n",
              "       [7.01675117e-01, 3.69575719e-04, 5.25863492e-04, 9.12710584e-06,\n",
              "        2.96924233e-01, 2.11967345e-04, 2.84119858e-04],\n",
              "       [8.80975585e-06, 9.14057851e-01, 4.17880938e-06, 3.03931211e-05,\n",
              "        2.02166066e-05, 8.58733132e-02, 5.19294508e-06],\n",
              "       [1.30384607e-04, 2.32203831e-07, 9.99360979e-01, 7.03327578e-06,\n",
              "        4.92456544e-04, 3.71236951e-07, 8.62724119e-06],\n",
              "       [1.18382377e-05, 9.96351719e-01, 7.01712133e-06, 8.89777206e-04,\n",
              "        1.27985826e-04, 2.59752036e-03, 1.41669479e-05],\n",
              "       [5.26725998e-05, 5.42042301e-07, 9.95934606e-01, 1.03298180e-05,\n",
              "        3.98424687e-03, 4.20057489e-07, 1.72086548e-05],\n",
              "       [7.83284049e-05, 4.38764546e-05, 1.42589906e-05, 3.71928650e-06,\n",
              "        9.99838948e-01, 1.60242780e-05, 4.88395153e-06],\n",
              "       [3.32267851e-01, 3.68619658e-04, 2.03158427e-03, 2.61391633e-05,\n",
              "        6.64831400e-01, 4.04279854e-04, 7.01497047e-05],\n",
              "       [9.85168208e-06, 9.79680359e-01, 3.76275784e-06, 3.87825385e-05,\n",
              "        3.59696496e-05, 2.02114191e-02, 1.99468122e-05],\n",
              "       [1.31096740e-05, 2.50113015e-08, 9.99954820e-01, 1.94611448e-06,\n",
              "        2.42973474e-05, 2.20921486e-08, 5.83358224e-06],\n",
              "       [1.52283235e-06, 1.26072473e-05, 2.71597619e-05, 3.00743136e-06,\n",
              "        9.99936938e-01, 1.40166333e-06, 1.73017852e-05],\n",
              "       [5.91729840e-05, 8.19571025e-04, 4.61749267e-04, 9.98038352e-01,\n",
              "        4.20140073e-04, 9.14181073e-05, 1.09665125e-04],\n",
              "       [8.63271125e-05, 3.28685076e-07, 9.99489665e-01, 1.39564809e-05,\n",
              "        3.98736505e-04, 5.91764149e-07, 1.04528008e-05],\n",
              "       [9.99720395e-01, 1.06047139e-06, 2.25710653e-04, 2.36951038e-07,\n",
              "        4.31745721e-05, 4.72286411e-06, 4.66421034e-06],\n",
              "       [2.02975771e-05, 5.18036742e-08, 9.99913931e-01, 2.05306583e-06,\n",
              "        5.19114183e-05, 3.72746598e-08, 1.16529827e-05],\n",
              "       [2.80181894e-06, 2.71546833e-06, 1.97749061e-04, 2.76248356e-05,\n",
              "        9.99747574e-01, 1.98030151e-07, 2.13007752e-05],\n",
              "       [5.08974972e-06, 9.94645357e-01, 2.78746938e-06, 2.42672657e-04,\n",
              "        2.77737927e-05, 5.07011311e-03, 6.20827359e-06],\n",
              "       [9.99587119e-01, 5.78300569e-06, 2.15189255e-04, 6.98502561e-07,\n",
              "        5.66115850e-05, 2.65967301e-05, 1.07872082e-04],\n",
              "       [4.06396284e-05, 1.67087754e-04, 5.31818159e-06, 2.23225038e-06,\n",
              "        4.81152802e-06, 9.99772608e-01, 7.38172639e-06],\n",
              "       [6.73411762e-07, 9.99910355e-01, 4.24064666e-07, 2.66133229e-05,\n",
              "        1.58102885e-05, 4.53594221e-05, 7.38502081e-07],\n",
              "       [6.99580778e-06, 1.80715020e-03, 3.39873426e-04, 5.03620863e-01,\n",
              "        1.86607596e-02, 1.02244594e-05, 4.75554168e-01],\n",
              "       [1.31241092e-02, 3.13079872e-05, 1.91197367e-04, 1.03135517e-05,\n",
              "        9.86465394e-01, 1.70413005e-05, 1.60597338e-04],\n",
              "       [5.15192141e-06, 5.60738670e-04, 3.76032199e-06, 1.52145994e-05,\n",
              "        1.99602673e-05, 9.99388814e-01, 6.36546974e-06],\n",
              "       [6.15215731e-06, 4.34184635e-08, 9.99851227e-01, 4.71490421e-06,\n",
              "        1.31276131e-04, 4.59828833e-08, 6.47596153e-06],\n",
              "       [7.49228911e-06, 4.35893277e-08, 9.99865055e-01, 4.77616550e-06,\n",
              "        1.16813360e-04, 3.13554054e-08, 5.82007851e-06],\n",
              "       [5.61880297e-05, 9.07829900e-08, 9.99803841e-01, 4.75147226e-06,\n",
              "        9.98905962e-05, 7.53998251e-08, 3.52221832e-05],\n",
              "       [2.00918498e-06, 3.10792238e-05, 1.65117439e-04, 9.98160303e-01,\n",
              "        1.26579049e-04, 5.67314657e-07, 1.51431956e-03],\n",
              "       [1.66017166e-03, 2.00455156e-06, 9.95095253e-01, 4.37574927e-05,\n",
              "        1.90685547e-04, 1.18532660e-06, 3.00692348e-03],\n",
              "       [5.85919879e-05, 3.53205483e-04, 7.44957291e-03, 8.88650656e-01,\n",
              "        1.79180093e-02, 5.06265633e-06, 8.55649039e-02],\n",
              "       [8.90833007e-07, 1.09174180e-05, 2.06567245e-04, 9.97581244e-01,\n",
              "        4.49665444e-04, 1.96962290e-07, 1.75047305e-03],\n",
              "       [4.23826123e-07, 5.27788006e-06, 3.35595578e-05, 9.99818861e-01,\n",
              "        9.56386648e-05, 2.85947067e-07, 4.59354414e-05],\n",
              "       [6.33724921e-05, 3.69662217e-07, 9.98923242e-01, 1.41524151e-05,\n",
              "        9.57762008e-04, 5.92536992e-07, 4.04532802e-05],\n",
              "       [7.81121023e-04, 5.84819500e-05, 8.83590244e-03, 4.26643557e-04,\n",
              "        1.16830121e-03, 3.36264156e-06, 9.88726199e-01],\n",
              "       [5.33517063e-01, 8.79068182e-07, 4.66380835e-01, 6.86845397e-06,\n",
              "        4.17023111e-05, 8.48776835e-06, 4.41271004e-05],\n",
              "       [1.11114769e-03, 9.33044575e-08, 9.98870552e-01, 2.13969633e-06,\n",
              "        1.18004818e-05, 2.85312666e-07, 3.98644579e-06],\n",
              "       [1.15534589e-04, 9.47867155e-01, 6.86593239e-06, 6.61568265e-05,\n",
              "        1.62333762e-03, 5.03044277e-02, 1.66825721e-05],\n",
              "       [1.98479960e-04, 9.07597132e-05, 3.36772951e-06, 1.75004334e-06,\n",
              "        2.61792729e-06, 9.99684215e-01, 1.88787235e-05],\n",
              "       [1.36793806e-05, 1.13429405e-05, 2.00766517e-05, 1.78200276e-06,\n",
              "        9.99944925e-01, 5.34654100e-06, 2.88702472e-06],\n",
              "       [3.13344663e-06, 2.08765105e-03, 5.72243080e-05, 9.78328586e-01,\n",
              "        1.55262504e-04, 1.85274712e-06, 1.93662420e-02],\n",
              "       [1.84888748e-04, 1.44904050e-07, 9.99698997e-01, 5.12296765e-06,\n",
              "        8.53367965e-05, 2.42914069e-07, 2.52376849e-05]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ],
      "source": [
        "y_test_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "id": "996eb306",
      "metadata": {
        "id": "996eb306"
      },
      "outputs": [],
      "source": [
        "y_test_predictions=np.argmax(y_test_predictions,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "id": "5f9002eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f9002eb",
        "outputId": "0bd32a3f-f634-4cce-bc97-5d59cd3a4fdf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 1, 4, 6, 2, 1, 1, 2, 2, 0, 0, 3, 6, 1, 1, 3, 0, 1, 2, 0, 5,\n",
              "       5, 0, 6, 5, 1, 5, 2, 1, 3, 2, 2, 1, 2, 4, 5, 3, 2, 3, 1, 4, 4, 2,\n",
              "       6, 4, 2, 5, 3, 4, 1, 3, 2, 0, 3, 4, 4, 2, 2, 2, 2, 6, 5, 6, 0, 1,\n",
              "       2, 6, 0, 1, 2, 1, 2, 4, 4, 1, 2, 4, 3, 2, 0, 2, 4, 1, 0, 5, 1, 3,\n",
              "       4, 5, 2, 2, 2, 3, 2, 3, 3, 3, 2, 6, 0, 2, 1, 5, 4, 3, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ],
      "source": [
        "y_test_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "id": "2a98a885",
      "metadata": {
        "id": "2a98a885"
      },
      "outputs": [],
      "source": [
        "# df.replace({ 'happyness': 0, 'neutral': 1,'anger': 2,'sadness': 3, 'fear':4,'boredom':5,'disgust':6}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "id": "d0b8e93d",
      "metadata": {
        "id": "d0b8e93d"
      },
      "outputs": [],
      "source": [
        "emotions={\n",
        " 0: 'happyness',\n",
        " 1: 'neutral',\n",
        " 2: 'anger',\n",
        " 3: 'sadness',\n",
        " 4: 'fear',\n",
        " 5: 'boredom',\n",
        " 6: 'disgust',\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "id": "a32e6964",
      "metadata": {
        "id": "a32e6964"
      },
      "outputs": [],
      "source": [
        "label=[]\n",
        "for i in y_test_predictions:\n",
        "    label1=emotions[i]\n",
        "    label.append(label1)\n",
        "label\n",
        "y_pred_acc=np.array(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "id": "c92b0963",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c92b0963",
        "outputId": "28b4e252-e8c0-405a-acc7-a0d785c68939"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['happyness', 'neutral', 'neutral', 'fear', 'disgust', 'anger',\n",
              "       'neutral', 'neutral', 'anger', 'anger', 'happyness', 'happyness',\n",
              "       'sadness', 'disgust', 'neutral', 'neutral', 'sadness', 'happyness',\n",
              "       'neutral', 'anger', 'happyness', 'boredom', 'boredom', 'happyness',\n",
              "       'disgust', 'boredom', 'neutral', 'boredom', 'anger', 'neutral',\n",
              "       'sadness', 'anger', 'anger', 'neutral', 'anger', 'fear', 'boredom',\n",
              "       'sadness', 'anger', 'sadness', 'neutral', 'fear', 'fear', 'anger',\n",
              "       'disgust', 'fear', 'anger', 'boredom', 'sadness', 'fear',\n",
              "       'neutral', 'sadness', 'anger', 'happyness', 'sadness', 'fear',\n",
              "       'fear', 'anger', 'anger', 'anger', 'anger', 'disgust', 'boredom',\n",
              "       'disgust', 'happyness', 'neutral', 'anger', 'disgust', 'happyness',\n",
              "       'neutral', 'anger', 'neutral', 'anger', 'fear', 'fear', 'neutral',\n",
              "       'anger', 'fear', 'sadness', 'anger', 'happyness', 'anger', 'fear',\n",
              "       'neutral', 'happyness', 'boredom', 'neutral', 'sadness', 'fear',\n",
              "       'boredom', 'anger', 'anger', 'anger', 'sadness', 'anger',\n",
              "       'sadness', 'sadness', 'sadness', 'anger', 'disgust', 'happyness',\n",
              "       'anger', 'neutral', 'boredom', 'fear', 'sadness', 'anger'],\n",
              "      dtype='<U9')"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ],
      "source": [
        "y_pred_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "id": "59bdabc1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59bdabc1",
        "outputId": "f87129df-f2a0-4a49-b8bb-46918427b879"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      0\n",
              "1      5\n",
              "2      1\n",
              "3      0\n",
              "4      5\n",
              "      ..\n",
              "102    1\n",
              "103    5\n",
              "104    4\n",
              "105    3\n",
              "106    2\n",
              "Name: 0.1, Length: 107, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "id": "0dec7231",
      "metadata": {
        "id": "0dec7231"
      },
      "outputs": [],
      "source": [
        "emotion={\n",
        " 0: 'happyness',\n",
        " 1: 'neutral',\n",
        " 2: 'anger',\n",
        " 3: 'sadness',\n",
        " 4: 'fear',\n",
        " 5: 'boredom',\n",
        " 6: 'disgust',\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "id": "06e8caee",
      "metadata": {
        "id": "06e8caee"
      },
      "outputs": [],
      "source": [
        "label_test=[]\n",
        "for i in y_test:\n",
        "    label_test.append(emotion[i])\n",
        "label_test\n",
        "y_true_accu=np.array(label_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "id": "9562d2b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9562d2b0",
        "outputId": "a63f66af-f3c6-4b24-e107-f456b846878a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['happyness', 'boredom', 'neutral', 'happyness', 'boredom', 'anger',\n",
              "       'neutral', 'neutral', 'anger', 'anger', 'happyness', 'happyness',\n",
              "       'sadness', 'happyness', 'neutral', 'neutral', 'sadness',\n",
              "       'happyness', 'boredom', 'anger', 'fear', 'neutral', 'boredom',\n",
              "       'happyness', 'disgust', 'boredom', 'neutral', 'sadness', 'anger',\n",
              "       'neutral', 'fear', 'anger', 'happyness', 'neutral', 'anger',\n",
              "       'fear', 'boredom', 'sadness', 'anger', 'boredom', 'boredom',\n",
              "       'fear', 'fear', 'anger', 'boredom', 'fear', 'anger', 'boredom',\n",
              "       'sadness', 'fear', 'neutral', 'disgust', 'anger', 'neutral',\n",
              "       'sadness', 'fear', 'disgust', 'anger', 'anger', 'anger', 'anger',\n",
              "       'disgust', 'boredom', 'disgust', 'anger', 'boredom', 'anger',\n",
              "       'disgust', 'happyness', 'boredom', 'anger', 'neutral', 'anger',\n",
              "       'fear', 'happyness', 'neutral', 'anger', 'fear', 'sadness',\n",
              "       'anger', 'fear', 'happyness', 'fear', 'neutral', 'happyness',\n",
              "       'boredom', 'neutral', 'disgust', 'fear', 'boredom', 'anger',\n",
              "       'anger', 'anger', 'sadness', 'disgust', 'sadness', 'sadness',\n",
              "       'sadness', 'anger', 'disgust', 'happyness', 'happyness', 'neutral',\n",
              "       'boredom', 'fear', 'sadness', 'anger'], dtype='<U9')"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ],
      "source": [
        "y_true_accu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "id": "8def2194",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8def2194",
        "outputId": "2163cb51-425f-4d24-80b7-220f98b41298"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 76.64%\n"
          ]
        }
      ],
      "source": [
        "#DataFlair - Calculate the accuracy of our model\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy=accuracy_score(y_true=y_true_accu, y_pred=y_pred_acc)\n",
        "\n",
        "#DataFlair - Print the accuracy\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "id": "0397dc13",
      "metadata": {
        "id": "0397dc13"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm=confusion_matrix(y_true=y_true_accu, y_pred=y_pred_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "id": "7cee98a1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cee98a1",
        "outputId": "8c686568-993a-42c7-c860-24732e04f7a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.86      0.96      0.91        26\n",
            "     boredom       0.80      0.50      0.62        16\n",
            "     disgust       0.62      0.56      0.59         9\n",
            "        fear       0.79      0.79      0.79        14\n",
            "   happyness       0.67      0.57      0.62        14\n",
            "     neutral       0.74      0.88      0.80        16\n",
            "     sadness       0.73      0.92      0.81        12\n",
            "\n",
            "    accuracy                           0.77       107\n",
            "   macro avg       0.74      0.74      0.73       107\n",
            "weighted avg       0.76      0.77      0.76       107\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true_accu,y_pred_acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "id": "73b3090a",
      "metadata": {
        "id": "73b3090a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "id": "4537d18f",
      "metadata": {
        "id": "4537d18f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}