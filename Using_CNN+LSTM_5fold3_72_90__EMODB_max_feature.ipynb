{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 522,
      "id": "e1f95dfd",
      "metadata": {
        "id": "e1f95dfd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import seaborn as sns\n",
        "import librosa\n",
        "import librosa.display\n",
        "import IPython.display as pld\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xe7uVNwQ3Ppw",
        "outputId": "439b3d84-4df5-405c-b4f4-2192576d7f84"
      },
      "id": "Xe7uVNwQ3Ppw",
      "execution_count": 523,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 524,
      "id": "603a1332",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "603a1332",
        "outputId": "722f5cd2-5ff1-44a6-8712-4ad45ebb8aa4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Unnamed: 0          0          1          2          3          4  \\\n",
              "0             0 -482.45233  62.835957  -0.348998  26.264595   2.978607   \n",
              "1             1 -469.48477  88.400730  -7.127512  29.156132   5.335554   \n",
              "2             2 -434.88647  41.972150 -29.416862  18.537344  -4.156565   \n",
              "3             3 -454.89886  45.067265  -0.193278  15.111545   3.080835   \n",
              "4             4 -447.12630  86.114920   4.772520  38.097256   8.324276   \n",
              "..          ...        ...        ...        ...        ...        ...   \n",
              "423         423 -418.62490  57.015880   6.383415  47.618423  -8.051490   \n",
              "424         424 -427.36716  51.473750   4.835125  40.900140   6.937289   \n",
              "425         425 -467.15588  52.217026  10.470471  47.414780   8.690019   \n",
              "426         426 -526.19570  11.317784 -18.942173  29.540787 -28.057306   \n",
              "427         427 -437.97220   5.289146 -22.667547  25.394840 -28.545520   \n",
              "\n",
              "             5          6         7          8  ...       250       251  \\\n",
              "0     6.900927 -13.006243  0.273391  -9.196591  ...  0.675823  0.684034   \n",
              "1     7.147227  -6.727572 -8.307674  -3.364513  ...  0.635522  0.544438   \n",
              "2     5.257353 -11.410935 -8.983023 -11.285996  ...  0.648380  0.675295   \n",
              "3     4.204241 -10.440731 -6.615343 -16.249382  ...  0.575784  0.523305   \n",
              "4     8.538317  -4.507682 -7.680664  -7.317249  ...  0.615263  0.566964   \n",
              "..         ...        ...       ...        ...  ...       ...       ...   \n",
              "423   4.213936 -15.029120 -2.448467 -10.805821  ...  0.569263  0.551809   \n",
              "424  13.700687  -8.331753 -0.583414  -6.279562  ...  0.638155  0.574542   \n",
              "425  15.601270  -2.032935  4.985774  -7.432734  ...  0.523647  0.509723   \n",
              "426   1.299599 -16.251814 -7.512440 -23.191568  ...  0.528861  0.501070   \n",
              "427   0.428451 -11.650926 -8.022680 -18.337156  ...  0.495079  0.539387   \n",
              "\n",
              "          252       253       254       255       256       257       258  0.1  \n",
              "0    0.627376 -0.001574  0.012645 -0.027069  0.019313 -0.002252 -0.006220    0  \n",
              "1    0.532856 -0.018488  0.011973 -0.011038  0.079758 -0.021044 -0.019445    1  \n",
              "2    0.560000 -0.010576 -0.000228  0.011102 -0.074188  0.012116 -0.000779    2  \n",
              "3    0.423375  0.009395 -0.025547 -0.035554 -0.025885  0.011198 -0.000163    0  \n",
              "4    0.605103 -0.008389 -0.051995 -0.056544 -0.020014  0.013964  0.008349    1  \n",
              "..        ...       ...       ...       ...       ...       ...       ...  ...  \n",
              "423  0.576764  0.016046 -0.025610  0.025426 -0.094371  0.008999 -0.004985    5  \n",
              "424  0.503516  0.019632  0.046315  0.074801  0.018078  0.022948 -0.016365    3  \n",
              "425  0.498566  0.051532  0.015525 -0.006052  0.018201  0.004361 -0.021649    3  \n",
              "426  0.496900  0.007856  0.005917 -0.045393 -0.004246 -0.001628  0.015051    2  \n",
              "427  0.527394 -0.001086  0.013948  0.013274 -0.056855 -0.001004 -0.001355    2  \n",
              "\n",
              "[428 rows x 261 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-52190609-cfbd-4750-a3bb-088e3a663d17\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>...</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "      <th>257</th>\n",
              "      <th>258</th>\n",
              "      <th>0.1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-482.45233</td>\n",
              "      <td>62.835957</td>\n",
              "      <td>-0.348998</td>\n",
              "      <td>26.264595</td>\n",
              "      <td>2.978607</td>\n",
              "      <td>6.900927</td>\n",
              "      <td>-13.006243</td>\n",
              "      <td>0.273391</td>\n",
              "      <td>-9.196591</td>\n",
              "      <td>...</td>\n",
              "      <td>0.675823</td>\n",
              "      <td>0.684034</td>\n",
              "      <td>0.627376</td>\n",
              "      <td>-0.001574</td>\n",
              "      <td>0.012645</td>\n",
              "      <td>-0.027069</td>\n",
              "      <td>0.019313</td>\n",
              "      <td>-0.002252</td>\n",
              "      <td>-0.006220</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>-469.48477</td>\n",
              "      <td>88.400730</td>\n",
              "      <td>-7.127512</td>\n",
              "      <td>29.156132</td>\n",
              "      <td>5.335554</td>\n",
              "      <td>7.147227</td>\n",
              "      <td>-6.727572</td>\n",
              "      <td>-8.307674</td>\n",
              "      <td>-3.364513</td>\n",
              "      <td>...</td>\n",
              "      <td>0.635522</td>\n",
              "      <td>0.544438</td>\n",
              "      <td>0.532856</td>\n",
              "      <td>-0.018488</td>\n",
              "      <td>0.011973</td>\n",
              "      <td>-0.011038</td>\n",
              "      <td>0.079758</td>\n",
              "      <td>-0.021044</td>\n",
              "      <td>-0.019445</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>-434.88647</td>\n",
              "      <td>41.972150</td>\n",
              "      <td>-29.416862</td>\n",
              "      <td>18.537344</td>\n",
              "      <td>-4.156565</td>\n",
              "      <td>5.257353</td>\n",
              "      <td>-11.410935</td>\n",
              "      <td>-8.983023</td>\n",
              "      <td>-11.285996</td>\n",
              "      <td>...</td>\n",
              "      <td>0.648380</td>\n",
              "      <td>0.675295</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>-0.010576</td>\n",
              "      <td>-0.000228</td>\n",
              "      <td>0.011102</td>\n",
              "      <td>-0.074188</td>\n",
              "      <td>0.012116</td>\n",
              "      <td>-0.000779</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>-454.89886</td>\n",
              "      <td>45.067265</td>\n",
              "      <td>-0.193278</td>\n",
              "      <td>15.111545</td>\n",
              "      <td>3.080835</td>\n",
              "      <td>4.204241</td>\n",
              "      <td>-10.440731</td>\n",
              "      <td>-6.615343</td>\n",
              "      <td>-16.249382</td>\n",
              "      <td>...</td>\n",
              "      <td>0.575784</td>\n",
              "      <td>0.523305</td>\n",
              "      <td>0.423375</td>\n",
              "      <td>0.009395</td>\n",
              "      <td>-0.025547</td>\n",
              "      <td>-0.035554</td>\n",
              "      <td>-0.025885</td>\n",
              "      <td>0.011198</td>\n",
              "      <td>-0.000163</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>-447.12630</td>\n",
              "      <td>86.114920</td>\n",
              "      <td>4.772520</td>\n",
              "      <td>38.097256</td>\n",
              "      <td>8.324276</td>\n",
              "      <td>8.538317</td>\n",
              "      <td>-4.507682</td>\n",
              "      <td>-7.680664</td>\n",
              "      <td>-7.317249</td>\n",
              "      <td>...</td>\n",
              "      <td>0.615263</td>\n",
              "      <td>0.566964</td>\n",
              "      <td>0.605103</td>\n",
              "      <td>-0.008389</td>\n",
              "      <td>-0.051995</td>\n",
              "      <td>-0.056544</td>\n",
              "      <td>-0.020014</td>\n",
              "      <td>0.013964</td>\n",
              "      <td>0.008349</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>423</th>\n",
              "      <td>423</td>\n",
              "      <td>-418.62490</td>\n",
              "      <td>57.015880</td>\n",
              "      <td>6.383415</td>\n",
              "      <td>47.618423</td>\n",
              "      <td>-8.051490</td>\n",
              "      <td>4.213936</td>\n",
              "      <td>-15.029120</td>\n",
              "      <td>-2.448467</td>\n",
              "      <td>-10.805821</td>\n",
              "      <td>...</td>\n",
              "      <td>0.569263</td>\n",
              "      <td>0.551809</td>\n",
              "      <td>0.576764</td>\n",
              "      <td>0.016046</td>\n",
              "      <td>-0.025610</td>\n",
              "      <td>0.025426</td>\n",
              "      <td>-0.094371</td>\n",
              "      <td>0.008999</td>\n",
              "      <td>-0.004985</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424</th>\n",
              "      <td>424</td>\n",
              "      <td>-427.36716</td>\n",
              "      <td>51.473750</td>\n",
              "      <td>4.835125</td>\n",
              "      <td>40.900140</td>\n",
              "      <td>6.937289</td>\n",
              "      <td>13.700687</td>\n",
              "      <td>-8.331753</td>\n",
              "      <td>-0.583414</td>\n",
              "      <td>-6.279562</td>\n",
              "      <td>...</td>\n",
              "      <td>0.638155</td>\n",
              "      <td>0.574542</td>\n",
              "      <td>0.503516</td>\n",
              "      <td>0.019632</td>\n",
              "      <td>0.046315</td>\n",
              "      <td>0.074801</td>\n",
              "      <td>0.018078</td>\n",
              "      <td>0.022948</td>\n",
              "      <td>-0.016365</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>425</th>\n",
              "      <td>425</td>\n",
              "      <td>-467.15588</td>\n",
              "      <td>52.217026</td>\n",
              "      <td>10.470471</td>\n",
              "      <td>47.414780</td>\n",
              "      <td>8.690019</td>\n",
              "      <td>15.601270</td>\n",
              "      <td>-2.032935</td>\n",
              "      <td>4.985774</td>\n",
              "      <td>-7.432734</td>\n",
              "      <td>...</td>\n",
              "      <td>0.523647</td>\n",
              "      <td>0.509723</td>\n",
              "      <td>0.498566</td>\n",
              "      <td>0.051532</td>\n",
              "      <td>0.015525</td>\n",
              "      <td>-0.006052</td>\n",
              "      <td>0.018201</td>\n",
              "      <td>0.004361</td>\n",
              "      <td>-0.021649</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>426</th>\n",
              "      <td>426</td>\n",
              "      <td>-526.19570</td>\n",
              "      <td>11.317784</td>\n",
              "      <td>-18.942173</td>\n",
              "      <td>29.540787</td>\n",
              "      <td>-28.057306</td>\n",
              "      <td>1.299599</td>\n",
              "      <td>-16.251814</td>\n",
              "      <td>-7.512440</td>\n",
              "      <td>-23.191568</td>\n",
              "      <td>...</td>\n",
              "      <td>0.528861</td>\n",
              "      <td>0.501070</td>\n",
              "      <td>0.496900</td>\n",
              "      <td>0.007856</td>\n",
              "      <td>0.005917</td>\n",
              "      <td>-0.045393</td>\n",
              "      <td>-0.004246</td>\n",
              "      <td>-0.001628</td>\n",
              "      <td>0.015051</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>427</th>\n",
              "      <td>427</td>\n",
              "      <td>-437.97220</td>\n",
              "      <td>5.289146</td>\n",
              "      <td>-22.667547</td>\n",
              "      <td>25.394840</td>\n",
              "      <td>-28.545520</td>\n",
              "      <td>0.428451</td>\n",
              "      <td>-11.650926</td>\n",
              "      <td>-8.022680</td>\n",
              "      <td>-18.337156</td>\n",
              "      <td>...</td>\n",
              "      <td>0.495079</td>\n",
              "      <td>0.539387</td>\n",
              "      <td>0.527394</td>\n",
              "      <td>-0.001086</td>\n",
              "      <td>0.013948</td>\n",
              "      <td>0.013274</td>\n",
              "      <td>-0.056855</td>\n",
              "      <td>-0.001004</td>\n",
              "      <td>-0.001355</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>428 rows × 261 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-52190609-cfbd-4750-a3bb-088e3a663d17')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-52190609-cfbd-4750-a3bb-088e3a663d17 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-52190609-cfbd-4750-a3bb-088e3a663d17');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-884c6aee-38fe-41a7-ae2d-c39dd718e8fe\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-884c6aee-38fe-41a7-ae2d-c39dd718e8fe')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-884c6aee-38fe-41a7-ae2d-c39dd718e8fe button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df1"
            }
          },
          "metadata": {},
          "execution_count": 524
        }
      ],
      "source": [
        "df1=pd.read_csv('/content/drive/MyDrive/DATASETS/EmoDB Dataset/train_fold3.csv')\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 525,
      "id": "a08acf9d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "a08acf9d",
        "outputId": "4705f379-09ae-4ed1-d635-6f534c54dfd2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Unnamed: 0          0          1          2          3          4  \\\n",
              "0             0 -451.45264  71.335200  12.223010  32.649555   3.997801   \n",
              "1             1 -438.46402  94.930030   5.689025  25.277182   2.760727   \n",
              "2             2 -451.66434  41.310406 -20.668978   6.190408 -11.533198   \n",
              "3             3 -415.74384  36.454792  -0.837229  20.279540  -5.861744   \n",
              "4             4 -444.03302  48.236977   7.208580  24.032001   0.882425   \n",
              "..          ...        ...        ...        ...        ...        ...   \n",
              "102         102 -465.86484  37.754044  11.739282  38.639137  12.334189   \n",
              "103         103 -441.11627  55.757492 -10.028031  25.111618   1.534458   \n",
              "104         104 -447.65580  59.147980  10.811775  34.741950   8.662245   \n",
              "105         105 -506.06097  21.649736 -21.605825  25.993687 -16.120153   \n",
              "106         106 -392.73694  58.074574  -2.923840  40.999270 -16.602533   \n",
              "\n",
              "             5          6         7          8  ...       250       251  \\\n",
              "0    15.200773  -2.812883 -1.384373  -6.467040  ...  0.719856  0.667424   \n",
              "1    18.562307  -5.944632 -2.279853  -1.935391  ...  0.663641  0.593349   \n",
              "2     6.807915 -13.584462 -4.452075 -20.147266  ...  0.528094  0.548985   \n",
              "3     3.429390 -12.954763 -4.034247 -20.398386  ...  0.656849  0.632091   \n",
              "4     4.175030  -7.778720 -0.999663 -14.226709  ...  0.627324  0.567694   \n",
              "..         ...        ...       ...        ...  ...       ...       ...   \n",
              "102  17.117222  -5.747991 -0.765464   0.111881  ...  0.601236  0.557807   \n",
              "103   6.648944 -16.968105 -9.461746 -11.462216  ...  0.539574  0.556530   \n",
              "104  23.050500  -5.105643 -0.383535  -7.201605  ...  0.526737  0.507366   \n",
              "105  -4.204089 -10.385810 -6.951331 -16.107372  ...  0.444902  0.483585   \n",
              "106  12.707182 -17.690449 -5.038430 -16.203160  ...  0.431654  0.480828   \n",
              "\n",
              "          252       253       254       255       256       257       258  0.1  \n",
              "0    0.570905  0.003044 -0.006101 -0.009038 -0.049699  0.027615  0.021319    3  \n",
              "1    0.568150 -0.007860 -0.022793 -0.053155 -0.042559 -0.019416  0.022956    3  \n",
              "2    0.598431  0.019574 -0.013364 -0.001339  0.002642  0.000159 -0.011073    2  \n",
              "3    0.612582  0.016577  0.009726  0.025019 -0.011375 -0.012012 -0.002431    4  \n",
              "4    0.531798  0.000531 -0.012216 -0.010982  0.028078  0.001874  0.005371    0  \n",
              "..        ...       ...       ...       ...       ...       ...       ...  ...  \n",
              "102  0.523268  0.018790  0.004060  0.101219 -0.063615  0.026759 -0.001490    3  \n",
              "103  0.571780  0.000074 -0.019049  0.000770 -0.035572  0.001909  0.012600    6  \n",
              "104  0.484251  0.008875 -0.001532  0.020466 -0.000596  0.013177 -0.000570    3  \n",
              "105  0.522145 -0.011355  0.025692 -0.011368 -0.021783 -0.013677 -0.005565    0  \n",
              "106  0.515922 -0.033965  0.000612  0.004869 -0.023429  0.010656  0.009665    5  \n",
              "\n",
              "[107 rows x 261 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-00f059c7-a5cb-4376-b7b7-56b131e88497\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>...</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "      <th>257</th>\n",
              "      <th>258</th>\n",
              "      <th>0.1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-451.45264</td>\n",
              "      <td>71.335200</td>\n",
              "      <td>12.223010</td>\n",
              "      <td>32.649555</td>\n",
              "      <td>3.997801</td>\n",
              "      <td>15.200773</td>\n",
              "      <td>-2.812883</td>\n",
              "      <td>-1.384373</td>\n",
              "      <td>-6.467040</td>\n",
              "      <td>...</td>\n",
              "      <td>0.719856</td>\n",
              "      <td>0.667424</td>\n",
              "      <td>0.570905</td>\n",
              "      <td>0.003044</td>\n",
              "      <td>-0.006101</td>\n",
              "      <td>-0.009038</td>\n",
              "      <td>-0.049699</td>\n",
              "      <td>0.027615</td>\n",
              "      <td>0.021319</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>-438.46402</td>\n",
              "      <td>94.930030</td>\n",
              "      <td>5.689025</td>\n",
              "      <td>25.277182</td>\n",
              "      <td>2.760727</td>\n",
              "      <td>18.562307</td>\n",
              "      <td>-5.944632</td>\n",
              "      <td>-2.279853</td>\n",
              "      <td>-1.935391</td>\n",
              "      <td>...</td>\n",
              "      <td>0.663641</td>\n",
              "      <td>0.593349</td>\n",
              "      <td>0.568150</td>\n",
              "      <td>-0.007860</td>\n",
              "      <td>-0.022793</td>\n",
              "      <td>-0.053155</td>\n",
              "      <td>-0.042559</td>\n",
              "      <td>-0.019416</td>\n",
              "      <td>0.022956</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>-451.66434</td>\n",
              "      <td>41.310406</td>\n",
              "      <td>-20.668978</td>\n",
              "      <td>6.190408</td>\n",
              "      <td>-11.533198</td>\n",
              "      <td>6.807915</td>\n",
              "      <td>-13.584462</td>\n",
              "      <td>-4.452075</td>\n",
              "      <td>-20.147266</td>\n",
              "      <td>...</td>\n",
              "      <td>0.528094</td>\n",
              "      <td>0.548985</td>\n",
              "      <td>0.598431</td>\n",
              "      <td>0.019574</td>\n",
              "      <td>-0.013364</td>\n",
              "      <td>-0.001339</td>\n",
              "      <td>0.002642</td>\n",
              "      <td>0.000159</td>\n",
              "      <td>-0.011073</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>-415.74384</td>\n",
              "      <td>36.454792</td>\n",
              "      <td>-0.837229</td>\n",
              "      <td>20.279540</td>\n",
              "      <td>-5.861744</td>\n",
              "      <td>3.429390</td>\n",
              "      <td>-12.954763</td>\n",
              "      <td>-4.034247</td>\n",
              "      <td>-20.398386</td>\n",
              "      <td>...</td>\n",
              "      <td>0.656849</td>\n",
              "      <td>0.632091</td>\n",
              "      <td>0.612582</td>\n",
              "      <td>0.016577</td>\n",
              "      <td>0.009726</td>\n",
              "      <td>0.025019</td>\n",
              "      <td>-0.011375</td>\n",
              "      <td>-0.012012</td>\n",
              "      <td>-0.002431</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>-444.03302</td>\n",
              "      <td>48.236977</td>\n",
              "      <td>7.208580</td>\n",
              "      <td>24.032001</td>\n",
              "      <td>0.882425</td>\n",
              "      <td>4.175030</td>\n",
              "      <td>-7.778720</td>\n",
              "      <td>-0.999663</td>\n",
              "      <td>-14.226709</td>\n",
              "      <td>...</td>\n",
              "      <td>0.627324</td>\n",
              "      <td>0.567694</td>\n",
              "      <td>0.531798</td>\n",
              "      <td>0.000531</td>\n",
              "      <td>-0.012216</td>\n",
              "      <td>-0.010982</td>\n",
              "      <td>0.028078</td>\n",
              "      <td>0.001874</td>\n",
              "      <td>0.005371</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>102</td>\n",
              "      <td>-465.86484</td>\n",
              "      <td>37.754044</td>\n",
              "      <td>11.739282</td>\n",
              "      <td>38.639137</td>\n",
              "      <td>12.334189</td>\n",
              "      <td>17.117222</td>\n",
              "      <td>-5.747991</td>\n",
              "      <td>-0.765464</td>\n",
              "      <td>0.111881</td>\n",
              "      <td>...</td>\n",
              "      <td>0.601236</td>\n",
              "      <td>0.557807</td>\n",
              "      <td>0.523268</td>\n",
              "      <td>0.018790</td>\n",
              "      <td>0.004060</td>\n",
              "      <td>0.101219</td>\n",
              "      <td>-0.063615</td>\n",
              "      <td>0.026759</td>\n",
              "      <td>-0.001490</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>103</td>\n",
              "      <td>-441.11627</td>\n",
              "      <td>55.757492</td>\n",
              "      <td>-10.028031</td>\n",
              "      <td>25.111618</td>\n",
              "      <td>1.534458</td>\n",
              "      <td>6.648944</td>\n",
              "      <td>-16.968105</td>\n",
              "      <td>-9.461746</td>\n",
              "      <td>-11.462216</td>\n",
              "      <td>...</td>\n",
              "      <td>0.539574</td>\n",
              "      <td>0.556530</td>\n",
              "      <td>0.571780</td>\n",
              "      <td>0.000074</td>\n",
              "      <td>-0.019049</td>\n",
              "      <td>0.000770</td>\n",
              "      <td>-0.035572</td>\n",
              "      <td>0.001909</td>\n",
              "      <td>0.012600</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>104</td>\n",
              "      <td>-447.65580</td>\n",
              "      <td>59.147980</td>\n",
              "      <td>10.811775</td>\n",
              "      <td>34.741950</td>\n",
              "      <td>8.662245</td>\n",
              "      <td>23.050500</td>\n",
              "      <td>-5.105643</td>\n",
              "      <td>-0.383535</td>\n",
              "      <td>-7.201605</td>\n",
              "      <td>...</td>\n",
              "      <td>0.526737</td>\n",
              "      <td>0.507366</td>\n",
              "      <td>0.484251</td>\n",
              "      <td>0.008875</td>\n",
              "      <td>-0.001532</td>\n",
              "      <td>0.020466</td>\n",
              "      <td>-0.000596</td>\n",
              "      <td>0.013177</td>\n",
              "      <td>-0.000570</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>105</td>\n",
              "      <td>-506.06097</td>\n",
              "      <td>21.649736</td>\n",
              "      <td>-21.605825</td>\n",
              "      <td>25.993687</td>\n",
              "      <td>-16.120153</td>\n",
              "      <td>-4.204089</td>\n",
              "      <td>-10.385810</td>\n",
              "      <td>-6.951331</td>\n",
              "      <td>-16.107372</td>\n",
              "      <td>...</td>\n",
              "      <td>0.444902</td>\n",
              "      <td>0.483585</td>\n",
              "      <td>0.522145</td>\n",
              "      <td>-0.011355</td>\n",
              "      <td>0.025692</td>\n",
              "      <td>-0.011368</td>\n",
              "      <td>-0.021783</td>\n",
              "      <td>-0.013677</td>\n",
              "      <td>-0.005565</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>106</td>\n",
              "      <td>-392.73694</td>\n",
              "      <td>58.074574</td>\n",
              "      <td>-2.923840</td>\n",
              "      <td>40.999270</td>\n",
              "      <td>-16.602533</td>\n",
              "      <td>12.707182</td>\n",
              "      <td>-17.690449</td>\n",
              "      <td>-5.038430</td>\n",
              "      <td>-16.203160</td>\n",
              "      <td>...</td>\n",
              "      <td>0.431654</td>\n",
              "      <td>0.480828</td>\n",
              "      <td>0.515922</td>\n",
              "      <td>-0.033965</td>\n",
              "      <td>0.000612</td>\n",
              "      <td>0.004869</td>\n",
              "      <td>-0.023429</td>\n",
              "      <td>0.010656</td>\n",
              "      <td>0.009665</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>107 rows × 261 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-00f059c7-a5cb-4376-b7b7-56b131e88497')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-00f059c7-a5cb-4376-b7b7-56b131e88497 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-00f059c7-a5cb-4376-b7b7-56b131e88497');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1d4ea745-39ac-4608-8975-bac6b6997a61\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1d4ea745-39ac-4608-8975-bac6b6997a61')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1d4ea745-39ac-4608-8975-bac6b6997a61 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df2"
            }
          },
          "metadata": {},
          "execution_count": 525
        }
      ],
      "source": [
        "df2=pd.read_csv('/content/drive/MyDrive/DATASETS/EmoDB Dataset/test_fold3.csv')\n",
        "df2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 526,
      "id": "a3d750f7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3d750f7",
        "outputId": "cda5545d-50bb-430b-f4b5-f30df24e8f51"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-482.45233, -469.48477, -434.88647, -454.89886, -447.1263 ,\n",
              "       -433.33972, -465.2206 , -407.61963, -478.92007, -445.90384,\n",
              "       -432.8532 , -449.64343, -448.45706, -447.8849 , -414.28708,\n",
              "       -414.1051 , -415.96158, -425.45212, -403.34033, -434.169  ,\n",
              "       -452.2124 , -491.33237, -433.93875, -456.17935, -381.5805 ,\n",
              "       -454.38022, -437.03818, -508.38065, -440.71924, -525.19977,\n",
              "       -451.39062, -447.41296, -448.62344, -474.65   , -456.05096,\n",
              "       -474.31326, -423.79208, -457.93448, -436.3282 , -439.48215,\n",
              "       -466.99695, -466.81842, -426.0529 , -430.6762 , -482.82642,\n",
              "       -448.18274, -410.33658, -446.7974 , -453.8969 , -521.504  ,\n",
              "       -453.46445, -376.84555, -493.1862 , -442.59415, -446.97107,\n",
              "       -454.48465, -468.04486, -456.72043, -416.0819 , -446.98584,\n",
              "       -418.3227 , -423.0657 , -461.82983, -478.63577, -483.07538,\n",
              "       -425.79813, -435.4896 , -399.51852, -402.0103 , -464.13718,\n",
              "       -474.8092 , -464.07745, -482.67584, -432.09225, -500.27402,\n",
              "       -465.1539 , -424.5544 , -456.9332 , -512.6878 , -469.68045,\n",
              "       -485.50922, -444.93484, -427.06485, -418.001  , -492.80527,\n",
              "       -488.90784, -477.71838, -468.5587 , -456.18542, -503.9724 ,\n",
              "       -439.49243, -447.12366, -437.33456, -504.06378, -514.9054 ,\n",
              "       -418.71957, -414.93512, -460.4001 , -453.48303, -488.23538,\n",
              "       -432.9396 , -449.0555 , -458.41788, -439.2523 , -446.3131 ,\n",
              "       -470.00208, -470.88403, -413.56833, -403.73657, -405.75662,\n",
              "       -377.2399 , -445.18192, -473.9847 , -433.1373 , -390.45312,\n",
              "       -434.17294, -448.68094, -434.1118 , -423.73795, -410.16275,\n",
              "       -447.18866, -406.1605 , -448.96103, -429.79407, -420.2247 ,\n",
              "       -446.1366 , -401.80896, -405.93726, -420.10004, -514.8218 ,\n",
              "       -345.723  , -403.9027 , -434.36496, -472.72705, -364.07712,\n",
              "       -412.15396, -413.8373 , -391.26596, -449.62973, -373.46188,\n",
              "       -427.5882 , -437.44412, -365.9771 , -464.32104, -418.5449 ,\n",
              "       -412.62817, -414.99844, -421.95407, -440.67435, -497.26077,\n",
              "       -463.6061 , -466.10834, -416.39185, -513.68   , -467.4495 ,\n",
              "       -457.78012, -454.6912 , -488.26974, -461.97726, -443.02942,\n",
              "       -464.94745, -460.19077, -451.02377, -452.5104 , -547.34924,\n",
              "       -470.54938, -426.89627, -421.20755, -443.90628, -481.4349 ,\n",
              "       -473.36694, -467.0893 , -407.5873 , -422.1737 , -473.6945 ,\n",
              "       -409.21838, -411.14822, -470.8944 , -479.00146, -413.97577,\n",
              "       -453.5756 , -471.02194, -513.1609 , -517.8032 , -469.223  ,\n",
              "       -479.2676 , -481.2874 , -443.0332 , -410.65417, -513.6494 ,\n",
              "       -441.25516, -434.86633, -458.29306, -462.08606, -435.5582 ,\n",
              "       -464.69373, -470.0053 , -482.287  , -416.1406 , -376.26874,\n",
              "       -501.81232, -413.52936, -444.86087, -418.1163 , -405.00757,\n",
              "       -435.22946, -443.34912, -443.18646, -470.02222, -377.4015 ,\n",
              "       -441.78366, -402.7788 , -467.03882, -457.7515 , -422.8841 ,\n",
              "       -428.71326, -438.01385, -446.06177, -475.37103, -466.87482,\n",
              "       -423.7153 , -420.7548 , -452.5595 , -443.76602, -422.04797,\n",
              "       -464.00873, -409.2057 , -459.8976 , -407.01892, -454.1528 ,\n",
              "       -419.83594, -407.77988, -455.91553, -395.2997 , -453.7782 ,\n",
              "       -393.9526 , -423.10638, -439.494  , -403.3263 , -386.62506,\n",
              "       -428.13434, -496.95926, -421.93167, -471.28308, -464.56476,\n",
              "       -399.32123, -413.09387, -391.08258, -481.6817 , -380.43036,\n",
              "       -365.09543, -410.75797, -433.9074 , -416.07724, -438.8606 ,\n",
              "       -403.1864 , -392.26648, -452.835  , -394.94235, -447.04645,\n",
              "       -389.36047, -387.3086 , -437.94302, -420.1675 , -441.02634,\n",
              "       -412.16312, -438.37827, -409.21655, -412.8278 , -414.18164,\n",
              "       -453.3949 , -406.41467, -427.86765, -468.193  , -444.60718,\n",
              "       -459.23962, -418.8131 , -460.4038 , -439.90536, -465.9359 ,\n",
              "       -445.24417, -484.93997, -446.16428, -367.194  , -481.92627,\n",
              "       -474.5434 , -442.24258, -522.2874 , -423.3954 , -502.14194,\n",
              "       -417.8694 , -461.70255, -470.61038, -491.2516 , -448.38986,\n",
              "       -446.5495 , -459.56256, -426.17914, -504.21188, -464.8163 ,\n",
              "       -454.7633 , -466.36984, -471.5478 , -436.0145 , -489.41888,\n",
              "       -421.56473, -429.95325, -443.65494, -440.50952, -484.456  ,\n",
              "       -428.66187, -424.6715 , -427.8873 , -412.55634, -449.63315,\n",
              "       -463.29285, -379.56708, -441.50995, -440.85495, -454.05173,\n",
              "       -431.52588, -385.76328, -442.1517 , -449.0907 , -412.06152,\n",
              "       -405.81763, -459.4864 , -378.77286, -432.5484 , -462.3047 ,\n",
              "       -434.34827, -407.74323, -386.1994 , -472.4163 , -419.2449 ,\n",
              "       -469.46295, -477.94263, -379.51093, -446.7613 , -415.02478,\n",
              "       -449.23508, -485.03238, -471.95856, -404.7512 , -421.0112 ,\n",
              "       -439.35873, -460.42752, -403.8571 , -412.05164, -372.68307,\n",
              "       -372.10165, -429.20193, -403.01038, -394.76035, -451.00165,\n",
              "       -412.0556 , -408.7152 , -426.08667, -332.69867, -414.32397,\n",
              "       -467.7443 , -428.96658, -398.8699 , -476.0034 , -452.18256,\n",
              "       -456.82553, -468.89825, -463.91916, -397.55426, -408.56897,\n",
              "       -425.47263, -423.54108, -393.92816, -437.02637, -457.2802 ,\n",
              "       -471.60022, -467.06317, -464.36145, -420.7758 , -416.08627,\n",
              "       -392.51147, -542.9345 , -465.24496, -432.88263, -404.11523,\n",
              "       -436.95248, -530.95605, -476.9987 , -411.51193, -492.2898 ,\n",
              "       -455.44485, -441.25778, -477.11566, -457.94778, -524.4419 ,\n",
              "       -422.18552, -358.59656, -458.9268 , -542.5666 , -396.28677,\n",
              "       -423.83383, -408.97797, -390.08295, -427.65176, -462.9359 ,\n",
              "       -450.64505, -386.80893, -435.4015 , -406.23282, -479.6645 ,\n",
              "       -427.612  , -485.59613, -464.9679 , -397.74173, -392.4652 ,\n",
              "       -491.1511 , -442.5056 , -475.25198, -431.76166, -505.94534,\n",
              "       -403.50656, -429.41583, -478.05527, -418.6249 , -427.36716,\n",
              "       -467.15588, -526.1957 , -437.9722 ])"
            ]
          },
          "metadata": {},
          "execution_count": 526
        }
      ],
      "source": [
        "df1['0'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 527,
      "id": "72e890ce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72e890ce",
        "outputId": "b88ea7e7-c3c5-448b-e3fd-c05be43fee72"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-451.45264, -438.46402, -451.66434, -415.74384, -444.03302,\n",
              "       -447.862  , -405.8289 , -440.73105, -390.37305, -446.3631 ,\n",
              "       -433.23328, -494.3395 , -436.69876, -434.9256 , -461.44766,\n",
              "       -503.61353, -453.04343, -409.22214, -531.9213 , -455.4235 ,\n",
              "       -456.90128, -503.28616, -459.55142, -492.041  , -407.6214 ,\n",
              "       -386.1615 , -463.09274, -447.04764, -405.84958, -378.2114 ,\n",
              "       -390.5131 , -470.87045, -433.5633 , -446.96768, -428.3561 ,\n",
              "       -389.66595, -402.585  , -409.9087 , -359.38922, -370.994  ,\n",
              "       -400.4402 , -521.55493, -420.12646, -460.52005, -408.07852,\n",
              "       -462.8844 , -434.42883, -474.72525, -439.98254, -417.20303,\n",
              "       -385.19785, -454.7903 , -382.74545, -484.20596, -423.8322 ,\n",
              "       -392.5137 , -375.1225 , -431.6872 , -405.38095, -473.18396,\n",
              "       -405.67502, -461.75302, -404.93027, -448.45685, -413.7153 ,\n",
              "       -421.18414, -421.31525, -425.45074, -400.93872, -421.23648,\n",
              "       -487.77856, -443.2527 , -487.9756 , -441.62894, -431.91074,\n",
              "       -420.90936, -493.87128, -449.66565, -466.64697, -483.3338 ,\n",
              "       -478.16122, -462.45517, -469.98242, -363.5735 , -414.5706 ,\n",
              "       -435.86343, -442.37482, -440.10852, -393.0507 , -411.78372,\n",
              "       -395.0607 , -404.50937, -412.13974, -432.17923, -404.80206,\n",
              "       -417.062  , -393.34085, -498.8898 , -457.2986 , -411.96274,\n",
              "       -360.47025, -527.49243, -465.86484, -441.11627, -447.6558 ,\n",
              "       -506.06097, -392.73694])"
            ]
          },
          "metadata": {},
          "execution_count": 527
        }
      ],
      "source": [
        "df2['0'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 528,
      "id": "caeabe41",
      "metadata": {
        "id": "caeabe41"
      },
      "outputs": [],
      "source": [
        "x_train=df1.iloc[:,0:(df1.shape[1]-1)]\n",
        "y_train=df1.iloc[:,-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 529,
      "id": "08d5857c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "08d5857c",
        "outputId": "cf9e74d5-7f80-4707-dc50-5c93cc17fd7d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Unnamed: 0          0          1          2          3          4  \\\n",
              "0             0 -482.45233  62.835957  -0.348998  26.264595   2.978607   \n",
              "1             1 -469.48477  88.400730  -7.127512  29.156132   5.335554   \n",
              "2             2 -434.88647  41.972150 -29.416862  18.537344  -4.156565   \n",
              "3             3 -454.89886  45.067265  -0.193278  15.111545   3.080835   \n",
              "4             4 -447.12630  86.114920   4.772520  38.097256   8.324276   \n",
              "..          ...        ...        ...        ...        ...        ...   \n",
              "423         423 -418.62490  57.015880   6.383415  47.618423  -8.051490   \n",
              "424         424 -427.36716  51.473750   4.835125  40.900140   6.937289   \n",
              "425         425 -467.15588  52.217026  10.470471  47.414780   8.690019   \n",
              "426         426 -526.19570  11.317784 -18.942173  29.540787 -28.057306   \n",
              "427         427 -437.97220   5.289146 -22.667547  25.394840 -28.545520   \n",
              "\n",
              "             5          6         7          8  ...       249       250  \\\n",
              "0     6.900927 -13.006243  0.273391  -9.196591  ...  0.635609  0.675823   \n",
              "1     7.147227  -6.727572 -8.307674  -3.364513  ...  0.646426  0.635522   \n",
              "2     5.257353 -11.410935 -8.983023 -11.285996  ...  0.600825  0.648380   \n",
              "3     4.204241 -10.440731 -6.615343 -16.249382  ...  0.506934  0.575784   \n",
              "4     8.538317  -4.507682 -7.680664  -7.317249  ...  0.698287  0.615263   \n",
              "..         ...        ...       ...        ...  ...       ...       ...   \n",
              "423   4.213936 -15.029120 -2.448467 -10.805821  ...  0.566316  0.569263   \n",
              "424  13.700687  -8.331753 -0.583414  -6.279562  ...  0.671834  0.638155   \n",
              "425  15.601270  -2.032935  4.985774  -7.432734  ...  0.531997  0.523647   \n",
              "426   1.299599 -16.251814 -7.512440 -23.191568  ...  0.497285  0.528861   \n",
              "427   0.428451 -11.650926 -8.022680 -18.337156  ...  0.510255  0.495079   \n",
              "\n",
              "          251       252       253       254       255       256       257  \\\n",
              "0    0.684034  0.627376 -0.001574  0.012645 -0.027069  0.019313 -0.002252   \n",
              "1    0.544438  0.532856 -0.018488  0.011973 -0.011038  0.079758 -0.021044   \n",
              "2    0.675295  0.560000 -0.010576 -0.000228  0.011102 -0.074188  0.012116   \n",
              "3    0.523305  0.423375  0.009395 -0.025547 -0.035554 -0.025885  0.011198   \n",
              "4    0.566964  0.605103 -0.008389 -0.051995 -0.056544 -0.020014  0.013964   \n",
              "..        ...       ...       ...       ...       ...       ...       ...   \n",
              "423  0.551809  0.576764  0.016046 -0.025610  0.025426 -0.094371  0.008999   \n",
              "424  0.574542  0.503516  0.019632  0.046315  0.074801  0.018078  0.022948   \n",
              "425  0.509723  0.498566  0.051532  0.015525 -0.006052  0.018201  0.004361   \n",
              "426  0.501070  0.496900  0.007856  0.005917 -0.045393 -0.004246 -0.001628   \n",
              "427  0.539387  0.527394 -0.001086  0.013948  0.013274 -0.056855 -0.001004   \n",
              "\n",
              "          258  \n",
              "0   -0.006220  \n",
              "1   -0.019445  \n",
              "2   -0.000779  \n",
              "3   -0.000163  \n",
              "4    0.008349  \n",
              "..        ...  \n",
              "423 -0.004985  \n",
              "424 -0.016365  \n",
              "425 -0.021649  \n",
              "426  0.015051  \n",
              "427 -0.001355  \n",
              "\n",
              "[428 rows x 260 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-62c03033-4fa6-41d4-88d5-233890f5c454\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>...</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "      <th>257</th>\n",
              "      <th>258</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-482.45233</td>\n",
              "      <td>62.835957</td>\n",
              "      <td>-0.348998</td>\n",
              "      <td>26.264595</td>\n",
              "      <td>2.978607</td>\n",
              "      <td>6.900927</td>\n",
              "      <td>-13.006243</td>\n",
              "      <td>0.273391</td>\n",
              "      <td>-9.196591</td>\n",
              "      <td>...</td>\n",
              "      <td>0.635609</td>\n",
              "      <td>0.675823</td>\n",
              "      <td>0.684034</td>\n",
              "      <td>0.627376</td>\n",
              "      <td>-0.001574</td>\n",
              "      <td>0.012645</td>\n",
              "      <td>-0.027069</td>\n",
              "      <td>0.019313</td>\n",
              "      <td>-0.002252</td>\n",
              "      <td>-0.006220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>-469.48477</td>\n",
              "      <td>88.400730</td>\n",
              "      <td>-7.127512</td>\n",
              "      <td>29.156132</td>\n",
              "      <td>5.335554</td>\n",
              "      <td>7.147227</td>\n",
              "      <td>-6.727572</td>\n",
              "      <td>-8.307674</td>\n",
              "      <td>-3.364513</td>\n",
              "      <td>...</td>\n",
              "      <td>0.646426</td>\n",
              "      <td>0.635522</td>\n",
              "      <td>0.544438</td>\n",
              "      <td>0.532856</td>\n",
              "      <td>-0.018488</td>\n",
              "      <td>0.011973</td>\n",
              "      <td>-0.011038</td>\n",
              "      <td>0.079758</td>\n",
              "      <td>-0.021044</td>\n",
              "      <td>-0.019445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>-434.88647</td>\n",
              "      <td>41.972150</td>\n",
              "      <td>-29.416862</td>\n",
              "      <td>18.537344</td>\n",
              "      <td>-4.156565</td>\n",
              "      <td>5.257353</td>\n",
              "      <td>-11.410935</td>\n",
              "      <td>-8.983023</td>\n",
              "      <td>-11.285996</td>\n",
              "      <td>...</td>\n",
              "      <td>0.600825</td>\n",
              "      <td>0.648380</td>\n",
              "      <td>0.675295</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>-0.010576</td>\n",
              "      <td>-0.000228</td>\n",
              "      <td>0.011102</td>\n",
              "      <td>-0.074188</td>\n",
              "      <td>0.012116</td>\n",
              "      <td>-0.000779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>-454.89886</td>\n",
              "      <td>45.067265</td>\n",
              "      <td>-0.193278</td>\n",
              "      <td>15.111545</td>\n",
              "      <td>3.080835</td>\n",
              "      <td>4.204241</td>\n",
              "      <td>-10.440731</td>\n",
              "      <td>-6.615343</td>\n",
              "      <td>-16.249382</td>\n",
              "      <td>...</td>\n",
              "      <td>0.506934</td>\n",
              "      <td>0.575784</td>\n",
              "      <td>0.523305</td>\n",
              "      <td>0.423375</td>\n",
              "      <td>0.009395</td>\n",
              "      <td>-0.025547</td>\n",
              "      <td>-0.035554</td>\n",
              "      <td>-0.025885</td>\n",
              "      <td>0.011198</td>\n",
              "      <td>-0.000163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>-447.12630</td>\n",
              "      <td>86.114920</td>\n",
              "      <td>4.772520</td>\n",
              "      <td>38.097256</td>\n",
              "      <td>8.324276</td>\n",
              "      <td>8.538317</td>\n",
              "      <td>-4.507682</td>\n",
              "      <td>-7.680664</td>\n",
              "      <td>-7.317249</td>\n",
              "      <td>...</td>\n",
              "      <td>0.698287</td>\n",
              "      <td>0.615263</td>\n",
              "      <td>0.566964</td>\n",
              "      <td>0.605103</td>\n",
              "      <td>-0.008389</td>\n",
              "      <td>-0.051995</td>\n",
              "      <td>-0.056544</td>\n",
              "      <td>-0.020014</td>\n",
              "      <td>0.013964</td>\n",
              "      <td>0.008349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>423</th>\n",
              "      <td>423</td>\n",
              "      <td>-418.62490</td>\n",
              "      <td>57.015880</td>\n",
              "      <td>6.383415</td>\n",
              "      <td>47.618423</td>\n",
              "      <td>-8.051490</td>\n",
              "      <td>4.213936</td>\n",
              "      <td>-15.029120</td>\n",
              "      <td>-2.448467</td>\n",
              "      <td>-10.805821</td>\n",
              "      <td>...</td>\n",
              "      <td>0.566316</td>\n",
              "      <td>0.569263</td>\n",
              "      <td>0.551809</td>\n",
              "      <td>0.576764</td>\n",
              "      <td>0.016046</td>\n",
              "      <td>-0.025610</td>\n",
              "      <td>0.025426</td>\n",
              "      <td>-0.094371</td>\n",
              "      <td>0.008999</td>\n",
              "      <td>-0.004985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424</th>\n",
              "      <td>424</td>\n",
              "      <td>-427.36716</td>\n",
              "      <td>51.473750</td>\n",
              "      <td>4.835125</td>\n",
              "      <td>40.900140</td>\n",
              "      <td>6.937289</td>\n",
              "      <td>13.700687</td>\n",
              "      <td>-8.331753</td>\n",
              "      <td>-0.583414</td>\n",
              "      <td>-6.279562</td>\n",
              "      <td>...</td>\n",
              "      <td>0.671834</td>\n",
              "      <td>0.638155</td>\n",
              "      <td>0.574542</td>\n",
              "      <td>0.503516</td>\n",
              "      <td>0.019632</td>\n",
              "      <td>0.046315</td>\n",
              "      <td>0.074801</td>\n",
              "      <td>0.018078</td>\n",
              "      <td>0.022948</td>\n",
              "      <td>-0.016365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>425</th>\n",
              "      <td>425</td>\n",
              "      <td>-467.15588</td>\n",
              "      <td>52.217026</td>\n",
              "      <td>10.470471</td>\n",
              "      <td>47.414780</td>\n",
              "      <td>8.690019</td>\n",
              "      <td>15.601270</td>\n",
              "      <td>-2.032935</td>\n",
              "      <td>4.985774</td>\n",
              "      <td>-7.432734</td>\n",
              "      <td>...</td>\n",
              "      <td>0.531997</td>\n",
              "      <td>0.523647</td>\n",
              "      <td>0.509723</td>\n",
              "      <td>0.498566</td>\n",
              "      <td>0.051532</td>\n",
              "      <td>0.015525</td>\n",
              "      <td>-0.006052</td>\n",
              "      <td>0.018201</td>\n",
              "      <td>0.004361</td>\n",
              "      <td>-0.021649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>426</th>\n",
              "      <td>426</td>\n",
              "      <td>-526.19570</td>\n",
              "      <td>11.317784</td>\n",
              "      <td>-18.942173</td>\n",
              "      <td>29.540787</td>\n",
              "      <td>-28.057306</td>\n",
              "      <td>1.299599</td>\n",
              "      <td>-16.251814</td>\n",
              "      <td>-7.512440</td>\n",
              "      <td>-23.191568</td>\n",
              "      <td>...</td>\n",
              "      <td>0.497285</td>\n",
              "      <td>0.528861</td>\n",
              "      <td>0.501070</td>\n",
              "      <td>0.496900</td>\n",
              "      <td>0.007856</td>\n",
              "      <td>0.005917</td>\n",
              "      <td>-0.045393</td>\n",
              "      <td>-0.004246</td>\n",
              "      <td>-0.001628</td>\n",
              "      <td>0.015051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>427</th>\n",
              "      <td>427</td>\n",
              "      <td>-437.97220</td>\n",
              "      <td>5.289146</td>\n",
              "      <td>-22.667547</td>\n",
              "      <td>25.394840</td>\n",
              "      <td>-28.545520</td>\n",
              "      <td>0.428451</td>\n",
              "      <td>-11.650926</td>\n",
              "      <td>-8.022680</td>\n",
              "      <td>-18.337156</td>\n",
              "      <td>...</td>\n",
              "      <td>0.510255</td>\n",
              "      <td>0.495079</td>\n",
              "      <td>0.539387</td>\n",
              "      <td>0.527394</td>\n",
              "      <td>-0.001086</td>\n",
              "      <td>0.013948</td>\n",
              "      <td>0.013274</td>\n",
              "      <td>-0.056855</td>\n",
              "      <td>-0.001004</td>\n",
              "      <td>-0.001355</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>428 rows × 260 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-62c03033-4fa6-41d4-88d5-233890f5c454')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-62c03033-4fa6-41d4-88d5-233890f5c454 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-62c03033-4fa6-41d4-88d5-233890f5c454');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5ac5d98c-0387-4dac-898e-19b3a3eee449\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5ac5d98c-0387-4dac-898e-19b3a3eee449')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5ac5d98c-0387-4dac-898e-19b3a3eee449 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "x_train"
            }
          },
          "metadata": {},
          "execution_count": 529
        }
      ],
      "source": [
        "x_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 530,
      "id": "d2127f10",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2127f10",
        "outputId": "9acf3d81-ead1-4c7c-c748-e6022d10cd9d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      0\n",
              "1      1\n",
              "2      2\n",
              "3      0\n",
              "4      1\n",
              "      ..\n",
              "423    5\n",
              "424    3\n",
              "425    3\n",
              "426    2\n",
              "427    2\n",
              "Name: 0.1, Length: 428, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 530
        }
      ],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 531,
      "id": "7311d746",
      "metadata": {
        "id": "7311d746"
      },
      "outputs": [],
      "source": [
        "x_test=df2.iloc[:,0:(df2.shape[1]-1)]\n",
        "y_test=df2.iloc[:,-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 532,
      "id": "ae86f98c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "ae86f98c",
        "outputId": "5b90788f-2b63-4c89-88ba-cd5c8982d44d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Unnamed: 0          0          1          2          3          4  \\\n",
              "0             0 -451.45264  71.335200  12.223010  32.649555   3.997801   \n",
              "1             1 -438.46402  94.930030   5.689025  25.277182   2.760727   \n",
              "2             2 -451.66434  41.310406 -20.668978   6.190408 -11.533198   \n",
              "3             3 -415.74384  36.454792  -0.837229  20.279540  -5.861744   \n",
              "4             4 -444.03302  48.236977   7.208580  24.032001   0.882425   \n",
              "..          ...        ...        ...        ...        ...        ...   \n",
              "102         102 -465.86484  37.754044  11.739282  38.639137  12.334189   \n",
              "103         103 -441.11627  55.757492 -10.028031  25.111618   1.534458   \n",
              "104         104 -447.65580  59.147980  10.811775  34.741950   8.662245   \n",
              "105         105 -506.06097  21.649736 -21.605825  25.993687 -16.120153   \n",
              "106         106 -392.73694  58.074574  -2.923840  40.999270 -16.602533   \n",
              "\n",
              "             5          6         7          8  ...       249       250  \\\n",
              "0    15.200773  -2.812883 -1.384373  -6.467040  ...  0.696758  0.719856   \n",
              "1    18.562307  -5.944632 -2.279853  -1.935391  ...  0.693280  0.663641   \n",
              "2     6.807915 -13.584462 -4.452075 -20.147266  ...  0.521494  0.528094   \n",
              "3     3.429390 -12.954763 -4.034247 -20.398386  ...  0.605959  0.656849   \n",
              "4     4.175030  -7.778720 -0.999663 -14.226709  ...  0.622825  0.627324   \n",
              "..         ...        ...       ...        ...  ...       ...       ...   \n",
              "102  17.117222  -5.747991 -0.765464   0.111881  ...  0.744749  0.601236   \n",
              "103   6.648944 -16.968105 -9.461746 -11.462216  ...  0.544072  0.539574   \n",
              "104  23.050500  -5.105643 -0.383535  -7.201605  ...  0.605310  0.526737   \n",
              "105  -4.204089 -10.385810 -6.951331 -16.107372  ...  0.449636  0.444902   \n",
              "106  12.707182 -17.690449 -5.038430 -16.203160  ...  0.413691  0.431654   \n",
              "\n",
              "          251       252       253       254       255       256       257  \\\n",
              "0    0.667424  0.570905  0.003044 -0.006101 -0.009038 -0.049699  0.027615   \n",
              "1    0.593349  0.568150 -0.007860 -0.022793 -0.053155 -0.042559 -0.019416   \n",
              "2    0.548985  0.598431  0.019574 -0.013364 -0.001339  0.002642  0.000159   \n",
              "3    0.632091  0.612582  0.016577  0.009726  0.025019 -0.011375 -0.012012   \n",
              "4    0.567694  0.531798  0.000531 -0.012216 -0.010982  0.028078  0.001874   \n",
              "..        ...       ...       ...       ...       ...       ...       ...   \n",
              "102  0.557807  0.523268  0.018790  0.004060  0.101219 -0.063615  0.026759   \n",
              "103  0.556530  0.571780  0.000074 -0.019049  0.000770 -0.035572  0.001909   \n",
              "104  0.507366  0.484251  0.008875 -0.001532  0.020466 -0.000596  0.013177   \n",
              "105  0.483585  0.522145 -0.011355  0.025692 -0.011368 -0.021783 -0.013677   \n",
              "106  0.480828  0.515922 -0.033965  0.000612  0.004869 -0.023429  0.010656   \n",
              "\n",
              "          258  \n",
              "0    0.021319  \n",
              "1    0.022956  \n",
              "2   -0.011073  \n",
              "3   -0.002431  \n",
              "4    0.005371  \n",
              "..        ...  \n",
              "102 -0.001490  \n",
              "103  0.012600  \n",
              "104 -0.000570  \n",
              "105 -0.005565  \n",
              "106  0.009665  \n",
              "\n",
              "[107 rows x 260 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fa803e10-0ba2-4054-afad-658c26f20d47\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>...</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "      <th>257</th>\n",
              "      <th>258</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-451.45264</td>\n",
              "      <td>71.335200</td>\n",
              "      <td>12.223010</td>\n",
              "      <td>32.649555</td>\n",
              "      <td>3.997801</td>\n",
              "      <td>15.200773</td>\n",
              "      <td>-2.812883</td>\n",
              "      <td>-1.384373</td>\n",
              "      <td>-6.467040</td>\n",
              "      <td>...</td>\n",
              "      <td>0.696758</td>\n",
              "      <td>0.719856</td>\n",
              "      <td>0.667424</td>\n",
              "      <td>0.570905</td>\n",
              "      <td>0.003044</td>\n",
              "      <td>-0.006101</td>\n",
              "      <td>-0.009038</td>\n",
              "      <td>-0.049699</td>\n",
              "      <td>0.027615</td>\n",
              "      <td>0.021319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>-438.46402</td>\n",
              "      <td>94.930030</td>\n",
              "      <td>5.689025</td>\n",
              "      <td>25.277182</td>\n",
              "      <td>2.760727</td>\n",
              "      <td>18.562307</td>\n",
              "      <td>-5.944632</td>\n",
              "      <td>-2.279853</td>\n",
              "      <td>-1.935391</td>\n",
              "      <td>...</td>\n",
              "      <td>0.693280</td>\n",
              "      <td>0.663641</td>\n",
              "      <td>0.593349</td>\n",
              "      <td>0.568150</td>\n",
              "      <td>-0.007860</td>\n",
              "      <td>-0.022793</td>\n",
              "      <td>-0.053155</td>\n",
              "      <td>-0.042559</td>\n",
              "      <td>-0.019416</td>\n",
              "      <td>0.022956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>-451.66434</td>\n",
              "      <td>41.310406</td>\n",
              "      <td>-20.668978</td>\n",
              "      <td>6.190408</td>\n",
              "      <td>-11.533198</td>\n",
              "      <td>6.807915</td>\n",
              "      <td>-13.584462</td>\n",
              "      <td>-4.452075</td>\n",
              "      <td>-20.147266</td>\n",
              "      <td>...</td>\n",
              "      <td>0.521494</td>\n",
              "      <td>0.528094</td>\n",
              "      <td>0.548985</td>\n",
              "      <td>0.598431</td>\n",
              "      <td>0.019574</td>\n",
              "      <td>-0.013364</td>\n",
              "      <td>-0.001339</td>\n",
              "      <td>0.002642</td>\n",
              "      <td>0.000159</td>\n",
              "      <td>-0.011073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>-415.74384</td>\n",
              "      <td>36.454792</td>\n",
              "      <td>-0.837229</td>\n",
              "      <td>20.279540</td>\n",
              "      <td>-5.861744</td>\n",
              "      <td>3.429390</td>\n",
              "      <td>-12.954763</td>\n",
              "      <td>-4.034247</td>\n",
              "      <td>-20.398386</td>\n",
              "      <td>...</td>\n",
              "      <td>0.605959</td>\n",
              "      <td>0.656849</td>\n",
              "      <td>0.632091</td>\n",
              "      <td>0.612582</td>\n",
              "      <td>0.016577</td>\n",
              "      <td>0.009726</td>\n",
              "      <td>0.025019</td>\n",
              "      <td>-0.011375</td>\n",
              "      <td>-0.012012</td>\n",
              "      <td>-0.002431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>-444.03302</td>\n",
              "      <td>48.236977</td>\n",
              "      <td>7.208580</td>\n",
              "      <td>24.032001</td>\n",
              "      <td>0.882425</td>\n",
              "      <td>4.175030</td>\n",
              "      <td>-7.778720</td>\n",
              "      <td>-0.999663</td>\n",
              "      <td>-14.226709</td>\n",
              "      <td>...</td>\n",
              "      <td>0.622825</td>\n",
              "      <td>0.627324</td>\n",
              "      <td>0.567694</td>\n",
              "      <td>0.531798</td>\n",
              "      <td>0.000531</td>\n",
              "      <td>-0.012216</td>\n",
              "      <td>-0.010982</td>\n",
              "      <td>0.028078</td>\n",
              "      <td>0.001874</td>\n",
              "      <td>0.005371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>102</td>\n",
              "      <td>-465.86484</td>\n",
              "      <td>37.754044</td>\n",
              "      <td>11.739282</td>\n",
              "      <td>38.639137</td>\n",
              "      <td>12.334189</td>\n",
              "      <td>17.117222</td>\n",
              "      <td>-5.747991</td>\n",
              "      <td>-0.765464</td>\n",
              "      <td>0.111881</td>\n",
              "      <td>...</td>\n",
              "      <td>0.744749</td>\n",
              "      <td>0.601236</td>\n",
              "      <td>0.557807</td>\n",
              "      <td>0.523268</td>\n",
              "      <td>0.018790</td>\n",
              "      <td>0.004060</td>\n",
              "      <td>0.101219</td>\n",
              "      <td>-0.063615</td>\n",
              "      <td>0.026759</td>\n",
              "      <td>-0.001490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>103</td>\n",
              "      <td>-441.11627</td>\n",
              "      <td>55.757492</td>\n",
              "      <td>-10.028031</td>\n",
              "      <td>25.111618</td>\n",
              "      <td>1.534458</td>\n",
              "      <td>6.648944</td>\n",
              "      <td>-16.968105</td>\n",
              "      <td>-9.461746</td>\n",
              "      <td>-11.462216</td>\n",
              "      <td>...</td>\n",
              "      <td>0.544072</td>\n",
              "      <td>0.539574</td>\n",
              "      <td>0.556530</td>\n",
              "      <td>0.571780</td>\n",
              "      <td>0.000074</td>\n",
              "      <td>-0.019049</td>\n",
              "      <td>0.000770</td>\n",
              "      <td>-0.035572</td>\n",
              "      <td>0.001909</td>\n",
              "      <td>0.012600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>104</td>\n",
              "      <td>-447.65580</td>\n",
              "      <td>59.147980</td>\n",
              "      <td>10.811775</td>\n",
              "      <td>34.741950</td>\n",
              "      <td>8.662245</td>\n",
              "      <td>23.050500</td>\n",
              "      <td>-5.105643</td>\n",
              "      <td>-0.383535</td>\n",
              "      <td>-7.201605</td>\n",
              "      <td>...</td>\n",
              "      <td>0.605310</td>\n",
              "      <td>0.526737</td>\n",
              "      <td>0.507366</td>\n",
              "      <td>0.484251</td>\n",
              "      <td>0.008875</td>\n",
              "      <td>-0.001532</td>\n",
              "      <td>0.020466</td>\n",
              "      <td>-0.000596</td>\n",
              "      <td>0.013177</td>\n",
              "      <td>-0.000570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>105</td>\n",
              "      <td>-506.06097</td>\n",
              "      <td>21.649736</td>\n",
              "      <td>-21.605825</td>\n",
              "      <td>25.993687</td>\n",
              "      <td>-16.120153</td>\n",
              "      <td>-4.204089</td>\n",
              "      <td>-10.385810</td>\n",
              "      <td>-6.951331</td>\n",
              "      <td>-16.107372</td>\n",
              "      <td>...</td>\n",
              "      <td>0.449636</td>\n",
              "      <td>0.444902</td>\n",
              "      <td>0.483585</td>\n",
              "      <td>0.522145</td>\n",
              "      <td>-0.011355</td>\n",
              "      <td>0.025692</td>\n",
              "      <td>-0.011368</td>\n",
              "      <td>-0.021783</td>\n",
              "      <td>-0.013677</td>\n",
              "      <td>-0.005565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>106</td>\n",
              "      <td>-392.73694</td>\n",
              "      <td>58.074574</td>\n",
              "      <td>-2.923840</td>\n",
              "      <td>40.999270</td>\n",
              "      <td>-16.602533</td>\n",
              "      <td>12.707182</td>\n",
              "      <td>-17.690449</td>\n",
              "      <td>-5.038430</td>\n",
              "      <td>-16.203160</td>\n",
              "      <td>...</td>\n",
              "      <td>0.413691</td>\n",
              "      <td>0.431654</td>\n",
              "      <td>0.480828</td>\n",
              "      <td>0.515922</td>\n",
              "      <td>-0.033965</td>\n",
              "      <td>0.000612</td>\n",
              "      <td>0.004869</td>\n",
              "      <td>-0.023429</td>\n",
              "      <td>0.010656</td>\n",
              "      <td>0.009665</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>107 rows × 260 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fa803e10-0ba2-4054-afad-658c26f20d47')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fa803e10-0ba2-4054-afad-658c26f20d47 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fa803e10-0ba2-4054-afad-658c26f20d47');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-26006da3-ae0b-40e0-8190-dd98f057c2c0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-26006da3-ae0b-40e0-8190-dd98f057c2c0')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-26006da3-ae0b-40e0-8190-dd98f057c2c0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "x_test"
            }
          },
          "metadata": {},
          "execution_count": 532
        }
      ],
      "source": [
        "x_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 533,
      "id": "62579094",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62579094",
        "outputId": "91a0baad-f049-4a81-fc7a-0e3ddb31c0dc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      3\n",
              "1      3\n",
              "2      2\n",
              "3      4\n",
              "4      0\n",
              "      ..\n",
              "102    3\n",
              "103    6\n",
              "104    3\n",
              "105    0\n",
              "106    5\n",
              "Name: 0.1, Length: 107, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 533
        }
      ],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 534,
      "id": "3beb4ef2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "3beb4ef2",
        "outputId": "8f520159-e0ef-445c-c3b5-ed0776b2c93e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: ylabel='count'>"
            ]
          },
          "metadata": {},
          "execution_count": 534
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "sns.countplot(df1['0'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 535,
      "id": "d58a5824",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "d58a5824",
        "outputId": "6c9d9df0-3e98-4e64-c835-c7711da3654b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: ylabel='count'>"
            ]
          },
          "metadata": {},
          "execution_count": 535
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "sns.countplot(df2['0'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 536,
      "id": "f4bf0f42",
      "metadata": {
        "id": "f4bf0f42"
      },
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 537,
      "id": "5bd3a370",
      "metadata": {
        "id": "5bd3a370"
      },
      "outputs": [],
      "source": [
        "# x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 538,
      "id": "7b9f98bc",
      "metadata": {
        "id": "7b9f98bc"
      },
      "outputs": [],
      "source": [
        "# x_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 539,
      "id": "c8d669e4",
      "metadata": {
        "id": "c8d669e4"
      },
      "outputs": [],
      "source": [
        "# y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 540,
      "id": "3e5f00f5",
      "metadata": {
        "id": "3e5f00f5"
      },
      "outputs": [],
      "source": [
        "# x_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 541,
      "id": "6d201bc1",
      "metadata": {
        "id": "6d201bc1"
      },
      "outputs": [],
      "source": [
        "# y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 542,
      "id": "000082a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "000082a7",
        "outputId": "aec0cf1f-7d07-46e3-d6fb-cbdf3eaf23e4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 2, 4, 0, 1, 6, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 542
        }
      ],
      "source": [
        "#Check unique values for y_test\n",
        "y_test.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 543,
      "id": "0c50dd63",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c50dd63",
        "outputId": "3d129dcf-2448-4c0a-ff0b-9b04be0ef152"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 4, 5, 3, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 543
        }
      ],
      "source": [
        "#Check unique values for y_train\n",
        "y_train.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 544,
      "id": "2f69e286",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f69e286",
        "outputId": "f40cf777-cded-481f-83c6-6cca8528855d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 1.0726817042606516,\n",
              " 1: 0.9705215419501134,\n",
              " 2: 0.5994397759103641,\n",
              " 3: 1.2478134110787171,\n",
              " 4: 1.1116883116883116,\n",
              " 5: 0.9406593406593406,\n",
              " 6: 1.6525096525096525}"
            ]
          },
          "metadata": {},
          "execution_count": 544
        }
      ],
      "source": [
        "from sklearn.utils import compute_class_weight\n",
        "class_weights = compute_class_weight(\n",
        "                                        class_weight = \"balanced\",\n",
        "                                        classes = np.unique(y_train),\n",
        "                                        y = y_train\n",
        "                                    )\n",
        "class_weights = dict(zip(np.unique(y_train), class_weights))\n",
        "class_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 545,
      "id": "87b553a9",
      "metadata": {
        "id": "87b553a9"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "#Normalize the data\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(x_train)\n",
        "X_train_scalled = scaler.transform(x_train)\n",
        "X_test_scalled = scaler.transform(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 546,
      "id": "7b62969d",
      "metadata": {
        "id": "7b62969d"
      },
      "outputs": [],
      "source": [
        "#Import packages for CNN\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, Conv1D\n",
        "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, BatchNormalization, Flatten, MaxPooling1D\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.regularizers import l2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 547,
      "id": "c598744f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c598744f",
        "outputId": "f18182b2-8693-474b-ef18-d60cad952892"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[958, 407, 808, 376]\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "res = []\n",
        "for j in range(4):\n",
        "    res.append(random.randint(300, 1000))\n",
        "# res.sort(reverse=True)\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(260, 1)))\n",
        "model.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='sigmoid'))\n",
        "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# model.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='sigmoid'))\n",
        "# model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Dropout(0.2))\n",
        "\n",
        "# model.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='sigmoid'))\n",
        "# model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Dropout(0.3))\n",
        "##\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "model.add(LSTM(256))\n",
        "\n",
        "\n",
        "model.add(Dense(256, activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(256, activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "##\n",
        "# model.add(Dense(64, activation='sigmoid'))\n",
        "# # model.add(BatchNormalization())\n",
        "# model.add(Dropout(0.3))\n",
        "\n",
        "# model.add(Dense(32, activation='sigmoid'))\n",
        "# # model.add(BatchNormalization())\n",
        "# model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(7, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Q6P4AQ4Mrdt",
        "outputId": "2a469d8e-1999-44b7-a14b-f5feea66ede5"
      },
      "id": "_Q6P4AQ4Mrdt",
      "execution_count": 548,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_40 (Conv1D)          (None, 260, 256)          1536      \n",
            "                                                                 \n",
            " max_pooling1d_40 (MaxPooli  (None, 130, 256)          0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_40 (Ba  (None, 130, 256)          1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv1d_41 (Conv1D)          (None, 130, 256)          327936    \n",
            "                                                                 \n",
            " max_pooling1d_41 (MaxPooli  (None, 65, 256)           0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_41 (Ba  (None, 65, 256)           1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_40 (Dropout)        (None, 65, 256)           0         \n",
            "                                                                 \n",
            " conv1d_42 (Conv1D)          (None, 65, 256)           327936    \n",
            "                                                                 \n",
            " max_pooling1d_42 (MaxPooli  (None, 33, 256)           0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_42 (Ba  (None, 33, 256)           1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_41 (Dropout)        (None, 33, 256)           0         \n",
            "                                                                 \n",
            " conv1d_43 (Conv1D)          (None, 33, 128)           163968    \n",
            "                                                                 \n",
            " max_pooling1d_43 (MaxPooli  (None, 17, 128)           0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_43 (Ba  (None, 17, 128)           512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_42 (Dropout)        (None, 17, 128)           0         \n",
            "                                                                 \n",
            " lstm_20 (LSTM)              (None, 17, 256)           394240    \n",
            "                                                                 \n",
            " lstm_21 (LSTM)              (None, 256)               525312    \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dropout_43 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 7)                 1799      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1877895 (7.16 MB)\n",
            "Trainable params: 1876103 (7.16 MB)\n",
            "Non-trainable params: 1792 (7.00 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tempfile\n",
        "initial_weights = os.path.join(tempfile.mkdtemp(), 'initial_weights')\n",
        "model.save_weights(initial_weights)"
      ],
      "metadata": {
        "id": "JCEBlr_YMzuM"
      },
      "id": "JCEBlr_YMzuM",
      "execution_count": 549,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(initial_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8l3JMDetM3-D",
        "outputId": "101eba83-c066-4a42-897d-efef4ea52738"
      },
      "id": "8l3JMDetM3-D",
      "execution_count": 550,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7a51f567bfa0>"
            ]
          },
          "metadata": {},
          "execution_count": 550
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimiser = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=optimiser,\n",
        "              loss='sparse_categorical_crossentropy',                             #CategoricalCrossentropy\n",
        "              metrics=['SparseCategoricalAccuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WglAV3TMM7KM",
        "outputId": "234713fd-09bf-4946-a1d6-7f4510146e47"
      },
      "id": "WglAV3TMM7KM",
      "execution_count": 551,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_40 (Conv1D)          (None, 260, 256)          1536      \n",
            "                                                                 \n",
            " max_pooling1d_40 (MaxPooli  (None, 130, 256)          0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_40 (Ba  (None, 130, 256)          1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv1d_41 (Conv1D)          (None, 130, 256)          327936    \n",
            "                                                                 \n",
            " max_pooling1d_41 (MaxPooli  (None, 65, 256)           0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_41 (Ba  (None, 65, 256)           1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_40 (Dropout)        (None, 65, 256)           0         \n",
            "                                                                 \n",
            " conv1d_42 (Conv1D)          (None, 65, 256)           327936    \n",
            "                                                                 \n",
            " max_pooling1d_42 (MaxPooli  (None, 33, 256)           0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_42 (Ba  (None, 33, 256)           1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_41 (Dropout)        (None, 33, 256)           0         \n",
            "                                                                 \n",
            " conv1d_43 (Conv1D)          (None, 33, 128)           163968    \n",
            "                                                                 \n",
            " max_pooling1d_43 (MaxPooli  (None, 17, 128)           0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_43 (Ba  (None, 17, 128)           512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_42 (Dropout)        (None, 17, 128)           0         \n",
            "                                                                 \n",
            " lstm_20 (LSTM)              (None, 17, 256)           394240    \n",
            "                                                                 \n",
            " lstm_21 (LSTM)              (None, 256)               525312    \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dropout_43 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 7)                 1799      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1877895 (7.16 MB)\n",
            "Trainable params: 1876103 (7.16 MB)\n",
            "Non-trainable params: 1792 (7.00 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path='cnn_lstm_emodb3.ckpt'\n",
        "checkpoint_dir=os.path.dirname(checkpoint_path)\n",
        "callback1=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, monitor='val_sparse_categorical_accuracy', verbose=1,\n",
        "   save_best_only=True,save_weights_only=True,)\n",
        "callback2=tf.keras.callbacks.EarlyStopping(monitor='val_sparse_categorical_accuracy',min_delta=0, patience=50, verbose=0, mode='auto',baseline=None,restore_best_weights=True)\n",
        "cp_callback=[callback1,callback2]"
      ],
      "metadata": {
        "id": "Kg0_S4kRM8DV"
      },
      "id": "Kg0_S4kRM8DV",
      "execution_count": 552,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train_scalled, y_train, validation_data=(X_test_scalled, y_test), batch_size=64, epochs=900, verbose=1,class_weight=class_weights,callbacks=cp_callback)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giDUxkTnNGXy",
        "outputId": "30ab1e04-0207-460d-b51b-981fdce1bca8"
      },
      "id": "giDUxkTnNGXy",
      "execution_count": 553,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.9241 - sparse_categorical_accuracy: 0.2570\n",
            "Epoch 1: val_sparse_categorical_accuracy improved from -inf to 0.14953, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 9s 205ms/step - loss: 1.9241 - sparse_categorical_accuracy: 0.2570 - val_loss: 1.9402 - val_sparse_categorical_accuracy: 0.1495\n",
            "Epoch 2/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.8677 - sparse_categorical_accuracy: 0.2804\n",
            "Epoch 2: val_sparse_categorical_accuracy did not improve from 0.14953\n",
            "7/7 [==============================] - 0s 30ms/step - loss: 1.8677 - sparse_categorical_accuracy: 0.2804 - val_loss: 1.9408 - val_sparse_categorical_accuracy: 0.1495\n",
            "Epoch 3/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.8267 - sparse_categorical_accuracy: 0.2827\n",
            "Epoch 3: val_sparse_categorical_accuracy did not improve from 0.14953\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 1.8267 - sparse_categorical_accuracy: 0.2827 - val_loss: 1.9413 - val_sparse_categorical_accuracy: 0.1495\n",
            "Epoch 4/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.7927 - sparse_categorical_accuracy: 0.2850\n",
            "Epoch 4: val_sparse_categorical_accuracy did not improve from 0.14953\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 1.7927 - sparse_categorical_accuracy: 0.2850 - val_loss: 1.9412 - val_sparse_categorical_accuracy: 0.1495\n",
            "Epoch 5/900\n",
            "5/7 [====================>.........] - ETA: 0s - loss: 1.7614 - sparse_categorical_accuracy: 0.3313\n",
            "Epoch 5: val_sparse_categorical_accuracy improved from 0.14953 to 0.17757, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 1.7569 - sparse_categorical_accuracy: 0.3154 - val_loss: 1.9421 - val_sparse_categorical_accuracy: 0.1776\n",
            "Epoch 6/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.7234 - sparse_categorical_accuracy: 0.3411\n",
            "Epoch 6: val_sparse_categorical_accuracy did not improve from 0.17757\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 1.7234 - sparse_categorical_accuracy: 0.3411 - val_loss: 1.9447 - val_sparse_categorical_accuracy: 0.1682\n",
            "Epoch 7/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.6737 - sparse_categorical_accuracy: 0.3668\n",
            "Epoch 7: val_sparse_categorical_accuracy did not improve from 0.17757\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 1.6737 - sparse_categorical_accuracy: 0.3668 - val_loss: 1.9471 - val_sparse_categorical_accuracy: 0.1682\n",
            "Epoch 8/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.6230 - sparse_categorical_accuracy: 0.3949\n",
            "Epoch 8: val_sparse_categorical_accuracy did not improve from 0.17757\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 1.6230 - sparse_categorical_accuracy: 0.3949 - val_loss: 1.9543 - val_sparse_categorical_accuracy: 0.1589\n",
            "Epoch 9/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.5529 - sparse_categorical_accuracy: 0.4369\n",
            "Epoch 9: val_sparse_categorical_accuracy did not improve from 0.17757\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 1.5529 - sparse_categorical_accuracy: 0.4369 - val_loss: 1.9682 - val_sparse_categorical_accuracy: 0.1495\n",
            "Epoch 10/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.4641 - sparse_categorical_accuracy: 0.4766\n",
            "Epoch 10: val_sparse_categorical_accuracy did not improve from 0.17757\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 1.4641 - sparse_categorical_accuracy: 0.4766 - val_loss: 1.9856 - val_sparse_categorical_accuracy: 0.1589\n",
            "Epoch 11/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.3792 - sparse_categorical_accuracy: 0.4790\n",
            "Epoch 11: val_sparse_categorical_accuracy did not improve from 0.17757\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 1.3792 - sparse_categorical_accuracy: 0.4790 - val_loss: 2.0155 - val_sparse_categorical_accuracy: 0.1589\n",
            "Epoch 12/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.2845 - sparse_categorical_accuracy: 0.5234\n",
            "Epoch 12: val_sparse_categorical_accuracy did not improve from 0.17757\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 1.2845 - sparse_categorical_accuracy: 0.5234 - val_loss: 2.0635 - val_sparse_categorical_accuracy: 0.1776\n",
            "Epoch 13/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.2161 - sparse_categorical_accuracy: 0.5397\n",
            "Epoch 13: val_sparse_categorical_accuracy did not improve from 0.17757\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 1.2161 - sparse_categorical_accuracy: 0.5397 - val_loss: 2.1366 - val_sparse_categorical_accuracy: 0.1589\n",
            "Epoch 14/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.1442 - sparse_categorical_accuracy: 0.5678\n",
            "Epoch 14: val_sparse_categorical_accuracy did not improve from 0.17757\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 1.1442 - sparse_categorical_accuracy: 0.5678 - val_loss: 2.0744 - val_sparse_categorical_accuracy: 0.1682\n",
            "Epoch 15/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.0810 - sparse_categorical_accuracy: 0.5491\n",
            "Epoch 15: val_sparse_categorical_accuracy did not improve from 0.17757\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 1.0810 - sparse_categorical_accuracy: 0.5491 - val_loss: 2.0615 - val_sparse_categorical_accuracy: 0.1682\n",
            "Epoch 16/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.9708 - sparse_categorical_accuracy: 0.6168\n",
            "Epoch 16: val_sparse_categorical_accuracy did not improve from 0.17757\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.9708 - sparse_categorical_accuracy: 0.6168 - val_loss: 1.9397 - val_sparse_categorical_accuracy: 0.1682\n",
            "Epoch 17/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.9240 - sparse_categorical_accuracy: 0.6425\n",
            "Epoch 17: val_sparse_categorical_accuracy did not improve from 0.17757\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.9240 - sparse_categorical_accuracy: 0.6425 - val_loss: 1.9350 - val_sparse_categorical_accuracy: 0.1682\n",
            "Epoch 18/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.8383 - sparse_categorical_accuracy: 0.6729\n",
            "Epoch 18: val_sparse_categorical_accuracy did not improve from 0.17757\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.8383 - sparse_categorical_accuracy: 0.6729 - val_loss: 1.9916 - val_sparse_categorical_accuracy: 0.1495\n",
            "Epoch 19/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.8026 - sparse_categorical_accuracy: 0.6869\n",
            "Epoch 19: val_sparse_categorical_accuracy did not improve from 0.17757\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.8026 - sparse_categorical_accuracy: 0.6869 - val_loss: 2.3607 - val_sparse_categorical_accuracy: 0.1589\n",
            "Epoch 20/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.7572 - sparse_categorical_accuracy: 0.6752\n",
            "Epoch 20: val_sparse_categorical_accuracy did not improve from 0.17757\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.7572 - sparse_categorical_accuracy: 0.6752 - val_loss: 2.4029 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 21/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.6549 - sparse_categorical_accuracy: 0.7290\n",
            "Epoch 21: val_sparse_categorical_accuracy did not improve from 0.17757\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.6549 - sparse_categorical_accuracy: 0.7290 - val_loss: 2.7813 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 22/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.6338 - sparse_categorical_accuracy: 0.7523\n",
            "Epoch 22: val_sparse_categorical_accuracy did not improve from 0.17757\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.6338 - sparse_categorical_accuracy: 0.7523 - val_loss: 2.9770 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 23/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.6055 - sparse_categorical_accuracy: 0.7430\n",
            "Epoch 23: val_sparse_categorical_accuracy did not improve from 0.17757\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.6055 - sparse_categorical_accuracy: 0.7430 - val_loss: 3.2645 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 24/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.4810 - sparse_categorical_accuracy: 0.8107\n",
            "Epoch 24: val_sparse_categorical_accuracy did not improve from 0.17757\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.4810 - sparse_categorical_accuracy: 0.8107 - val_loss: 3.4740 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 25/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.4674 - sparse_categorical_accuracy: 0.8131\n",
            "Epoch 25: val_sparse_categorical_accuracy did not improve from 0.17757\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.4674 - sparse_categorical_accuracy: 0.8131 - val_loss: 3.6593 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 26/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.4571 - sparse_categorical_accuracy: 0.8014\n",
            "Epoch 26: val_sparse_categorical_accuracy did not improve from 0.17757\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.4571 - sparse_categorical_accuracy: 0.8014 - val_loss: 3.5946 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 27/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.3950 - sparse_categorical_accuracy: 0.8411\n",
            "Epoch 27: val_sparse_categorical_accuracy did not improve from 0.17757\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.3950 - sparse_categorical_accuracy: 0.8411 - val_loss: 3.8926 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 28/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.3420 - sparse_categorical_accuracy: 0.8715\n",
            "Epoch 28: val_sparse_categorical_accuracy did not improve from 0.17757\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.3420 - sparse_categorical_accuracy: 0.8715 - val_loss: 4.2002 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 29/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.3486 - sparse_categorical_accuracy: 0.8551\n",
            "Epoch 29: val_sparse_categorical_accuracy did not improve from 0.17757\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.3486 - sparse_categorical_accuracy: 0.8551 - val_loss: 4.1211 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 30/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.3198 - sparse_categorical_accuracy: 0.8879\n",
            "Epoch 30: val_sparse_categorical_accuracy did not improve from 0.17757\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.3198 - sparse_categorical_accuracy: 0.8879 - val_loss: 4.3483 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 31/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.2608 - sparse_categorical_accuracy: 0.8925\n",
            "Epoch 31: val_sparse_categorical_accuracy did not improve from 0.17757\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.2608 - sparse_categorical_accuracy: 0.8925 - val_loss: 4.6326 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 32/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.2771 - sparse_categorical_accuracy: 0.8808\n",
            "Epoch 32: val_sparse_categorical_accuracy did not improve from 0.17757\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.2771 - sparse_categorical_accuracy: 0.8808 - val_loss: 4.2269 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 33/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.2237 - sparse_categorical_accuracy: 0.9089\n",
            "Epoch 33: val_sparse_categorical_accuracy did not improve from 0.17757\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.2237 - sparse_categorical_accuracy: 0.9089 - val_loss: 4.7853 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 34/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1770 - sparse_categorical_accuracy: 0.9322\n",
            "Epoch 34: val_sparse_categorical_accuracy did not improve from 0.17757\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.1770 - sparse_categorical_accuracy: 0.9322 - val_loss: 4.9329 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 35/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.2415 - sparse_categorical_accuracy: 0.9065\n",
            "Epoch 35: val_sparse_categorical_accuracy did not improve from 0.17757\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.2415 - sparse_categorical_accuracy: 0.9065 - val_loss: 5.3481 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 36/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.2142 - sparse_categorical_accuracy: 0.9182\n",
            "Epoch 36: val_sparse_categorical_accuracy did not improve from 0.17757\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 0.2142 - sparse_categorical_accuracy: 0.9182 - val_loss: 5.3971 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 37/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1620 - sparse_categorical_accuracy: 0.9393\n",
            "Epoch 37: val_sparse_categorical_accuracy did not improve from 0.17757\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 0.1620 - sparse_categorical_accuracy: 0.9393 - val_loss: 5.4767 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 38/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.1310 - sparse_categorical_accuracy: 0.9635\n",
            "Epoch 38: val_sparse_categorical_accuracy improved from 0.17757 to 0.19626, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.1353 - sparse_categorical_accuracy: 0.9626 - val_loss: 5.5379 - val_sparse_categorical_accuracy: 0.1963\n",
            "Epoch 39/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1720 - sparse_categorical_accuracy: 0.9486\n",
            "Epoch 39: val_sparse_categorical_accuracy did not improve from 0.19626\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 0.1720 - sparse_categorical_accuracy: 0.9486 - val_loss: 5.7559 - val_sparse_categorical_accuracy: 0.1402\n",
            "Epoch 40/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1595 - sparse_categorical_accuracy: 0.9346\n",
            "Epoch 40: val_sparse_categorical_accuracy did not improve from 0.19626\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 0.1595 - sparse_categorical_accuracy: 0.9346 - val_loss: 6.2085 - val_sparse_categorical_accuracy: 0.1308\n",
            "Epoch 41/900\n",
            "5/7 [====================>.........] - ETA: 0s - loss: 0.1151 - sparse_categorical_accuracy: 0.9500\n",
            "Epoch 41: val_sparse_categorical_accuracy did not improve from 0.19626\n",
            "7/7 [==============================] - 0s 35ms/step - loss: 0.1242 - sparse_categorical_accuracy: 0.9509 - val_loss: 6.2263 - val_sparse_categorical_accuracy: 0.1495\n",
            "Epoch 42/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1001 - sparse_categorical_accuracy: 0.9579\n",
            "Epoch 42: val_sparse_categorical_accuracy did not improve from 0.19626\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.1001 - sparse_categorical_accuracy: 0.9579 - val_loss: 6.0268 - val_sparse_categorical_accuracy: 0.1776\n",
            "Epoch 43/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0843 - sparse_categorical_accuracy: 0.9696\n",
            "Epoch 43: val_sparse_categorical_accuracy improved from 0.19626 to 0.24299, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 61ms/step - loss: 0.0843 - sparse_categorical_accuracy: 0.9696 - val_loss: 5.8882 - val_sparse_categorical_accuracy: 0.2430\n",
            "Epoch 44/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0956 - sparse_categorical_accuracy: 0.9720\n",
            "Epoch 44: val_sparse_categorical_accuracy improved from 0.24299 to 0.25234, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0956 - sparse_categorical_accuracy: 0.9720 - val_loss: 5.7354 - val_sparse_categorical_accuracy: 0.2523\n",
            "Epoch 45/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0875 - sparse_categorical_accuracy: 0.9740\n",
            "Epoch 45: val_sparse_categorical_accuracy did not improve from 0.25234\n",
            "7/7 [==============================] - 0s 34ms/step - loss: 0.0884 - sparse_categorical_accuracy: 0.9720 - val_loss: 5.7852 - val_sparse_categorical_accuracy: 0.2430\n",
            "Epoch 46/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0902 - sparse_categorical_accuracy: 0.9720\n",
            "Epoch 46: val_sparse_categorical_accuracy did not improve from 0.25234\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 0.0902 - sparse_categorical_accuracy: 0.9720 - val_loss: 6.0199 - val_sparse_categorical_accuracy: 0.2523\n",
            "Epoch 47/900\n",
            "5/7 [====================>.........] - ETA: 0s - loss: 0.0745 - sparse_categorical_accuracy: 0.9719\n",
            "Epoch 47: val_sparse_categorical_accuracy did not improve from 0.25234\n",
            "7/7 [==============================] - 0s 34ms/step - loss: 0.0701 - sparse_categorical_accuracy: 0.9766 - val_loss: 6.0777 - val_sparse_categorical_accuracy: 0.2336\n",
            "Epoch 48/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0800 - sparse_categorical_accuracy: 0.9603\n",
            "Epoch 48: val_sparse_categorical_accuracy did not improve from 0.25234\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0800 - sparse_categorical_accuracy: 0.9603 - val_loss: 5.9882 - val_sparse_categorical_accuracy: 0.2336\n",
            "Epoch 49/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0575 - sparse_categorical_accuracy: 0.9766\n",
            "Epoch 49: val_sparse_categorical_accuracy did not improve from 0.25234\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0575 - sparse_categorical_accuracy: 0.9766 - val_loss: 6.0462 - val_sparse_categorical_accuracy: 0.2336\n",
            "Epoch 50/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0739 - sparse_categorical_accuracy: 0.9790\n",
            "Epoch 50: val_sparse_categorical_accuracy did not improve from 0.25234\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0739 - sparse_categorical_accuracy: 0.9790 - val_loss: 6.0460 - val_sparse_categorical_accuracy: 0.2523\n",
            "Epoch 51/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0481 - sparse_categorical_accuracy: 0.9836\n",
            "Epoch 51: val_sparse_categorical_accuracy did not improve from 0.25234\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0481 - sparse_categorical_accuracy: 0.9836 - val_loss: 5.9640 - val_sparse_categorical_accuracy: 0.2430\n",
            "Epoch 52/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0476 - sparse_categorical_accuracy: 0.9790\n",
            "Epoch 52: val_sparse_categorical_accuracy did not improve from 0.25234\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0476 - sparse_categorical_accuracy: 0.9790 - val_loss: 6.0499 - val_sparse_categorical_accuracy: 0.2430\n",
            "Epoch 53/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0432 - sparse_categorical_accuracy: 0.9860\n",
            "Epoch 53: val_sparse_categorical_accuracy did not improve from 0.25234\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0432 - sparse_categorical_accuracy: 0.9860 - val_loss: 5.8774 - val_sparse_categorical_accuracy: 0.2523\n",
            "Epoch 54/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0537 - sparse_categorical_accuracy: 0.9860\n",
            "Epoch 54: val_sparse_categorical_accuracy did not improve from 0.25234\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0537 - sparse_categorical_accuracy: 0.9860 - val_loss: 5.7777 - val_sparse_categorical_accuracy: 0.2150\n",
            "Epoch 55/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1923 - sparse_categorical_accuracy: 0.9533\n",
            "Epoch 55: val_sparse_categorical_accuracy did not improve from 0.25234\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.1923 - sparse_categorical_accuracy: 0.9533 - val_loss: 6.0743 - val_sparse_categorical_accuracy: 0.1869\n",
            "Epoch 56/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.2035 - sparse_categorical_accuracy: 0.9416\n",
            "Epoch 56: val_sparse_categorical_accuracy did not improve from 0.25234\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.2035 - sparse_categorical_accuracy: 0.9416 - val_loss: 6.1755 - val_sparse_categorical_accuracy: 0.2336\n",
            "Epoch 57/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1295 - sparse_categorical_accuracy: 0.9650\n",
            "Epoch 57: val_sparse_categorical_accuracy did not improve from 0.25234\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.1295 - sparse_categorical_accuracy: 0.9650 - val_loss: 5.8880 - val_sparse_categorical_accuracy: 0.2336\n",
            "Epoch 58/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0767 - sparse_categorical_accuracy: 0.9743\n",
            "Epoch 58: val_sparse_categorical_accuracy improved from 0.25234 to 0.26168, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 52ms/step - loss: 0.0767 - sparse_categorical_accuracy: 0.9743 - val_loss: 5.4906 - val_sparse_categorical_accuracy: 0.2617\n",
            "Epoch 59/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0906 - sparse_categorical_accuracy: 0.9626\n",
            "Epoch 59: val_sparse_categorical_accuracy did not improve from 0.26168\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0906 - sparse_categorical_accuracy: 0.9626 - val_loss: 5.6191 - val_sparse_categorical_accuracy: 0.2336\n",
            "Epoch 60/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1004 - sparse_categorical_accuracy: 0.9813\n",
            "Epoch 60: val_sparse_categorical_accuracy did not improve from 0.26168\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.1004 - sparse_categorical_accuracy: 0.9813 - val_loss: 5.5872 - val_sparse_categorical_accuracy: 0.2523\n",
            "Epoch 61/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0591 - sparse_categorical_accuracy: 0.9836\n",
            "Epoch 61: val_sparse_categorical_accuracy did not improve from 0.26168\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0591 - sparse_categorical_accuracy: 0.9836 - val_loss: 5.3842 - val_sparse_categorical_accuracy: 0.2430\n",
            "Epoch 62/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0552 - sparse_categorical_accuracy: 0.9813\n",
            "Epoch 62: val_sparse_categorical_accuracy improved from 0.26168 to 0.28972, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 52ms/step - loss: 0.0552 - sparse_categorical_accuracy: 0.9813 - val_loss: 4.5513 - val_sparse_categorical_accuracy: 0.2897\n",
            "Epoch 63/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0958 - sparse_categorical_accuracy: 0.9533\n",
            "Epoch 63: val_sparse_categorical_accuracy improved from 0.28972 to 0.30841, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 53ms/step - loss: 0.0958 - sparse_categorical_accuracy: 0.9533 - val_loss: 4.3871 - val_sparse_categorical_accuracy: 0.3084\n",
            "Epoch 64/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0342 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 64: val_sparse_categorical_accuracy did not improve from 0.30841\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0342 - sparse_categorical_accuracy: 0.9907 - val_loss: 4.6470 - val_sparse_categorical_accuracy: 0.2991\n",
            "Epoch 65/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0532 - sparse_categorical_accuracy: 0.9836\n",
            "Epoch 65: val_sparse_categorical_accuracy did not improve from 0.30841\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0532 - sparse_categorical_accuracy: 0.9836 - val_loss: 4.6961 - val_sparse_categorical_accuracy: 0.3084\n",
            "Epoch 66/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0318 - sparse_categorical_accuracy: 0.9836\n",
            "Epoch 66: val_sparse_categorical_accuracy did not improve from 0.30841\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0318 - sparse_categorical_accuracy: 0.9836 - val_loss: 4.5194 - val_sparse_categorical_accuracy: 0.2897\n",
            "Epoch 67/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0162 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 67: val_sparse_categorical_accuracy did not improve from 0.30841\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0162 - sparse_categorical_accuracy: 0.9953 - val_loss: 4.4923 - val_sparse_categorical_accuracy: 0.2897\n",
            "Epoch 68/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0313 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 68: val_sparse_categorical_accuracy did not improve from 0.30841\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0313 - sparse_categorical_accuracy: 0.9930 - val_loss: 4.1530 - val_sparse_categorical_accuracy: 0.3084\n",
            "Epoch 69/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0307 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 69: val_sparse_categorical_accuracy improved from 0.30841 to 0.36449, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 53ms/step - loss: 0.0307 - sparse_categorical_accuracy: 0.9907 - val_loss: 3.7506 - val_sparse_categorical_accuracy: 0.3645\n",
            "Epoch 70/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0310 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 70: val_sparse_categorical_accuracy improved from 0.36449 to 0.38318, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 52ms/step - loss: 0.0310 - sparse_categorical_accuracy: 0.9907 - val_loss: 3.7421 - val_sparse_categorical_accuracy: 0.3832\n",
            "Epoch 71/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0236 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 71: val_sparse_categorical_accuracy did not improve from 0.38318\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0236 - sparse_categorical_accuracy: 0.9930 - val_loss: 4.1611 - val_sparse_categorical_accuracy: 0.3458\n",
            "Epoch 72/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0389 - sparse_categorical_accuracy: 0.9860\n",
            "Epoch 72: val_sparse_categorical_accuracy improved from 0.38318 to 0.41121, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 56ms/step - loss: 0.0389 - sparse_categorical_accuracy: 0.9860 - val_loss: 3.2699 - val_sparse_categorical_accuracy: 0.4112\n",
            "Epoch 73/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0443 - sparse_categorical_accuracy: 0.9836\n",
            "Epoch 73: val_sparse_categorical_accuracy improved from 0.41121 to 0.44860, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 52ms/step - loss: 0.0443 - sparse_categorical_accuracy: 0.9836 - val_loss: 2.9930 - val_sparse_categorical_accuracy: 0.4486\n",
            "Epoch 74/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0391 - sparse_categorical_accuracy: 0.9883\n",
            "Epoch 74: val_sparse_categorical_accuracy did not improve from 0.44860\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0391 - sparse_categorical_accuracy: 0.9883 - val_loss: 3.1638 - val_sparse_categorical_accuracy: 0.4299\n",
            "Epoch 75/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0266 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 75: val_sparse_categorical_accuracy did not improve from 0.44860\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0266 - sparse_categorical_accuracy: 0.9930 - val_loss: 3.3257 - val_sparse_categorical_accuracy: 0.4019\n",
            "Epoch 76/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0338 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 76: val_sparse_categorical_accuracy did not improve from 0.44860\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0338 - sparse_categorical_accuracy: 0.9907 - val_loss: 3.1545 - val_sparse_categorical_accuracy: 0.4206\n",
            "Epoch 77/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0349 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 77: val_sparse_categorical_accuracy did not improve from 0.44860\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0349 - sparse_categorical_accuracy: 0.9907 - val_loss: 3.0616 - val_sparse_categorical_accuracy: 0.4486\n",
            "Epoch 78/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0242 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 78: val_sparse_categorical_accuracy did not improve from 0.44860\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0242 - sparse_categorical_accuracy: 0.9930 - val_loss: 3.0636 - val_sparse_categorical_accuracy: 0.4393\n",
            "Epoch 79/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0075 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 79: val_sparse_categorical_accuracy improved from 0.44860 to 0.46729, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 51ms/step - loss: 0.0075 - sparse_categorical_accuracy: 1.0000 - val_loss: 2.9363 - val_sparse_categorical_accuracy: 0.4673\n",
            "Epoch 80/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0212 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 80: val_sparse_categorical_accuracy did not improve from 0.46729\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0212 - sparse_categorical_accuracy: 0.9953 - val_loss: 2.8670 - val_sparse_categorical_accuracy: 0.4579\n",
            "Epoch 81/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0176 - sparse_categorical_accuracy: 0.9974\n",
            "Epoch 81: val_sparse_categorical_accuracy improved from 0.46729 to 0.48598, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 52ms/step - loss: 0.0193 - sparse_categorical_accuracy: 0.9953 - val_loss: 2.8175 - val_sparse_categorical_accuracy: 0.4860\n",
            "Epoch 82/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0179 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 82: val_sparse_categorical_accuracy improved from 0.48598 to 0.49533, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 50ms/step - loss: 0.0179 - sparse_categorical_accuracy: 0.9977 - val_loss: 2.8188 - val_sparse_categorical_accuracy: 0.4953\n",
            "Epoch 83/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0126 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 83: val_sparse_categorical_accuracy did not improve from 0.49533\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0126 - sparse_categorical_accuracy: 0.9977 - val_loss: 2.7710 - val_sparse_categorical_accuracy: 0.4953\n",
            "Epoch 84/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0181 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 84: val_sparse_categorical_accuracy improved from 0.49533 to 0.50467, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 51ms/step - loss: 0.0181 - sparse_categorical_accuracy: 0.9953 - val_loss: 2.8037 - val_sparse_categorical_accuracy: 0.5047\n",
            "Epoch 85/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0390 - sparse_categorical_accuracy: 0.9836\n",
            "Epoch 85: val_sparse_categorical_accuracy improved from 0.50467 to 0.54206, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 53ms/step - loss: 0.0390 - sparse_categorical_accuracy: 0.9836 - val_loss: 2.6285 - val_sparse_categorical_accuracy: 0.5421\n",
            "Epoch 86/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0178 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 86: val_sparse_categorical_accuracy did not improve from 0.54206\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0178 - sparse_categorical_accuracy: 0.9930 - val_loss: 2.7252 - val_sparse_categorical_accuracy: 0.5327\n",
            "Epoch 87/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0417 - sparse_categorical_accuracy: 0.9883\n",
            "Epoch 87: val_sparse_categorical_accuracy improved from 0.54206 to 0.60748, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 53ms/step - loss: 0.0417 - sparse_categorical_accuracy: 0.9883 - val_loss: 2.1183 - val_sparse_categorical_accuracy: 0.6075\n",
            "Epoch 88/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0818 - sparse_categorical_accuracy: 0.9766\n",
            "Epoch 88: val_sparse_categorical_accuracy did not improve from 0.60748\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0818 - sparse_categorical_accuracy: 0.9766 - val_loss: 2.3274 - val_sparse_categorical_accuracy: 0.5514\n",
            "Epoch 89/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0503 - sparse_categorical_accuracy: 0.9813\n",
            "Epoch 89: val_sparse_categorical_accuracy did not improve from 0.60748\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0503 - sparse_categorical_accuracy: 0.9813 - val_loss: 2.1792 - val_sparse_categorical_accuracy: 0.6075\n",
            "Epoch 90/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0368 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 90: val_sparse_categorical_accuracy improved from 0.60748 to 0.61682, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.0368 - sparse_categorical_accuracy: 0.9907 - val_loss: 2.0587 - val_sparse_categorical_accuracy: 0.6168\n",
            "Epoch 91/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0453 - sparse_categorical_accuracy: 0.9792\n",
            "Epoch 91: val_sparse_categorical_accuracy did not improve from 0.61682\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 0.0437 - sparse_categorical_accuracy: 0.9790 - val_loss: 1.9221 - val_sparse_categorical_accuracy: 0.6168\n",
            "Epoch 92/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0186 - sparse_categorical_accuracy: 0.9974\n",
            "Epoch 92: val_sparse_categorical_accuracy did not improve from 0.61682\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 0.0181 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.9606 - val_sparse_categorical_accuracy: 0.6075\n",
            "Epoch 93/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0448 - sparse_categorical_accuracy: 0.9790\n",
            "Epoch 93: val_sparse_categorical_accuracy improved from 0.61682 to 0.63551, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0448 - sparse_categorical_accuracy: 0.9790 - val_loss: 2.0291 - val_sparse_categorical_accuracy: 0.6355\n",
            "Epoch 94/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0163 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 94: val_sparse_categorical_accuracy did not improve from 0.63551\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 0.0163 - sparse_categorical_accuracy: 0.9930 - val_loss: 2.0745 - val_sparse_categorical_accuracy: 0.5981\n",
            "Epoch 95/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0335 - sparse_categorical_accuracy: 0.9836\n",
            "Epoch 95: val_sparse_categorical_accuracy did not improve from 0.63551\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 0.0335 - sparse_categorical_accuracy: 0.9836 - val_loss: 1.9391 - val_sparse_categorical_accuracy: 0.6355\n",
            "Epoch 96/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0071 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 96: val_sparse_categorical_accuracy improved from 0.63551 to 0.64486, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.0079 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.9461 - val_sparse_categorical_accuracy: 0.6449\n",
            "Epoch 97/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0170 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 97: val_sparse_categorical_accuracy improved from 0.64486 to 0.66355, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.0170 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.9387 - val_sparse_categorical_accuracy: 0.6636\n",
            "Epoch 98/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0066 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 98: val_sparse_categorical_accuracy improved from 0.66355 to 0.67290, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 0.0088 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.9664 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 99/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0146 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 99: val_sparse_categorical_accuracy did not improve from 0.67290\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.0146 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.9242 - val_sparse_categorical_accuracy: 0.6636\n",
            "Epoch 100/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0201 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 100: val_sparse_categorical_accuracy improved from 0.67290 to 0.71028, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 62ms/step - loss: 0.0201 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.8529 - val_sparse_categorical_accuracy: 0.7103\n",
            "Epoch 101/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0164 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 101: val_sparse_categorical_accuracy did not improve from 0.71028\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0164 - sparse_categorical_accuracy: 0.9907 - val_loss: 2.0284 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 102/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0142 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 102: val_sparse_categorical_accuracy did not improve from 0.71028\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0142 - sparse_categorical_accuracy: 0.9953 - val_loss: 2.0723 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 103/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0108 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 103: val_sparse_categorical_accuracy did not improve from 0.71028\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0108 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.9736 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 104/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0081 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 104: val_sparse_categorical_accuracy did not improve from 0.71028\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0081 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.9244 - val_sparse_categorical_accuracy: 0.6822\n",
            "Epoch 105/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0171 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 105: val_sparse_categorical_accuracy did not improve from 0.71028\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0171 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.9313 - val_sparse_categorical_accuracy: 0.6542\n",
            "Epoch 106/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0184 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 106: val_sparse_categorical_accuracy did not improve from 0.71028\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0184 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.7665 - val_sparse_categorical_accuracy: 0.6822\n",
            "Epoch 107/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0558 - sparse_categorical_accuracy: 0.9883\n",
            "Epoch 107: val_sparse_categorical_accuracy did not improve from 0.71028\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0558 - sparse_categorical_accuracy: 0.9883 - val_loss: 1.9487 - val_sparse_categorical_accuracy: 0.6636\n",
            "Epoch 108/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0170 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 108: val_sparse_categorical_accuracy did not improve from 0.71028\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0170 - sparse_categorical_accuracy: 0.9953 - val_loss: 2.2706 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 109/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0300 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 109: val_sparse_categorical_accuracy did not improve from 0.71028\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0300 - sparse_categorical_accuracy: 0.9907 - val_loss: 2.0134 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 110/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0188 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 110: val_sparse_categorical_accuracy did not improve from 0.71028\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0188 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.8514 - val_sparse_categorical_accuracy: 0.6822\n",
            "Epoch 111/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0192 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 111: val_sparse_categorical_accuracy did not improve from 0.71028\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0192 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.8418 - val_sparse_categorical_accuracy: 0.6636\n",
            "Epoch 112/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0181 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 112: val_sparse_categorical_accuracy did not improve from 0.71028\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0181 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.9790 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 113/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0250 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 113: val_sparse_categorical_accuracy did not improve from 0.71028\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0250 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.8520 - val_sparse_categorical_accuracy: 0.6542\n",
            "Epoch 114/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0189 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 114: val_sparse_categorical_accuracy did not improve from 0.71028\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0189 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.7469 - val_sparse_categorical_accuracy: 0.6542\n",
            "Epoch 115/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0063 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 115: val_sparse_categorical_accuracy did not improve from 0.71028\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0063 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.6411 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 116/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0080 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 116: val_sparse_categorical_accuracy did not improve from 0.71028\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0080 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.6292 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 117/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0148 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 117: val_sparse_categorical_accuracy did not improve from 0.71028\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0148 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.7159 - val_sparse_categorical_accuracy: 0.6822\n",
            "Epoch 118/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0101 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 118: val_sparse_categorical_accuracy did not improve from 0.71028\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0101 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.9145 - val_sparse_categorical_accuracy: 0.6636\n",
            "Epoch 119/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0211 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 119: val_sparse_categorical_accuracy did not improve from 0.71028\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0211 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.8341 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 120/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0100 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 120: val_sparse_categorical_accuracy improved from 0.71028 to 0.71963, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 53ms/step - loss: 0.0100 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.6698 - val_sparse_categorical_accuracy: 0.7196\n",
            "Epoch 121/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0460 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 121: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0460 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.5145 - val_sparse_categorical_accuracy: 0.7103\n",
            "Epoch 122/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0232 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 122: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0232 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.7428 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 123/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0333 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 123: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0333 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.7227 - val_sparse_categorical_accuracy: 0.6822\n",
            "Epoch 124/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0241 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 124: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0241 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.6203 - val_sparse_categorical_accuracy: 0.6636\n",
            "Epoch 125/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0696 - sparse_categorical_accuracy: 0.9790\n",
            "Epoch 125: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0696 - sparse_categorical_accuracy: 0.9790 - val_loss: 1.9851 - val_sparse_categorical_accuracy: 0.6822\n",
            "Epoch 126/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0575 - sparse_categorical_accuracy: 0.9743\n",
            "Epoch 126: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0575 - sparse_categorical_accuracy: 0.9743 - val_loss: 1.8027 - val_sparse_categorical_accuracy: 0.6822\n",
            "Epoch 127/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0340 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 127: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0340 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.7758 - val_sparse_categorical_accuracy: 0.7103\n",
            "Epoch 128/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0425 - sparse_categorical_accuracy: 0.9860\n",
            "Epoch 128: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0425 - sparse_categorical_accuracy: 0.9860 - val_loss: 2.0459 - val_sparse_categorical_accuracy: 0.6262\n",
            "Epoch 129/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0619 - sparse_categorical_accuracy: 0.9836\n",
            "Epoch 129: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0619 - sparse_categorical_accuracy: 0.9836 - val_loss: 1.7831 - val_sparse_categorical_accuracy: 0.6075\n",
            "Epoch 130/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0441 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 130: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0441 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.6797 - val_sparse_categorical_accuracy: 0.6822\n",
            "Epoch 131/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0252 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 131: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0252 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.9050 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 132/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0227 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 132: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0227 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.7864 - val_sparse_categorical_accuracy: 0.6822\n",
            "Epoch 133/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0237 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 133: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0237 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.6612 - val_sparse_categorical_accuracy: 0.7103\n",
            "Epoch 134/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0154 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 134: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0154 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.7865 - val_sparse_categorical_accuracy: 0.6822\n",
            "Epoch 135/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0153 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 135: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0153 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.7875 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 136/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0252 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 136: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0252 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.7101 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 137/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0164 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 137: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0164 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.7149 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 138/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0329 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 138: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0329 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.9325 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 139/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0347 - sparse_categorical_accuracy: 0.9883\n",
            "Epoch 139: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0347 - sparse_categorical_accuracy: 0.9883 - val_loss: 1.9891 - val_sparse_categorical_accuracy: 0.6636\n",
            "Epoch 140/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0163 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 140: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0163 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.6580 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 141/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0072 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 141: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0072 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.6594 - val_sparse_categorical_accuracy: 0.6542\n",
            "Epoch 142/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0058 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 142: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0058 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.7206 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 143/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0082 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 143: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0082 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.7934 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 144/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0138 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 144: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0138 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.8075 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 145/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0111 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 145: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0111 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.7588 - val_sparse_categorical_accuracy: 0.7103\n",
            "Epoch 146/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0055 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 146: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0055 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.9368 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 147/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0149 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 147: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0149 - sparse_categorical_accuracy: 0.9953 - val_loss: 2.1374 - val_sparse_categorical_accuracy: 0.6542\n",
            "Epoch 148/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0063 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 148: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0063 - sparse_categorical_accuracy: 0.9977 - val_loss: 2.1958 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 149/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0095 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 149: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0095 - sparse_categorical_accuracy: 0.9977 - val_loss: 2.0929 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 150/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0019 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 150: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0019 - sparse_categorical_accuracy: 1.0000 - val_loss: 2.0041 - val_sparse_categorical_accuracy: 0.6822\n",
            "Epoch 151/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0019 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 151: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.0023 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.9572 - val_sparse_categorical_accuracy: 0.7103\n",
            "Epoch 152/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0055 - sparse_categorical_accuracy: 0.9974\n",
            "Epoch 152: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 0.0050 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.9513 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 153/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0048 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 153: val_sparse_categorical_accuracy did not improve from 0.71963\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 0.0048 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.9179 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 154/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0066 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 154: val_sparse_categorical_accuracy improved from 0.71963 to 0.72897, saving model to cnn_lstm_emodb3.ckpt\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.0066 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.7890 - val_sparse_categorical_accuracy: 0.7290\n",
            "Epoch 155/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0037 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 155: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 0.0034 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.8199 - val_sparse_categorical_accuracy: 0.7290\n",
            "Epoch 156/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0049 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 156: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 0.0049 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.8540 - val_sparse_categorical_accuracy: 0.7196\n",
            "Epoch 157/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0028 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 157: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 0.0028 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.9166 - val_sparse_categorical_accuracy: 0.7103\n",
            "Epoch 158/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0061 - sparse_categorical_accuracy: 0.9974\n",
            "Epoch 158: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 35ms/step - loss: 0.0059 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.9580 - val_sparse_categorical_accuracy: 0.6822\n",
            "Epoch 159/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0033 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 159: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 30ms/step - loss: 0.0033 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.9816 - val_sparse_categorical_accuracy: 0.6822\n",
            "Epoch 160/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0071 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 160: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 37ms/step - loss: 0.0071 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.9178 - val_sparse_categorical_accuracy: 0.6636\n",
            "Epoch 161/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0033 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 161: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 35ms/step - loss: 0.0031 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.9301 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 162/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0033 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 162: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 34ms/step - loss: 0.0031 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.9637 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 163/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0051 - sparse_categorical_accuracy: 0.9974\n",
            "Epoch 163: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 35ms/step - loss: 0.0047 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.9850 - val_sparse_categorical_accuracy: 0.6822\n",
            "Epoch 164/900\n",
            "5/7 [====================>.........] - ETA: 0s - loss: 0.0015 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 164: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 36ms/step - loss: 0.0029 - sparse_categorical_accuracy: 1.0000 - val_loss: 2.0015 - val_sparse_categorical_accuracy: 0.6822\n",
            "Epoch 165/900\n",
            "5/7 [====================>.........] - ETA: 0s - loss: 0.0036 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 165: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 0.0058 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.9163 - val_sparse_categorical_accuracy: 0.6822\n",
            "Epoch 166/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0046 - sparse_categorical_accuracy: 0.9974\n",
            "Epoch 166: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 30ms/step - loss: 0.0042 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.9530 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 167/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0177 - sparse_categorical_accuracy: 0.9883\n",
            "Epoch 167: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0177 - sparse_categorical_accuracy: 0.9883 - val_loss: 2.0621 - val_sparse_categorical_accuracy: 0.6636\n",
            "Epoch 168/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0267 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 168: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0267 - sparse_categorical_accuracy: 0.9930 - val_loss: 2.0684 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 169/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0699 - sparse_categorical_accuracy: 0.9860\n",
            "Epoch 169: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0699 - sparse_categorical_accuracy: 0.9860 - val_loss: 1.9213 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 170/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0472 - sparse_categorical_accuracy: 0.9860\n",
            "Epoch 170: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0472 - sparse_categorical_accuracy: 0.9860 - val_loss: 1.8147 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 171/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0604 - sparse_categorical_accuracy: 0.9813\n",
            "Epoch 171: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 30ms/step - loss: 0.0604 - sparse_categorical_accuracy: 0.9813 - val_loss: 2.0274 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 172/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0171 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 172: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0171 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.8442 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 173/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0187 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 173: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0187 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.6226 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 174/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0103 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 174: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0103 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.7658 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 175/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0055 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 175: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0055 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.8541 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 176/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0412 - sparse_categorical_accuracy: 0.9860\n",
            "Epoch 176: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 30ms/step - loss: 0.0412 - sparse_categorical_accuracy: 0.9860 - val_loss: 1.9513 - val_sparse_categorical_accuracy: 0.6636\n",
            "Epoch 177/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0297 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 177: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0297 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.6889 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 178/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0273 - sparse_categorical_accuracy: 0.9883\n",
            "Epoch 178: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0273 - sparse_categorical_accuracy: 0.9883 - val_loss: 1.9289 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 179/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.1029 - sparse_categorical_accuracy: 0.9790\n",
            "Epoch 179: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.1029 - sparse_categorical_accuracy: 0.9790 - val_loss: 1.7272 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 180/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0636 - sparse_categorical_accuracy: 0.9743\n",
            "Epoch 180: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0636 - sparse_categorical_accuracy: 0.9743 - val_loss: 2.0166 - val_sparse_categorical_accuracy: 0.6449\n",
            "Epoch 181/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0431 - sparse_categorical_accuracy: 0.9836\n",
            "Epoch 181: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0431 - sparse_categorical_accuracy: 0.9836 - val_loss: 2.0719 - val_sparse_categorical_accuracy: 0.6542\n",
            "Epoch 182/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0557 - sparse_categorical_accuracy: 0.9836\n",
            "Epoch 182: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0557 - sparse_categorical_accuracy: 0.9836 - val_loss: 1.6933 - val_sparse_categorical_accuracy: 0.6542\n",
            "Epoch 183/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0560 - sparse_categorical_accuracy: 0.9766\n",
            "Epoch 183: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0560 - sparse_categorical_accuracy: 0.9766 - val_loss: 1.5135 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 184/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0182 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 184: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0182 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.8145 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 185/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0219 - sparse_categorical_accuracy: 0.9860\n",
            "Epoch 185: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0219 - sparse_categorical_accuracy: 0.9860 - val_loss: 2.0469 - val_sparse_categorical_accuracy: 0.6636\n",
            "Epoch 186/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0285 - sparse_categorical_accuracy: 0.9930\n",
            "Epoch 186: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0285 - sparse_categorical_accuracy: 0.9930 - val_loss: 1.7822 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 187/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0456 - sparse_categorical_accuracy: 0.9836\n",
            "Epoch 187: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0456 - sparse_categorical_accuracy: 0.9836 - val_loss: 1.9295 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 188/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0176 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 188: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0176 - sparse_categorical_accuracy: 0.9953 - val_loss: 2.0214 - val_sparse_categorical_accuracy: 0.6822\n",
            "Epoch 189/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0187 - sparse_categorical_accuracy: 0.9907\n",
            "Epoch 189: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0187 - sparse_categorical_accuracy: 0.9907 - val_loss: 1.8754 - val_sparse_categorical_accuracy: 0.6636\n",
            "Epoch 190/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0040 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 190: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0040 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.8205 - val_sparse_categorical_accuracy: 0.6729\n",
            "Epoch 191/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0122 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 191: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0122 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.8540 - val_sparse_categorical_accuracy: 0.7103\n",
            "Epoch 192/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0093 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 192: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0093 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.8619 - val_sparse_categorical_accuracy: 0.7103\n",
            "Epoch 193/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0122 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 193: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0122 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.8459 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 194/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0049 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 194: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0049 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.9685 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 195/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0149 - sparse_categorical_accuracy: 0.9977\n",
            "Epoch 195: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0149 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.8838 - val_sparse_categorical_accuracy: 0.6822\n",
            "Epoch 196/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0020 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 196: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0020 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.8416 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 197/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0024 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 197: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0024 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.8177 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 198/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0026 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 198: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0026 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.8260 - val_sparse_categorical_accuracy: 0.6916\n",
            "Epoch 199/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0020 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 199: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0020 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.8568 - val_sparse_categorical_accuracy: 0.6822\n",
            "Epoch 200/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0018 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 200: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0018 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.8715 - val_sparse_categorical_accuracy: 0.6822\n",
            "Epoch 201/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0037 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 201: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0037 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.8659 - val_sparse_categorical_accuracy: 0.7009\n",
            "Epoch 202/900\n",
            "6/7 [========================>.....] - ETA: 0s - loss: 0.0068 - sparse_categorical_accuracy: 0.9974\n",
            "Epoch 202: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 0.0065 - sparse_categorical_accuracy: 0.9977 - val_loss: 1.8992 - val_sparse_categorical_accuracy: 0.6822\n",
            "Epoch 203/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0023 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 203: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0023 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.9363 - val_sparse_categorical_accuracy: 0.6822\n",
            "Epoch 204/900\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0265 - sparse_categorical_accuracy: 0.9953\n",
            "Epoch 204: val_sparse_categorical_accuracy did not improve from 0.72897\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 0.0265 - sparse_categorical_accuracy: 0.9953 - val_loss: 1.8433 - val_sparse_categorical_accuracy: 0.6729\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 554,
      "id": "1b676d1c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "1b676d1c",
        "outputId": "5e4e7dbd-90c9-414d-c084-21a10c00f632"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Plot model accuracy over ephocs\n",
        "plt.plot(history.history['sparse_categorical_accuracy'])\n",
        "plt.plot(history.history['val_sparse_categorical_accuracy'])\n",
        "plt.ylim(0, 1)\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 555,
      "id": "df8c9ade",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df8c9ade",
        "outputId": "50b2c403-4380-4086-a637-1525a4cd7ba6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7a51f44aabc0>"
            ]
          },
          "metadata": {},
          "execution_count": 555
        }
      ],
      "source": [
        "model.load_weights(checkpoint_path) #to load model with highest accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 556,
      "id": "2f08c66c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f08c66c",
        "outputId": "033e0cf1-3afe-4338-c6ec-7386451333e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 8ms/step - loss: 1.7890 - sparse_categorical_accuracy: 0.7290\n",
            "Pre-training accuracy: 72.8972%\n"
          ]
        }
      ],
      "source": [
        "# Calculate pre-training accuracy\n",
        "score = model.evaluate(X_test_scalled, y_test, verbose=1)\n",
        "accuracy = 100*score[1]\n",
        "\n",
        "print(\"Pre-training accuracy: %.4f%%\" % accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 557,
      "id": "1bdfcfc8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bdfcfc8",
        "outputId": "5a3e2093-0403-4beb-f8c0-65c2b6b8305d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy:  1.0\n",
            "Testing Accuracy:  0.7289719581604004\n"
          ]
        }
      ],
      "source": [
        "# Evaluating the model on the training and testing set\n",
        "score = model.evaluate(X_train_scalled, y_train, verbose=0)\n",
        "print(\"Training Accuracy: \", score[1])\n",
        "\n",
        "score = model.evaluate(X_test_scalled, y_test, verbose=0)\n",
        "print(\"Testing Accuracy: \", score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 558,
      "id": "eaac1550",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaac1550",
        "outputId": "5716f3ff-1fc4-41ca-fc17-28f9e486e51d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 1s 7ms/step\n"
          ]
        }
      ],
      "source": [
        "#Get predictions from model\n",
        "y_test_predictions = model.predict(X_test_scalled) # it will give the prediction data of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 559,
      "id": "8a9df249",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a9df249",
        "outputId": "bda3b0c2-ce6f-4f8d-9f6b-de9f0da6c46f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(107, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 559
        }
      ],
      "source": [
        "y_test_predictions.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 560,
      "id": "dda064ae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dda064ae",
        "outputId": "de02f9b6-f74b-44fc-c2a2-8875f8a6d6d2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.95773046e-05, 5.50605182e-04, 5.60007466e-04, 9.40734744e-01,\n",
              "        5.75255118e-02, 7.85927696e-05, 5.30928257e-04],\n",
              "       [2.91953838e-05, 3.07838380e-01, 1.82346426e-04, 6.63693412e-04,\n",
              "        3.39132966e-05, 6.91211998e-01, 4.04712773e-05],\n",
              "       [7.56862428e-05, 7.77808040e-08, 9.99901056e-01, 1.04155106e-07,\n",
              "        8.44326041e-06, 9.99698204e-06, 4.54629617e-06],\n",
              "       [1.05641829e-03, 9.56225904e-06, 1.77105851e-02, 8.43732869e-06,\n",
              "        9.80865300e-01, 2.39571746e-05, 3.25710193e-04],\n",
              "       [9.86966133e-01, 1.40404300e-05, 6.43757312e-03, 1.22702386e-06,\n",
              "        6.29910128e-03, 5.66406197e-06, 2.76295847e-04],\n",
              "       [1.12289716e-04, 2.45658782e-07, 9.99622226e-01, 4.44280658e-07,\n",
              "        2.26117423e-04, 1.05609288e-05, 2.81493722e-05],\n",
              "       [1.20688812e-03, 1.46393368e-05, 4.64703096e-03, 6.43304202e-06,\n",
              "        9.93984640e-01, 1.30611210e-04, 9.78701519e-06],\n",
              "       [5.37633753e-07, 9.99897361e-01, 1.17001598e-07, 8.51578989e-06,\n",
              "        5.10709185e-07, 9.23584230e-05, 5.55385043e-07],\n",
              "       [1.24266844e-05, 1.49774633e-03, 6.99183147e-04, 1.11935762e-04,\n",
              "        3.62612554e-05, 9.97519135e-01, 1.23371836e-04],\n",
              "       [1.08249966e-04, 2.94482132e-04, 1.83353113e-04, 1.23720383e-06,\n",
              "        1.54258100e-06, 9.99345720e-01, 6.54005780e-05],\n",
              "       [3.22241902e-01, 1.49958243e-04, 1.96747314e-02, 1.28918309e-05,\n",
              "        6.57687008e-01, 1.75864028e-04, 5.77205101e-05],\n",
              "       [1.84567849e-04, 5.22400612e-08, 9.99780476e-01, 1.65027558e-07,\n",
              "        2.01982925e-06, 2.70769233e-05, 5.66597828e-06],\n",
              "       [2.11625093e-06, 9.99674082e-01, 5.64823267e-07, 2.57782522e-04,\n",
              "        1.45378945e-05, 4.54916444e-05, 5.39884923e-06],\n",
              "       [9.56253111e-01, 1.97280515e-05, 2.77569890e-02, 2.27145892e-06,\n",
              "        1.55607592e-02, 4.86726276e-05, 3.58372490e-04],\n",
              "       [9.99596059e-01, 8.92374317e-07, 3.86311556e-04, 2.61342326e-08,\n",
              "        9.70252131e-06, 2.70517580e-06, 4.38140069e-06],\n",
              "       [5.68436235e-07, 3.94564158e-05, 2.17077250e-05, 9.99782741e-01,\n",
              "        1.25709412e-04, 8.33856848e-06, 2.14653192e-05],\n",
              "       [1.00440542e-04, 9.69135883e-08, 9.99823034e-01, 1.06015094e-07,\n",
              "        4.93782791e-05, 8.73992212e-06, 1.81227024e-05],\n",
              "       [5.56004234e-05, 6.82477951e-02, 2.89678643e-03, 7.07340539e-02,\n",
              "        1.80403600e-04, 8.57551098e-01, 3.34186858e-04],\n",
              "       [1.41164633e-02, 3.18372315e-07, 9.85736191e-01, 4.17480436e-07,\n",
              "        5.12089264e-06, 1.01745936e-04, 3.96107826e-05],\n",
              "       [9.98099029e-01, 4.76184914e-06, 5.77752537e-04, 3.26811289e-07,\n",
              "        1.27476128e-03, 1.37294364e-06, 4.19036987e-05],\n",
              "       [1.23522545e-06, 9.99706805e-01, 2.99043194e-07, 2.21573609e-05,\n",
              "        1.23141467e-06, 2.67132535e-04, 1.16990179e-06],\n",
              "       [1.03022670e-03, 1.59930096e-07, 9.98717189e-01, 2.62260187e-07,\n",
              "        2.04521029e-06, 2.31318802e-04, 1.87490587e-05],\n",
              "       [5.85629314e-05, 3.06842480e-08, 9.99924064e-01, 3.55802356e-08,\n",
              "        2.23832353e-06, 8.31156831e-06, 6.72325905e-06],\n",
              "       [7.22061435e-04, 6.12223090e-08, 9.99228716e-01, 1.31565301e-07,\n",
              "        1.95955408e-06, 2.72775869e-05, 1.98358957e-05],\n",
              "       [1.41712826e-05, 8.08366537e-01, 1.01405494e-04, 1.81350648e-01,\n",
              "        3.74464202e-04, 8.07632226e-03, 1.71645393e-03],\n",
              "       [2.12032872e-04, 2.22184360e-01, 4.04738821e-03, 3.35509390e-01,\n",
              "        1.04644239e-01, 3.31889927e-01, 1.51265587e-03],\n",
              "       [1.65015878e-03, 5.71743044e-07, 9.98136759e-01, 7.22814377e-07,\n",
              "        5.25811047e-05, 6.11171345e-05, 9.80845580e-05],\n",
              "       [3.19803506e-03, 9.51740560e-07, 9.94477868e-01, 8.42973407e-07,\n",
              "        1.94629101e-04, 3.18835409e-05, 2.09576730e-03],\n",
              "       [7.27993097e-07, 9.99813497e-01, 2.71125174e-07, 1.26070212e-04,\n",
              "        1.89121522e-06, 5.54210565e-05, 2.14510715e-06],\n",
              "       [8.59431623e-07, 3.06206732e-03, 9.39502752e-06, 9.96583819e-01,\n",
              "        1.98736030e-04, 1.07836167e-04, 3.71896058e-05],\n",
              "       [1.33800531e-05, 2.18471934e-04, 2.36693177e-05, 5.21579641e-05,\n",
              "        9.99524236e-01, 1.58598232e-05, 1.52237655e-04],\n",
              "       [1.41083911e-01, 1.71190754e-06, 8.58202100e-01, 2.80587358e-07,\n",
              "        6.40518147e-06, 6.43418287e-04, 6.21398722e-05],\n",
              "       [2.66561733e-06, 9.99325633e-01, 1.03346031e-06, 3.60570848e-05,\n",
              "        2.47396974e-06, 6.29382557e-04, 2.85169358e-06],\n",
              "       [1.25064782e-03, 4.35614396e-07, 9.98308420e-01, 2.25634125e-07,\n",
              "        1.96761248e-05, 4.16521885e-04, 4.03101330e-06],\n",
              "       [2.39926740e-05, 4.72603290e-08, 9.99918938e-01, 6.07729973e-08,\n",
              "        3.08936433e-05, 1.98800426e-05, 6.10028883e-06],\n",
              "       [6.92197937e-05, 8.75805598e-03, 1.34260505e-02, 1.74197167e-01,\n",
              "        5.17794073e-01, 2.84891844e-01, 8.63535039e-04],\n",
              "       [9.84214921e-07, 1.35431561e-04, 3.76888020e-05, 6.43080193e-06,\n",
              "        9.43928512e-07, 9.99818146e-01, 3.86212946e-07],\n",
              "       [9.91240680e-01, 6.82292057e-06, 1.16164330e-03, 4.54551497e-07,\n",
              "        7.47208670e-03, 4.37470544e-06, 1.13858914e-04],\n",
              "       [7.65279310e-06, 2.96414219e-05, 1.38911455e-05, 6.68220309e-06,\n",
              "        9.99920368e-01, 3.77918764e-06, 1.79937324e-05],\n",
              "       [1.61147564e-05, 1.79405119e-02, 2.42183055e-03, 8.66379976e-01,\n",
              "        1.38935167e-03, 1.07767880e-01, 4.08434402e-03],\n",
              "       [7.26833321e-07, 2.99431986e-06, 1.20061733e-04, 2.65335046e-07,\n",
              "        2.50087368e-07, 9.99875546e-01, 1.65863113e-07],\n",
              "       [3.05363908e-04, 5.40964642e-08, 9.99629259e-01, 1.69712862e-07,\n",
              "        1.35140601e-06, 4.44961042e-05, 1.93632513e-05],\n",
              "       [5.27944008e-04, 2.84162700e-01, 1.99710150e-04, 1.39378081e-03,\n",
              "        3.98986088e-03, 7.34248140e-04, 7.08991766e-01],\n",
              "       [1.15019357e-04, 1.54142619e-08, 9.99871850e-01, 2.16399663e-08,\n",
              "        5.70076566e-07, 9.84372491e-06, 2.73011415e-06],\n",
              "       [1.54532677e-06, 3.11129975e-06, 2.61026071e-06, 2.96438429e-06,\n",
              "        9.99988079e-01, 5.49606170e-07, 1.07322535e-06],\n",
              "       [5.15273632e-06, 9.93327379e-01, 3.77618767e-06, 1.46899329e-04,\n",
              "        4.97662586e-06, 6.50697341e-03, 4.83391386e-06],\n",
              "       [9.50954709e-05, 9.88465071e-01, 1.56347451e-05, 6.20702340e-05,\n",
              "        1.10432258e-04, 1.12357605e-02, 1.59833799e-05],\n",
              "       [5.18601562e-04, 8.32028277e-08, 9.99409199e-01, 1.25485002e-07,\n",
              "        1.57627267e-06, 5.62725982e-05, 1.41145538e-05],\n",
              "       [7.42460325e-05, 2.05523975e-07, 9.99765456e-01, 3.48396526e-07,\n",
              "        6.91038076e-05, 1.31897123e-05, 7.74111977e-05],\n",
              "       [4.48862556e-06, 5.08613593e-05, 4.36841083e-06, 4.11822257e-05,\n",
              "        9.99894023e-01, 1.58704040e-06, 3.47503692e-06],\n",
              "       [7.17065413e-04, 4.54274006e-03, 7.25667831e-03, 2.17717583e-03,\n",
              "        8.61814380e-01, 1.23417042e-01, 7.48945677e-05],\n",
              "       [1.55243615e-04, 1.68246675e-06, 9.98327196e-01, 1.02493132e-05,\n",
              "        1.37211615e-03, 6.16827820e-05, 7.18265437e-05],\n",
              "       [2.90725166e-05, 1.80406787e-03, 9.42811952e-04, 4.18216878e-05,\n",
              "        2.12990244e-06, 9.95543540e-01, 1.63658534e-03],\n",
              "       [1.13803882e-03, 5.79036474e-01, 3.36212630e-04, 1.35031864e-01,\n",
              "        2.68517524e-01, 4.18228935e-03, 1.17575806e-02],\n",
              "       [9.01627004e-07, 1.02285439e-05, 1.11054678e-05, 3.89009287e-08,\n",
              "        5.04836599e-08, 9.99977589e-01, 9.48969259e-08],\n",
              "       [2.60754814e-06, 6.47383285e-06, 5.42820499e-06, 3.19786773e-06,\n",
              "        7.09145854e-04, 2.26569199e-07, 9.99272883e-01],\n",
              "       [1.49811924e-04, 2.36219205e-02, 2.91863427e-04, 3.24065797e-03,\n",
              "        9.69528615e-01, 2.65953480e-03, 5.07567835e-04],\n",
              "       [3.61160096e-06, 2.13608491e-05, 5.48402613e-06, 4.42206256e-05,\n",
              "        9.99903917e-01, 1.43676687e-06, 2.00130708e-05],\n",
              "       [7.07959771e-06, 9.56362247e-01, 1.60740055e-05, 6.29784190e-04,\n",
              "        1.32047653e-05, 4.29511219e-02, 2.05127471e-05],\n",
              "       [1.76184159e-03, 2.31033994e-07, 9.98037994e-01, 2.24483259e-07,\n",
              "        1.10970432e-05, 1.68815113e-05, 1.71728607e-04],\n",
              "       [2.37791392e-06, 9.99666810e-01, 9.70137080e-07, 2.14095067e-04,\n",
              "        4.07154903e-06, 7.88421894e-05, 3.28234310e-05],\n",
              "       [9.99943495e-01, 8.19796981e-07, 4.50093830e-05, 7.25260962e-09,\n",
              "        5.88848297e-06, 8.12372264e-07, 3.84859959e-06],\n",
              "       [2.79911751e-06, 9.98660803e-01, 1.25429244e-06, 1.20230613e-03,\n",
              "        9.00377472e-06, 6.03185981e-05, 6.34551907e-05],\n",
              "       [9.99806106e-01, 1.62193078e-06, 1.35032707e-04, 3.71854298e-08,\n",
              "        5.12551778e-05, 1.03744867e-06, 4.88900059e-06],\n",
              "       [1.03004072e-06, 9.99615073e-01, 4.75941135e-07, 1.84118238e-04,\n",
              "        1.19026083e-06, 1.96273060e-04, 1.82070403e-06],\n",
              "       [2.42824996e-07, 9.44653948e-05, 3.65751157e-06, 9.99812901e-01,\n",
              "        7.62716663e-05, 4.41798647e-06, 8.00971884e-06],\n",
              "       [9.05677837e-07, 9.99030709e-01, 3.16103126e-07, 1.37474372e-05,\n",
              "        4.74087329e-07, 9.53160750e-04, 6.34239825e-07],\n",
              "       [2.04354956e-06, 9.99666452e-01, 7.86373789e-07, 2.30833815e-04,\n",
              "        4.97551719e-06, 7.77163368e-05, 1.71164684e-05],\n",
              "       [5.47448348e-04, 1.46285029e-05, 3.96796763e-01, 1.94109525e-05,\n",
              "        6.01508260e-01, 1.07492250e-03, 3.85635831e-05],\n",
              "       [7.18859064e-06, 2.39312765e-03, 1.83738652e-04, 9.61114109e-01,\n",
              "        2.45639239e-03, 1.11082569e-04, 3.37343141e-02],\n",
              "       [8.01491988e-05, 2.00521999e-08, 9.99897838e-01, 6.19641796e-08,\n",
              "        7.57543717e-07, 1.46902503e-05, 6.37832136e-06],\n",
              "       [7.67865174e-07, 1.88201564e-04, 1.62032320e-05, 9.99383330e-01,\n",
              "        2.60186207e-04, 1.05537138e-05, 1.40708536e-04],\n",
              "       [7.64550464e-07, 6.04970774e-05, 3.61681268e-05, 9.99472678e-01,\n",
              "        3.33014905e-04, 1.09236680e-05, 8.59759675e-05],\n",
              "       [1.04221726e-04, 8.06389338e-08, 9.99851346e-01, 8.62008207e-08,\n",
              "        2.20633920e-05, 2.02595420e-05, 1.84558235e-06],\n",
              "       [2.39090900e-06, 1.44481746e-04, 8.64852918e-05, 1.20051857e-06,\n",
              "        4.51913365e-07, 9.99756157e-01, 8.84947440e-06],\n",
              "       [1.51216154e-05, 8.19593515e-06, 1.48855570e-05, 2.52100244e-06,\n",
              "        9.99949336e-01, 1.86665852e-06, 8.02815794e-06],\n",
              "       [7.47060418e-01, 1.84466114e-06, 2.52632588e-01, 5.33067578e-07,\n",
              "        3.32128802e-05, 5.81373024e-05, 2.13217063e-04],\n",
              "       [8.12801123e-01, 1.98634025e-05, 1.77403465e-01, 5.05184971e-06,\n",
              "        6.09189039e-03, 3.86480715e-05, 3.63997277e-03],\n",
              "       [6.41334627e-05, 4.99258058e-05, 7.10167660e-05, 2.32560367e-07,\n",
              "        6.29839349e-07, 9.99811709e-01, 2.37407312e-06],\n",
              "       [5.07908735e-05, 1.14826116e-06, 9.71469935e-03, 2.09038135e-05,\n",
              "        7.85399025e-05, 8.16115426e-06, 9.90125656e-01],\n",
              "       [1.42113538e-03, 1.11964184e-07, 9.98359144e-01, 1.48125451e-07,\n",
              "        2.65386439e-06, 3.21911320e-05, 1.84668956e-04],\n",
              "       [1.11076213e-06, 2.07136298e-04, 1.89592738e-05, 9.99636054e-01,\n",
              "        8.57811683e-05, 2.91949000e-05, 2.16524604e-05],\n",
              "       [9.99761522e-01, 1.14230340e-06, 2.22052491e-04, 2.11256630e-08,\n",
              "        9.68109180e-06, 2.62412414e-06, 3.00218153e-06],\n",
              "       [3.62164683e-05, 3.46626257e-05, 1.48415775e-05, 4.85360852e-06,\n",
              "        9.99748528e-01, 3.27467092e-06, 1.57687100e-04],\n",
              "       [9.96784449e-01, 2.55238592e-06, 2.85444525e-03, 7.90432466e-08,\n",
              "        3.37425561e-04, 1.50913456e-05, 5.98670886e-06],\n",
              "       [4.16283501e-06, 9.96543586e-01, 2.33953438e-06, 1.27615902e-04,\n",
              "        5.00362648e-06, 3.31603992e-03, 1.28090937e-06],\n",
              "       [2.27881465e-05, 3.76009197e-08, 9.99954104e-01, 5.18593382e-08,\n",
              "        8.27897929e-06, 8.30970203e-06, 6.58011322e-06],\n",
              "       [4.88343794e-05, 1.45422504e-07, 9.99679923e-01, 2.42575140e-07,\n",
              "        8.38421693e-05, 1.42443005e-05, 1.72784523e-04],\n",
              "       [4.54743765e-03, 3.99127093e-05, 6.89553618e-01, 3.17797931e-05,\n",
              "        2.87023664e-01, 2.91897391e-04, 1.85116921e-02],\n",
              "       [1.59083011e-05, 7.46581674e-01, 7.38889212e-05, 2.46329904e-01,\n",
              "        1.94168184e-04, 6.72903284e-03, 7.53803106e-05],\n",
              "       [4.13020134e-05, 1.81525611e-05, 3.03614579e-05, 4.02262140e-06,\n",
              "        9.99873638e-01, 1.35780047e-05, 1.90076898e-05],\n",
              "       [3.48101980e-06, 7.42753968e-03, 4.20078795e-05, 8.52363737e-05,\n",
              "        6.73000295e-06, 9.92431402e-01, 3.57410386e-06],\n",
              "       [9.91054952e-01, 2.52603320e-04, 2.87071522e-03, 1.27272756e-06,\n",
              "        7.28585175e-04, 5.05095581e-03, 4.09729291e-05],\n",
              "       [9.98745322e-01, 8.65568836e-06, 6.09601208e-04, 1.87159458e-07,\n",
              "        6.12816133e-04, 1.82132026e-05, 5.14797603e-06],\n",
              "       [1.04119281e-05, 2.37301319e-05, 6.69770961e-05, 1.16956544e-05,\n",
              "        9.99865055e-01, 9.63421826e-06, 1.24789276e-05],\n",
              "       [3.04061396e-05, 3.97927500e-03, 2.16486165e-03, 3.49416733e-02,\n",
              "        7.50587496e-04, 1.11817580e-03, 9.57015038e-01],\n",
              "       [9.18843434e-05, 1.81391723e-02, 8.06882221e-04, 2.28947820e-03,\n",
              "        2.02197931e-03, 9.76637721e-01, 1.29354521e-05],\n",
              "       [1.70787156e-01, 3.49028960e-05, 1.35762142e-02, 1.15414105e-05,\n",
              "        1.74560264e-04, 2.37633431e-04, 8.15177917e-01],\n",
              "       [2.21140668e-04, 1.30952358e-07, 9.99691367e-01, 1.77744809e-07,\n",
              "        6.08291739e-05, 1.71061074e-05, 9.33423144e-06],\n",
              "       [1.75398884e-06, 8.05199215e-06, 9.35017852e-06, 7.02387524e-06,\n",
              "        6.36735967e-06, 5.38118456e-07, 9.99966860e-01],\n",
              "       [3.73845338e-04, 5.34578487e-02, 3.32951266e-03, 6.42412296e-03,\n",
              "        8.95547450e-01, 3.97255421e-02, 1.14170089e-03],\n",
              "       [8.91031474e-02, 6.94668870e-07, 9.10656333e-01, 3.41025128e-07,\n",
              "        6.99155498e-06, 1.70215251e-04, 6.22938387e-05],\n",
              "       [7.46838111e-07, 8.11262144e-05, 1.33802796e-05, 9.99569714e-01,\n",
              "        2.94586644e-04, 5.75614240e-06, 3.47724636e-05],\n",
              "       [3.17663362e-05, 2.96075746e-06, 1.46123202e-04, 3.70926796e-06,\n",
              "        5.46089905e-06, 8.11227324e-07, 9.99809206e-01],\n",
              "       [4.91676701e-06, 2.87417613e-04, 1.21850899e-04, 9.80943620e-01,\n",
              "        1.83894653e-02, 2.17781617e-05, 2.30900972e-04],\n",
              "       [1.28190528e-04, 3.27598606e-08, 9.99826849e-01, 1.54443441e-07,\n",
              "        1.22096958e-06, 3.93777082e-05, 4.21912910e-06],\n",
              "       [3.29342511e-05, 2.43457853e-05, 8.87044662e-05, 8.21806134e-06,\n",
              "        9.99812067e-01, 3.11641634e-05, 2.64507594e-06]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 560
        }
      ],
      "source": [
        "y_test_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 561,
      "id": "996eb306",
      "metadata": {
        "id": "996eb306"
      },
      "outputs": [],
      "source": [
        "y_test_predictions=np.argmax(y_test_predictions,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 562,
      "id": "5f9002eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f9002eb",
        "outputId": "991967f2-a878-40d5-e774-e143fb8e15f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 5, 2, 4, 0, 2, 4, 1, 5, 5, 4, 2, 1, 0, 0, 3, 2, 5, 2, 0, 1, 2,\n",
              "       2, 2, 1, 3, 2, 2, 1, 3, 4, 2, 1, 2, 2, 4, 5, 0, 4, 3, 5, 2, 6, 2,\n",
              "       4, 1, 1, 2, 2, 4, 4, 2, 5, 1, 5, 6, 4, 4, 1, 2, 1, 0, 1, 0, 1, 3,\n",
              "       1, 1, 4, 3, 2, 3, 3, 2, 5, 4, 0, 0, 5, 6, 2, 3, 0, 4, 0, 1, 2, 2,\n",
              "       2, 1, 4, 5, 0, 0, 4, 6, 5, 6, 2, 6, 4, 2, 3, 6, 3, 2, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 562
        }
      ],
      "source": [
        "y_test_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 563,
      "id": "2a98a885",
      "metadata": {
        "id": "2a98a885"
      },
      "outputs": [],
      "source": [
        "# df.replace({ 'happyness': 0, 'neutral': 1,'anger': 2,'sadness': 3, 'fear':4,'boredom':5,'disgust':6}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 564,
      "id": "d0b8e93d",
      "metadata": {
        "id": "d0b8e93d"
      },
      "outputs": [],
      "source": [
        "emotions={\n",
        " 0: 'happyness',\n",
        " 1: 'neutral',\n",
        " 2: 'anger',\n",
        " 3: 'sadness',\n",
        " 4: 'fear',\n",
        " 5: 'boredom',\n",
        " 6: 'disgust',\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 565,
      "id": "a32e6964",
      "metadata": {
        "id": "a32e6964"
      },
      "outputs": [],
      "source": [
        "label=[]\n",
        "for i in y_test_predictions:\n",
        "    label1=emotions[i]\n",
        "    label.append(label1)\n",
        "label\n",
        "y_pred_acc=np.array(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 566,
      "id": "c92b0963",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c92b0963",
        "outputId": "a87c91c8-5dec-4613-89d6-c992679a7b4d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['sadness', 'boredom', 'anger', 'fear', 'happyness', 'anger',\n",
              "       'fear', 'neutral', 'boredom', 'boredom', 'fear', 'anger',\n",
              "       'neutral', 'happyness', 'happyness', 'sadness', 'anger', 'boredom',\n",
              "       'anger', 'happyness', 'neutral', 'anger', 'anger', 'anger',\n",
              "       'neutral', 'sadness', 'anger', 'anger', 'neutral', 'sadness',\n",
              "       'fear', 'anger', 'neutral', 'anger', 'anger', 'fear', 'boredom',\n",
              "       'happyness', 'fear', 'sadness', 'boredom', 'anger', 'disgust',\n",
              "       'anger', 'fear', 'neutral', 'neutral', 'anger', 'anger', 'fear',\n",
              "       'fear', 'anger', 'boredom', 'neutral', 'boredom', 'disgust',\n",
              "       'fear', 'fear', 'neutral', 'anger', 'neutral', 'happyness',\n",
              "       'neutral', 'happyness', 'neutral', 'sadness', 'neutral', 'neutral',\n",
              "       'fear', 'sadness', 'anger', 'sadness', 'sadness', 'anger',\n",
              "       'boredom', 'fear', 'happyness', 'happyness', 'boredom', 'disgust',\n",
              "       'anger', 'sadness', 'happyness', 'fear', 'happyness', 'neutral',\n",
              "       'anger', 'anger', 'anger', 'neutral', 'fear', 'boredom',\n",
              "       'happyness', 'happyness', 'fear', 'disgust', 'boredom', 'disgust',\n",
              "       'anger', 'disgust', 'fear', 'anger', 'sadness', 'disgust',\n",
              "       'sadness', 'anger', 'fear'], dtype='<U9')"
            ]
          },
          "metadata": {},
          "execution_count": 566
        }
      ],
      "source": [
        "y_pred_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 567,
      "id": "59bdabc1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59bdabc1",
        "outputId": "a402402b-ef77-4c8e-e5b0-ded67538928f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      3\n",
              "1      3\n",
              "2      2\n",
              "3      4\n",
              "4      0\n",
              "      ..\n",
              "102    3\n",
              "103    6\n",
              "104    3\n",
              "105    0\n",
              "106    5\n",
              "Name: 0.1, Length: 107, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 567
        }
      ],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 568,
      "id": "0dec7231",
      "metadata": {
        "id": "0dec7231"
      },
      "outputs": [],
      "source": [
        "emotion={\n",
        " 0: 'happyness',\n",
        " 1: 'neutral',\n",
        " 2: 'anger',\n",
        " 3: 'sadness',\n",
        " 4: 'fear',\n",
        " 5: 'boredom',\n",
        " 6: 'disgust',\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 569,
      "id": "06e8caee",
      "metadata": {
        "id": "06e8caee"
      },
      "outputs": [],
      "source": [
        "label_test=[]\n",
        "for i in y_test:\n",
        "    label_test.append(emotion[i])\n",
        "label_test\n",
        "y_true_accu=np.array(label_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 570,
      "id": "9562d2b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9562d2b0",
        "outputId": "0bde8af0-344c-439b-9e48-52dea29b0f21"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['sadness', 'sadness', 'anger', 'fear', 'happyness', 'anger',\n",
              "       'happyness', 'neutral', 'disgust', 'neutral', 'fear', 'anger',\n",
              "       'neutral', 'fear', 'happyness', 'sadness', 'anger', 'boredom',\n",
              "       'anger', 'fear', 'neutral', 'anger', 'anger', 'anger', 'neutral',\n",
              "       'sadness', 'anger', 'disgust', 'neutral', 'sadness', 'anger',\n",
              "       'anger', 'neutral', 'anger', 'anger', 'sadness', 'boredom',\n",
              "       'happyness', 'fear', 'neutral', 'boredom', 'anger', 'disgust',\n",
              "       'anger', 'fear', 'boredom', 'neutral', 'anger', 'anger', 'fear',\n",
              "       'anger', 'anger', 'boredom', 'fear', 'boredom', 'disgust',\n",
              "       'boredom', 'fear', 'boredom', 'anger', 'neutral', 'happyness',\n",
              "       'neutral', 'happyness', 'boredom', 'sadness', 'boredom', 'neutral',\n",
              "       'fear', 'sadness', 'happyness', 'sadness', 'sadness', 'anger',\n",
              "       'boredom', 'fear', 'happyness', 'happyness', 'boredom', 'disgust',\n",
              "       'happyness', 'sadness', 'anger', 'fear', 'happyness', 'neutral',\n",
              "       'anger', 'anger', 'anger', 'boredom', 'fear', 'boredom', 'neutral',\n",
              "       'neutral', 'fear', 'disgust', 'neutral', 'disgust', 'happyness',\n",
              "       'disgust', 'boredom', 'happyness', 'sadness', 'disgust', 'sadness',\n",
              "       'happyness', 'boredom'], dtype='<U9')"
            ]
          },
          "metadata": {},
          "execution_count": 570
        }
      ],
      "source": [
        "y_true_accu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 571,
      "id": "8def2194",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8def2194",
        "outputId": "9f127000-1558-4a63-c448-170485077524"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 72.90%\n"
          ]
        }
      ],
      "source": [
        "#DataFlair - Calculate the accuracy of our model\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy=accuracy_score(y_true=y_true_accu, y_pred=y_pred_acc)\n",
        "\n",
        "#DataFlair - Print the accuracy\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 572,
      "id": "0397dc13",
      "metadata": {
        "id": "0397dc13"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm=confusion_matrix(y_true=y_true_accu, y_pred=y_pred_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 573,
      "id": "7cee98a1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cee98a1",
        "outputId": "1874d6d2-cc64-47be-8e23-5ff394e3e98c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.79      0.88      0.83        25\n",
            "     boredom       0.67      0.50      0.57        16\n",
            "     disgust       1.00      0.78      0.88         9\n",
            "        fear       0.61      0.79      0.69        14\n",
            "   happyness       0.62      0.57      0.59        14\n",
            "     neutral       0.65      0.69      0.67        16\n",
            "     sadness       0.92      0.85      0.88        13\n",
            "\n",
            "    accuracy                           0.73       107\n",
            "   macro avg       0.75      0.72      0.73       107\n",
            "weighted avg       0.74      0.73      0.73       107\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true_accu,y_pred_acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 573,
      "id": "73b3090a",
      "metadata": {
        "id": "73b3090a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 573,
      "id": "4537d18f",
      "metadata": {
        "id": "4537d18f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}